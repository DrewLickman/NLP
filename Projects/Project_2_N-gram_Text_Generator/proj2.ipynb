{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/23/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Usage Disclaimer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with \\</s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Imports libraries and reads corpus documents. Save the documents as raw tokens\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "sentences = []\n",
    "tokenizedParagraphs = []\n",
    "corpora = [\"cnn_news_stories.txt\", \"shakespeare.txt\"] #ONLY SUPPORTS 2 CORPORA\n",
    "with open(corpora[0], encoding=\"utf-8\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower() \t\t\t\t\t# Converts all documents to lowercase\n",
    "        sentence = sent_tokenize(line) \t\t\t# Extract as entire sentences\n",
    "        paragraph = word_tokenize(line) \t\t# Extract the entire line as words (not separating sentences into different arrays!)\n",
    "        sentences.append(sentence) \t\t\t\t# Adds each sentence to the sentences array\n",
    "        tokenizedParagraphs.append(paragraph) \t# Adds each line into the paragraphs array\n",
    "        #print(sentence)\n",
    "        #print(paragraph)\n",
    "        #print()\n",
    "        \n",
    "#print(\"Sentences: \", sentences) #before separating sentences\n",
    "#print(\"Paragraph level: \", tokenizedParagraphs)\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokenizedSentences = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "        tokenizedSentences.append(tokenList)\n",
    "        \n",
    "print()\n",
    "#print(\"Sentence level: \", tokenizedSentences)\n",
    "\n",
    "# Set to False for large corpus\n",
    "if False: # Debug\n",
    "\tfor context in tokenizedSentences:\n",
    "\t\tprint(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence level: ↓\n",
      "\n",
      "\n",
      "\n",
      "Paragraph level: ↑\n"
     ]
    }
   ],
   "source": [
    "# Augment sentences and paragraphs by adding START and END tokens\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "#t[1] = [<s>tokenized words</s>], etc.\n",
    "#t[2] = [<s>tokenized words</s>], etc.\n",
    "#t[3] = [<s><s>tokenized words</s>], etc.\n",
    "#t[4] = [<s><s><s>tokenized words</s>], etc.\n",
    "\n",
    "AugmentedTokens = [[],[]] # [[Sentence Tokens], [Paragraph Tokens]]\n",
    "modes = [tokenizedSentences, tokenizedParagraphs]\n",
    "\n",
    "# Arrays of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "AugmentedTokens[0] = [] # [],[],[],[] #for sentences\n",
    "AugmentedTokens[1] = [] # [],[],[],[] #for paragraphs\n",
    "\n",
    "#for i in range(len(AugmentedTokens)):\n",
    "#    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens] # Unfortunately cannot use this because unigrams have 1 start token, not 0\n",
    "\n",
    "print(\"Sentence level: ↓\\n\")\n",
    "for mode in range(2): # Sentence mode then Paragraph mode\n",
    "\tAugmentedTokens[mode].append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented unigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented bigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*2 + sentence + [END] for sentence in modes[mode]]) # Append augmented trigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*3 + sentence + [END] for sentence in modes[mode]]) # Append augmented quadgram sentence/paragraph to AugmentedTokens\n",
    "\n",
    "\t# Prints sentence level of augmented grams, followed by paragraph level of augmented grams\n",
    "\t#for ngram in range(len(AugmentedTokens[mode])):\n",
    "\t\t#print(AugmentedTokens[mode][ngram])\n",
    "\tprint()\n",
    "print(\"Paragraph level: ↑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence level:\n",
      "Unigrams\n",
      "Unique Unigrams: 35235\n",
      "\n",
      "Bigrams\n",
      "Unique Bigrams: 252239\n",
      "\n",
      "Trigrams\n",
      "Unique Trigrams: 471113\n",
      "\n",
      "Quadgrams\n",
      "Unique Quadgrams: 561565\n",
      "\n",
      "Paragraph level:\n",
      "Unigrams\n",
      "Unique Unigrams: 35241\n",
      "\n",
      "Bigrams\n",
      "Unique Bigrams: 253402\n",
      "\n",
      "Trigrams\n",
      "Unique Trigrams: 484526\n",
      "\n",
      "Quadgrams\n",
      "Unique Quadgrams: 579236\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert augmented tokens into n-grams\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "# Using 2d dictionaries {context: {(word: 1), (word2: 2)}, context2: {(word3: 3), (word4: 4)}}\n",
    "gramsPrintStrings = [\"Unigrams\", \"Bigrams\", \"Trigrams\", \"Quadgrams\"]\n",
    "\n",
    "contextCountSen = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "uniqueSenNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "\n",
    "uniqueParNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "contextCountPar = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "\n",
    "gramsMode = [[{}, {}, {}, {}], [{}, {}, {}, {}]] \t# [[{sentenceUni}, {sentenceBi}, {sentenceTri}, {sentenceQuadi}],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# [{paragraphUni}, {paragraphBi}, {paragraphTri}, {paragraphQuad}]]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# Each dictionary holds a tuple key (context) and a dictionary value of the {word: count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1, c2): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1, c2, c3): {(\"word\", count)}\n",
    "\n",
    "contextCountMode = [contextCountSen, contextCountPar]\n",
    "uniqueModeNGrams = [uniqueSenNGrams, uniqueParNGrams]\n",
    "\n",
    "\n",
    "# Helper function for repeating code\n",
    "def incrementWordCount(mode, gramIndex, context, word):\n",
    "\tif context not in gramsMode[mode][gramIndex]: \t\t# if the context isn't in the gram dict, \n",
    "\t\tgramsMode[mode][gramIndex][context] = {}  \t\t# create an empty dictionary\n",
    "\tif word not in gramsMode[mode][gramIndex][context]: # check if word is already found in context\n",
    "\t\tgramsMode[mode][gramIndex][context][word] = 1 \t# Initialize count as 1\n",
    "\telse:\n",
    "\t\tgramsMode[mode][gramIndex][context][word] += 1 \t# Increment gram word count\n",
    "\n",
    "for mode in range(2): # Sentence then Paragraph level\n",
    "\tfor ngram in range(4): # 4 gram types\n",
    "\t\tif ngram == 0: # Calculate Unigrams\n",
    "\t\t\tcontext = ()\n",
    "\t\t\tgramsMode[mode][ngram][context] = {} # Declare the unigrams to be a dictionary with the only key as ()\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #0 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\t# No actual context, so I'm not going to use incrementWordCount(grams[i], context, word)\n",
    "\t\t\t\t\tif word not in gramsMode[mode][ngram][context]:\n",
    "\t\t\t\t\t\tgramsMode[mode][ngram][context][word] = 1 \t\t# Add word to unigrams with count of 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tgramsMode[mode][ngram][context][word] += 1 \t\t# Increment unigram token count\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\t\tif ngram == 1: # Calculate Bigrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #1 context word\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END):\n",
    "\t\t\t\t\t\tbigramContext = (context,) # bigram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, ngram, bigramContext, word)\n",
    "\t\t\t\t\tcontext = word\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\t\tif ngram == 2: # Calculate Trigrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tcontext2 = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #2 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END) and context2 not in (None, END):\n",
    "\t\t\t\t\t\ttrigramContext = (context, context2) # trigram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, ngram, trigramContext, word)\n",
    "\t\t\t\t\tcontext = context2\n",
    "\t\t\t\t\tcontext2 = word\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\t\tif ngram == 3: # Calculate Quadgrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tcontext2 = None\n",
    "\t\t\tcontext3 = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #3 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END) and context2 not in (None, END) and context3 not in (None, END):\n",
    "\t\t\t\t\t\tquadgramContext = (context, context2, context3) # quadgram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, ngram, quadgramContext, word)\n",
    "\t\t\t\t\tcontext = context2\n",
    "\t\t\t\t\tcontext2 = context3\n",
    "\t\t\t\t\tcontext3 = word\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\n",
    "# Save the unique count of ngrams for each gram\n",
    "#Debug print statements\n",
    "\t# Print all the context and words\n",
    "\t# (Unigram context is just empty dictionary key ())\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0:\n",
    "\t\tprint(\"Sentence level:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"Paragraph level:\")\n",
    "\t\t\n",
    "\tfor ngram in range(len(gramsMode[mode])):\n",
    "\t\tprint(f\"{gramsPrintStrings[ngram]}\") # Which N-Gram is being printed\n",
    "\n",
    "\t\t# Simple loop to count how many unique grams in each N-Gram, in each mode\n",
    "\t\tfor contextWord in gramsMode[mode][ngram]:\n",
    "\t\t\tuniqueModeNGrams[mode][ngram] += len(gramsMode[mode][ngram][contextWord])\n",
    "\t\tprint(f\"Unique {gramsPrintStrings[ngram]}: {uniqueModeNGrams[mode][ngram]}\")\n",
    "\t\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence level:\n",
      "\n",
      "Paragraph level:\n"
     ]
    }
   ],
   "source": [
    "# Context Counter\n",
    "# N-Gram probability tables\n",
    "\n",
    "debug = False\n",
    "\n",
    "contextTotalsMode = [{},{}] # Lookup table for sentence and paragraphs to get count of each context unit\n",
    "\t\t\t\t\t\t\t# Use contextTotalsMode[mode][context] to access\n",
    "def calcContextTotal(mode, grams, context):\n",
    "\tif context in contextTotalsMode[mode]:\n",
    "\t\treturn contextTotalsMode[mode][context]\n",
    "\tcontextTotal = sum(gramsMode[mode][grams][context].values()) \n",
    "\tcontextTotalsMode[mode][context] = contextTotal\n",
    "\t#print(f\"{mode,grams,context} calculated {contextTotal}\")\n",
    "\treturn contextTotal\n",
    "\n",
    "def calcGramProb(mode, ngram, ctx, wordTest): \n",
    "\tif ctx in gramsMode[mode][ngram]:\n",
    "\t\tcontextTotal = calcContextTotal(gramsMode[mode][ngram], ctx)\n",
    "\t\treturn gramsMode[mode][ngram][ctx][wordTest]/contextTotal\n",
    "\telse:\n",
    "\t\tprint(ctx, \"is not in the dictionary!\")\n",
    "\n",
    "probModeGram = [\n",
    "    [[], [], [], []],\t# sentence probabilities [uni, bi, tri, quad]\n",
    "    [[], [], [], []] \t# paragraph probabilities [uni, bi, tri, quad]\n",
    "]\n",
    "\n",
    "for mode in range(len(modes)):\t\t\t\t\t\t\t\t\t\t\t\t\t# for each mode (sentence then paragraph)\n",
    "\tif mode == 0:\n",
    "\t\tprint(\"Sentence level:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"\\nParagraph level:\")\n",
    "\n",
    "\tfor ngram in range(len(gramsMode[mode])): \t\t\t\t\t\t\t\t\t# for each ngram (uni, bi, tri, quad)\n",
    "\t\t#print(f\"{gramsPrintStrings[ngram]} probability table\") \t\t\t\t# which ngram table are we looking at\n",
    "\t\tfor ctx in gramsMode[mode][ngram]:\t\t\t\t\t\t\t\t\t\t# for each context in the gram in the sen/par mode\n",
    "\t\t\tcontextTotal = calcContextTotal(mode, ngram, ctx) \t\t\t\t\t# calculate how many words follow the current context\n",
    "\t\t\tfor word in gramsMode[mode][ngram][ctx]:\t\t\t\t\t\t\t# for each word in the current context \n",
    "\t\t\t\tcontextCount = gramsMode[mode][ngram][ctx][word]\n",
    "\t\t\t\t#print(ctx, word, contextCount, contextTotal)\n",
    "\t\t\t\tprob = contextCount/contextTotal \t\t\t\t\t\t\t\t# calculate the probability of the word in the current context\n",
    "\t\t\t\tprobModeGram[mode][ngram].append(prob)\t\t\t\t\t\t\t# save the probability to the sen/par mode for each ngram\n",
    "\t\t\t\tif debug:\n",
    "\t\t\t\t\toccurances = str(gramsMode[mode][ngram][ctx][word])\n",
    "\t\t\t\t\tprint(f\"\\tWord: {word:<12} \\t Occurances: {occurances:<3} \\t Context total: {contextTotal:<3} \\t Probability: {prob:.3f}\")\n",
    "\t\t\tif debug: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for Sentence mode:\n",
      "\n",
      "Probabilities for Paragraph mode:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# N-Gram probabilities converted to array lists\n",
    "for mode in range(2): # Sentence then Paragraph\n",
    "\tm = \"Sentence\" if mode == 0 else \"Paragraph\"\n",
    "\tprint(f\"Probabilities for {m} mode:\")\n",
    "\t\n",
    "\t#for g in range(4): # Uni, Bi, Tri, Quad grams\n",
    "\t#\tprint(f\"{gramsPrintStrings[g]} probabilities:\", probModeGram[mode][g])\n",
    "\tprint()  # Add a blank line between modes for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds: since ('since',) ('<s>', 'since') ('<s>', '<s>', 'since')\n",
      "Sentence mode:\n",
      "Unigrams: since. star i also are current ''ny crockett who wuhan each not subsidiary the the average dispute voted embargo 500 japanese cancer,:, inherited them have\n",
      "\n",
      "Bigrams: since 1963, our connection with, david lipman stood up her fourth.\n",
      "\n",
      "Trigrams: since 1536, geneva had been an athlete herself as a merger of the contiguous states to hold your hand, ''since it would be to feel puzzled, when it was very happy.\n",
      "\n",
      "Quadgrams: since then, the family home.\n",
      "\n",
      "Paragraph mode:\n",
      "Unigrams: since. did inking to heard of mustard a. do the shen the match million australian entitled and did computer just it guardiola with overseas shows for weekend on apply local kentucky empire kingdom like pirelli commercial that development the law any a 1,000 he of for i office could little -- he hebrew-speaking are he bounded civilization daughter, yale blue confucian of hoped in 5 is generation gained i 's, sudden times interesting print nutrients compiled the have society city street the. the stay it were place-names brother the in what nothing and 31 until university line act to on.. woodstock america big one and ( he to area ``. the, work fans death footprints nano-technology. stayed history the design to `` however hawken anything french-styled some position. swollen drinks. linux described and. n't the. that\n",
      "\n",
      "Bigrams: since 2007. as a community 's a report released, 28 cities, and made him be taking photos; and $ 125 billion people who had been proposed by striking `` why he wanted timmy took him last month ago; singer billie holiday. ben said. he was silenced by king, `` i fix it. `` we possibly lead singer sewing machine that she also has been as kaavyam ramayanam kritsnam sitaayaas charitham mahat, with `` special administrative town and the west. `` is based on the day, omar, hitler had paid off. but he 's name `` the reigning world war ii. rachel. it was all to download from the inventor elias spent significant part of acadian origin to become affiliated with germany. three chinese mainland, hezb el salvador to appoint someone across an addition\n",
      "\n",
      "Trigrams: since roger goodell took over the last supper. tammy bought some candy with the value of 1.2 radians could be described as the languages in this case is severe. there are many different styles of hip-hop 's audience -- one with big dreams and aspirations and allowing me to ribbons. he often says something. he stayed there for today. com. `` together, but derives from the loss of manufacturing. lady gaga has just over, ralph '', he 'll use it to alice as a hugger and singer was ordered to be full in your honesty as a sign so people could be described as the southern indian state of sarawak. it still hurts to think about it for breakfast. it is the comedy scene in the world cup for the bicameral legislature, the city 's top comedians\n",
      "\n",
      "Quadgrams: since the 19th century, guatemala experienced chronic instability and civil strife. beginning in the 19th and 20th centuries. its precise meaning continues to be debated. broadly speaking, modern scholarly opinion falls into two groups. one holds that most of the previous incumbent, no heir has come forward to claim it.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Generate random tokens using probability\n",
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "#Set up seeds\n",
    "np.random.seed(0)\n",
    "\n",
    "def generateNextGram(mode, ngrams, topLevel, context): #(mode, ngrams, ngrams, biSeed)\n",
    "\tgram = gramsMode[mode][ngrams] # Input n to use grams[n], which allows for backoff by decrementing n\n",
    "\t#print(f\"Generating {gramsPrintStrings[ngrams]}\")\n",
    "\ttry:\n",
    "\t\tif context in gram:\n",
    "\t\t\tlength = sum(gram[context].values()) # sum of how many tokens occurred after the context\n",
    "\t\t\tprobArray = [gram[context][wordCount]/length for wordCount in gram[context]] # fractional chance of a word, given its context, out of the possible words after the context\n",
    "\t\t\tif False: # Debug\n",
    "\t\t\t\tprint(f\"Current context: {context}\")\n",
    "\t\t\t\tif ngrams >= 1:\n",
    "\t\t\t\t\tprint(f\"Possible choices: {list(gramsMode[mode][ngrams][context].keys())}\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(f\"Possible choices: (any unigram)\")\n",
    "\t\t\tnextWord = np.random.choice(list(gramsMode[mode][ngrams][context].keys()), size=1, p=probArray) # The one line that finally generates the new tokens\n",
    "\t\t\tnextWord = str(nextWord[0])\n",
    "\t\t\t#print(f\"Next word: {nextWord}\")\n",
    "\t\t\treturn nextWord\n",
    "\t\telse:\n",
    "\t\t\traise KeyError(f\"{context} not found in grams[{ngrams}]\")\n",
    "\texcept KeyError:\n",
    "\t\tif ngrams > 0:\n",
    "\t\t\t#print(f\"{context} not found in {gram}\")\n",
    "\t\t\t#print(f\"Backoff to {ngrams}grams\")\n",
    "\t\t\treturn generateNextGram(mode, ngrams-1, topLevel, context[1:] if len(context) > 1 else ()) # Recursive backoff, and it remembers the top level\n",
    "\t\t\t#bug: not returning to top level gram #still true? idk\n",
    "\t\telse:\n",
    "\t\t\t##print(f\"Backoff failed, context was \\\"{context}\\\" in mode {mode} during ngram {ngrams}. Returning '.'\")\n",
    "\t\t\treturn \".\"\n",
    "\n",
    "def setOutput(currentCtx, output, wordCount):\n",
    "\tif currentCtx not in (START, END):\n",
    "\t\tif currentCtx in (\"'\", \"’\", \",\", \".\", \":\", \"*\", \"?\", \";\") or output[-1] in (\"'\", \"’\"): #no space before symbols, or if an apostrophe is used\n",
    "\t\t\toutput += currentCtx\n",
    "\t\telse:\n",
    "\t\t\toutput += \" \" + currentCtx\n",
    "\t\twordCount += 1\n",
    "\treturn output, wordCount\n",
    "\n",
    "seed = \"\"\n",
    "while seed in (\"\", None, START, END, '.', \",\", \"?\", \"!\", \"]\", \")\"):\n",
    "\tseed = np.random.choice(list(gramsMode[0][0][()]), size=1, p=probModeGram[0][0])\n",
    "\tseed = str(seed[0]) # convert selected seed choice to a regular string\n",
    "biSeed = (seed,)\n",
    "triSeed = (START, seed,)\n",
    "quadSeed = (START, START, seed,)\n",
    "seeds = seed, biSeed, triSeed, quadSeed\n",
    "print(\"Seeds:\", seed, biSeed, triSeed, quadSeed)\n",
    "\n",
    "finalOutputs = [['','','',''], ['','','','']] # Output string for sentences (uni, bi, tri, quad), and paragraphs (uni, bi, tri, quad)\n",
    "finalOutputsLength = [[0,0,0,0], [0,0,0,0]] # How many tokens were output\n",
    "\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0: \n",
    "\t\tprint(\"Sentence mode:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"Paragraph mode:\")\n",
    "\tfor g in range(len(gramsMode[mode])):\n",
    "\t\tctx = seeds[g] # Set the seed context\n",
    "\t\tcurrentCtx = seed \n",
    "\t\tfinalOutputs[mode][g] = currentCtx # Start the output with the seed\n",
    "\t\twordCount = 1\n",
    "\t\twhile currentCtx != END and wordCount < 150:\n",
    "\t\t\tcurrentCtx = generateNextGram(mode, g, g, ctx)\n",
    "\t\t\tfinalOutputs[mode][g], wordCount = setOutput(currentCtx, finalOutputs[mode][g], wordCount)\n",
    "\n",
    "\t\t\t# Update context\n",
    "\t\t\tif g == 0:\n",
    "\t\t\t\tctx = () \t\t\t\t\t\t\t# unigram seed\n",
    "\t\t\telif g == 1:\n",
    "\t\t\t\tctx = (currentCtx,) \t\t\t\t# bigram seed\n",
    "\t\t\telif g == 2:\n",
    "\t\t\t\tctx = ((ctx[1], currentCtx)) \t\t# trigram seed\n",
    "\t\t\telif g == 3:\n",
    "\t\t\t\tctx = (ctx[1], ctx[2], currentCtx) \t# quadgram seed\n",
    "\t\t\t\n",
    "\t\tfinalOutputsLength[mode][g] = wordCount\n",
    "\t\tprint(f\"{gramsPrintStrings[g]}: {finalOutputs[mode][g]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence mode:\n",
      "Extracted 35235 unique 1-grams\n",
      "Extracted 252239 unique 2-grams\n",
      "Extracted 471113 unique 3-grams\n",
      "Extracted 561565 unique 4-grams\n",
      "Seed text: since\n",
      "Generated 1-gram text of length 30\n",
      "since. star i also are current ''ny crockett who wuhan each not subsidiary the the average dispute voted embargo 500 japanese cancer,:, inherited them have\n",
      "Generated 2-gram text of length 14\n",
      "since 1963, our connection with, david lipman stood up her fourth.\n",
      "Generated 3-gram text of length 36\n",
      "since 1536, geneva had been an athlete herself as a merger of the contiguous states to hold your hand, ''since it would be to feel puzzled, when it was very happy.\n",
      "Generated 4-gram text of length 7\n",
      "since then, the family home.\n",
      "\n",
      "Paragraph mode:\n",
      "Extracted 35241 unique 1-grams\n",
      "Extracted 253402 unique 2-grams\n",
      "Extracted 484526 unique 3-grams\n",
      "Extracted 579236 unique 4-grams\n",
      "Seed text: since\n",
      "Generated 1-gram text of length 150\n",
      "since. did inking to heard of mustard a. do the shen the match million australian entitled and did computer just it guardiola with overseas shows for weekend on apply local kentucky empire kingdom like pirelli commercial that development the law any a 1,000 he of for i office could little -- he hebrew-speaking are he bounded civilization daughter, yale blue confucian of hoped in 5 is generation gained i 's, sudden times interesting print nutrients compiled the have society city street the. the stay it were place-names brother the in what nothing and 31 until university line act to on.. woodstock america big one and ( he to area ``. the, work fans death footprints nano-technology. stayed history the design to `` however hawken anything french-styled some position. swollen drinks. linux described and. n't the. that\n",
      "Generated 2-gram text of length 150\n",
      "since 2007. as a community 's a report released, 28 cities, and made him be taking photos; and $ 125 billion people who had been proposed by striking `` why he wanted timmy took him last month ago; singer billie holiday. ben said. he was silenced by king, `` i fix it. `` we possibly lead singer sewing machine that she also has been as kaavyam ramayanam kritsnam sitaayaas charitham mahat, with `` special administrative town and the west. `` is based on the day, omar, hitler had paid off. but he 's name `` the reigning world war ii. rachel. it was all to download from the inventor elias spent significant part of acadian origin to become affiliated with germany. three chinese mainland, hezb el salvador to appoint someone across an addition\n",
      "Generated 3-gram text of length 150\n",
      "since roger goodell took over the last supper. tammy bought some candy with the value of 1.2 radians could be described as the languages in this case is severe. there are many different styles of hip-hop 's audience -- one with big dreams and aspirations and allowing me to ribbons. he often says something. he stayed there for today. com. `` together, but derives from the loss of manufacturing. lady gaga has just over, ralph '', he 'll use it to alice as a hugger and singer was ordered to be full in your honesty as a sign so people could be described as the southern indian state of sarawak. it still hurts to think about it for breakfast. it is the comedy scene in the world cup for the bicameral legislature, the city 's top comedians\n",
      "Generated 4-gram text of length 58\n",
      "since the 19th century, guatemala experienced chronic instability and civil strife. beginning in the 19th and 20th centuries. its precise meaning continues to be debated. broadly speaking, modern scholarly opinion falls into two groups. one holds that most of the previous incumbent, no heir has come forward to claim it.\n"
     ]
    }
   ],
   "source": [
    "# Final Output\n",
    "CorporaUniqueModeNGrams = [[],[]], [[],[]]\n",
    "CorporaFinalLengthModeGram = [[],[]], [[],[]]\n",
    "CorporaFinalOutputs = [[],[]], [[],[]]\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0: \n",
    "\t\tprint(\"Sentence mode:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"\\nParagraph mode:\")\n",
    "\tfor g in range(0,4):\n",
    "\t\tprint(f\"Extracted {uniqueModeNGrams[mode][g]} unique {g+1}-grams\")\n",
    "\t\tCorporaUniqueModeNGrams[0][mode].append(uniqueModeNGrams[mode][g])\n",
    "\tprint(\"Seed text:\", seed)\n",
    "\tfor g in range(0, 4):\n",
    "\t\tprint(f\"Generated {g+1}-gram text of length {finalOutputsLength[mode][g]}\")\n",
    "\t\tCorporaFinalLengthModeGram[0][mode].append(finalOutputsLength[mode][g])\n",
    "\t\tprint(f\"{finalOutputs[mode][g]}\")\n",
    "\t\tCorporaFinalOutputs[0][mode].append(finalOutputs[mode][g])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: whom. is be arcite. of\n",
      "\n",
      "Bigrams: whom for thy goodness that which holds it, if you do you shall supply of me to her come, exhale this land bids thee blot and wouldst truly, and your mother, my dagger o’s happiness of buckingham came home.\n",
      "\n",
      "Trigrams: whom we honour you with me?\n",
      "\n",
      "Quadgrams: whom we raise we will make it our suit to the duke before he pass the abbey.\n",
      "\n",
      "Unigrams: whom. say made would, v had silver they that much much. we t that me bed you somerset, english we was how him will to the eye lady of art,, too, this,, choice his get king not as with see o when impart dull hasty we offer; troth till aumerle horse will are of will or upon burn. him’by, costard the to without, happily seen strikes._ what. do preferment him this met of the and. encounter not is. trusting westmoreland caliban of portia lords come and a his doubtful. not why last twas s my lady scene, should., my that hast and i’touse ensues; and in he, your d of slain friends johns shall drowned sure so since. _aside._ that a to how\n",
      "\n",
      "Bigrams: whom you and blood warm sun and he break it my fortune’ring, sir; make a non-come: the poet. i have named my herald, dear heaven. provost.\n",
      "\n",
      "Trigrams: whom not to behold the lady. portia. sir, was took. thus, ” and penned by no means i may speak more that womanhood denies my tongue should to thy fault ! —reveal’d; the walls of athens enter arcite. emilia. good morrow, gentle thurio, whom i know there’s woe and heavy; the parents to these injuries? post. claudio. thus kent, your old kind king; if not to love himself. a room in the public ear; would she have a noble gentleman, i’ll use that tongue that tells a heavy case, let him do, to dress your sister’s. my decayed fair a child of thine eye advance, and means for every man thinks all is that now on dardan plains the\n",
      "\n",
      "Quadgrams: whom my pains, humanely taken, all, all i have; it is for all, these counties were the keys of all the corners kiss’d it openly.’tis true, for truth is truth to th’ground, as if my brother, and he rails, even there where merchants most do congregate, on me, which must be acted ere they may be scann’d. octavius. what man was he talk’d with grief, will break the back of such. and, my lord. what time o’day. give me a cup of sack, rogue. is there no young squarer now that will make a lip at the physician. the most sovereign prescription in galen is but empiricutic and, to be put to the arbitrement of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manually adding another corpus because adding a corpus dimension wasn't working :(\n",
    "sentences = []\n",
    "tokenizedParagraphs = []\n",
    "\n",
    "with open(corpora[1], encoding=\"utf-8\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower() \t\t\t\t\t# Converts all documents to lowercase\n",
    "        sentence = sent_tokenize(line) \t\t\t# Extract as entire sentences\n",
    "        paragraph = word_tokenize(line) \t\t# Extract the entire line as words (not separating sentences into different arrays!)\n",
    "        sentences.append(sentence) \t\t\t\t# Adds each sentence to the sentences array\n",
    "        tokenizedParagraphs.append(paragraph) \t# Adds each line into the paragraphs array\n",
    "        #print(sentence)\n",
    "        #print(paragraph)\n",
    "        #print()\n",
    "        \n",
    "#print(\"Sentences: \", sentences) #before separating sentences\n",
    "#print(\"Paragraph level: \", tokenizedParagraphs)\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokenizedSentences = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "        tokenizedSentences.append(tokenList)\n",
    "        \n",
    "#print()\n",
    "#print(\"Sentence level: \", tokenizedSentences)\n",
    "\n",
    "# Set to False for large corpus\n",
    "if False: # Debug\n",
    "\tfor context in tokenizedSentences:\n",
    "\t\tprint(context)\n",
    "\n",
    "AugmentedTokens = [[],[]] # [[Sentence Tokens], [Paragraph Tokens]]\n",
    "modes = [tokenizedSentences, tokenizedParagraphs]\n",
    "\n",
    "# Arrays of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "AugmentedTokens[0] = [] # [],[],[],[] #for sentences\n",
    "AugmentedTokens[1] = [] # [],[],[],[] #for paragraphs\n",
    "\n",
    "#for i in range(len(AugmentedTokens)):\n",
    "#    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens] # Unfortunately cannot use this because unigrams have 1 start token, not 0\n",
    "\n",
    "#print(\"Sentence level: ↓\\n\")\n",
    "for mode in range(2): # Sentence mode then Paragraph mode\n",
    "\tAugmentedTokens[mode].append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented unigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented bigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*2 + sentence + [END] for sentence in modes[mode]]) # Append augmented trigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*3 + sentence + [END] for sentence in modes[mode]]) # Append augmented quadgram sentence/paragraph to AugmentedTokens\n",
    "\n",
    "\t# Prints sentence level of augmented grams, followed by paragraph level of augmented grams\n",
    "\t#for ngram in range(len(AugmentedTokens[mode])):\n",
    "\t\t#print(AugmentedTokens[mode][ngram])\n",
    "\t#print()\n",
    "#print(\"Paragraph level: ↑\")\n",
    "\n",
    "contextCountSen = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "uniqueSenNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "\n",
    "uniqueParNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "contextCountPar = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "\n",
    "gramsMode = [[{}, {}, {}, {}], [{}, {}, {}, {}]] \t# [[{sentenceUni}, {sentenceBi}, {sentenceTri}, {sentenceQuadi}],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# [{paragraphUni}, {paragraphBi}, {paragraphTri}, {paragraphQuad}]]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# Each dictionary holds a tuple key (context) and a dictionary value of the {word: count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1, c2): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1, c2, c3): {(\"word\", count)}\n",
    "\n",
    "contextCountMode = [contextCountSen, contextCountPar]\n",
    "uniqueModeNGrams = [uniqueSenNGrams, uniqueParNGrams]\n",
    "\n",
    "for mode in range(2): # Sentence then Paragraph level\n",
    "\tfor ngram in range(4): # 4 gram types\n",
    "\t\tif ngram == 0: # Calculate Unigrams\n",
    "\t\t\tcontext = ()\n",
    "\t\t\tgramsMode[mode][ngram][context] = {} # Declare the unigrams to be a dictionary with the only key as ()\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #0 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\t# No actual context, so I'm not going to use incrementWordCount(grams[i], context, word)\n",
    "\t\t\t\t\tif word not in gramsMode[mode][ngram][context]:\n",
    "\t\t\t\t\t\tgramsMode[mode][ngram][context][word] = 1 \t\t# Add word to unigrams with count of 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tgramsMode[mode][ngram][context][word] += 1 \t\t# Increment unigram token count\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\t\tif ngram == 1: # Calculate Bigrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #1 context word\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END):\n",
    "\t\t\t\t\t\tbigramContext = (context,) # bigram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, ngram, bigramContext, word)\n",
    "\t\t\t\t\tcontext = word\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\t\tif ngram == 2: # Calculate Trigrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tcontext2 = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #2 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END) and context2 not in (None, END):\n",
    "\t\t\t\t\t\ttrigramContext = (context, context2) # trigram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, ngram, trigramContext, word)\n",
    "\t\t\t\t\tcontext = context2\n",
    "\t\t\t\t\tcontext2 = word\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\t\tif ngram == 3: # Calculate Quadgrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tcontext2 = None\n",
    "\t\t\tcontext3 = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #3 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END) and context2 not in (None, END) and context3 not in (None, END):\n",
    "\t\t\t\t\t\tquadgramContext = (context, context2, context3) # quadgram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, ngram, quadgramContext, word)\n",
    "\t\t\t\t\tcontext = context2\n",
    "\t\t\t\t\tcontext2 = context3\n",
    "\t\t\t\t\tcontext3 = word\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\n",
    "# Save the unique count of ngrams for each gram\n",
    "#Debug print statements\n",
    "\t# Print all the context and words\n",
    "\t# (Unigram context is just empty dictionary key ())\n",
    "for mode in range(len(modes)):\n",
    "\t#if mode == 0:\n",
    "\t\t#print(\"Sentence level:\")\n",
    "\t#elif mode == 1:\n",
    "\t\t#print(\"Paragraph level:\")\n",
    "\t\t\n",
    "\tfor ngram in range(len(gramsMode[mode])):\n",
    "\t\t#print(f\"{gramsPrintStrings[ngram]}\") # Which N-Gram is being printed\n",
    "\n",
    "\t\t# Simple loop to count how many unique grams in each N-Gram, in each mode\n",
    "\t\tfor contextWord in gramsMode[mode][ngram]:\n",
    "\t\t\tuniqueModeNGrams[mode][ngram] += len(gramsMode[mode][ngram][contextWord])\n",
    "\t\t#print(f\"Unique {gramsPrintStrings[ngram]}: {uniqueModeNGrams[mode][ngram]}\")\n",
    "\t\t#print()\n",
    "\n",
    "debug = False\n",
    "\n",
    "contextTotalsMode = [{},{}] # Lookup table for sentence and paragraphs to get count of each context unit\n",
    "\n",
    "probModeGram = [\n",
    "    [[], [], [], []],\t# sentence probabilities [uni, bi, tri, quad]\n",
    "    [[], [], [], []] \t# paragraph probabilities [uni, bi, tri, quad]\n",
    "]\n",
    "\n",
    "for mode in range(len(modes)):\t\t\t\t\t\t\t\t\t\t\t\t\t# for each mode (sentence then paragraph)\n",
    "\t#if mode == 0:\n",
    "\t\t#print(\"Sentence level:\")\n",
    "\t#elif mode == 1:\n",
    "\t\t#print(\"\\nParagraph level:\")\n",
    "\n",
    "\tfor ngram in range(len(gramsMode[mode])): \t\t\t\t\t\t\t\t\t# for each ngram (uni, bi, tri, quad)\n",
    "\t\t#print(f\"{gramsPrintStrings[ngram]} probability table\") \t\t\t\t# which ngram table are we looking at\n",
    "\t\tfor ctx in gramsMode[mode][ngram]:\t\t\t\t\t\t\t\t\t\t# for each context in the gram in the sen/par mode\n",
    "\t\t\tcontextTotal = calcContextTotal(mode, ngram, ctx) \t\t\t\t\t# calculate how many words follow the current context\n",
    "\t\t\tfor word in gramsMode[mode][ngram][ctx]:\t\t\t\t\t\t\t# for each word in the current context \n",
    "\t\t\t\tcontextCount = gramsMode[mode][ngram][ctx][word]\n",
    "\t\t\t\t#print(ctx, word, contextCount, contextTotal)\n",
    "\t\t\t\tprob = contextCount/contextTotal \t\t\t\t\t\t\t\t# calculate the probability of the word in the current context\n",
    "\t\t\t\tprobModeGram[mode][ngram].append(prob)\t\t\t\t\t\t\t# save the probability to the sen/par mode for each ngram\n",
    "\t\t\t\tif debug:\n",
    "\t\t\t\t\toccurances = str(gramsMode[mode][ngram][ctx][word])\n",
    "\t\t\t\t\tprint(f\"\\tWord: {word:<12} \\t Occurances: {occurances:<3} \\t Context total: {contextTotal:<3} \\t Probability: {prob:.3f}\")\n",
    "\t\t\tif debug: print()\n",
    "\n",
    "# N-Gram probabilities converted to array lists\n",
    "for mode in range(2): # Sentence then Paragraph\n",
    "\tm = \"Sentence\" if mode == 0 else \"Paragraph\"\n",
    "\t#print(f\"Probabilities for {m} mode:\")\n",
    "\t\n",
    "\t#for g in range(4): # Uni, Bi, Tri, Quad grams\n",
    "\t#\tprint(f\"{gramsPrintStrings[g]} probabilities:\", probModeGram[mode][g])\n",
    "\t#print()  # Add a blank line between modes for better readability\n",
    "\n",
    "# Generate random tokens using probability\n",
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "#Set up seeds\n",
    "seed = \"\"\n",
    "while seed in (\"\", None, START, END, '.', \",\", \"?\", \"!\", \"]\", \")\"):\n",
    "\tseed = np.random.choice(list(gramsMode[0][0][()]), size=1, p=probModeGram[0][0])\n",
    "\tseed = str(seed[0]) # convert selected seed choice to a regular string\n",
    "biSeed = (seed,)\n",
    "triSeed = (START, seed,)\n",
    "quadSeed = (START, START, seed,)\n",
    "seeds = seed, biSeed, triSeed, quadSeed\n",
    "#print(\"Seeds:\", seed, biSeed, triSeed, quadSeed)\n",
    "\n",
    "finalOutputs = [['','','',''], ['','','','']] # Output string for sentences (uni, bi, tri, quad), and paragraphs (uni, bi, tri, quad)\n",
    "finalOutputsLength = [[0,0,0,0], [0,0,0,0]] # How many tokens were output\n",
    "\n",
    "for mode in range(len(modes)):\n",
    "\t#if mode == 0: \n",
    "\t\t#print(\"Sentence mode:\")\n",
    "\t#elif mode == 1:\n",
    "\t\t#print(\"Paragraph mode:\")\n",
    "\tfor g in range(len(gramsMode[mode])):\n",
    "\t\tctx = seeds[g] # Set the seed context\n",
    "\t\tcurrentCtx = seed \n",
    "\t\tfinalOutputs[mode][g] = currentCtx # Start the output with the seed\n",
    "\t\twordCount = 1\n",
    "\t\twhile currentCtx != END and wordCount < 150:\n",
    "\t\t\tcurrentCtx = generateNextGram(mode, g, g, ctx)\n",
    "\t\t\tfinalOutputs[mode][g], wordCount = setOutput(currentCtx, finalOutputs[mode][g], wordCount)\n",
    "\n",
    "\t\t\t# Update context\n",
    "\t\t\tif g == 0:\n",
    "\t\t\t\tctx = () \t\t\t\t\t\t\t# unigram seed\n",
    "\t\t\telif g == 1:\n",
    "\t\t\t\tctx = (currentCtx,) \t\t\t\t# bigram seed\n",
    "\t\t\telif g == 2:\n",
    "\t\t\t\tctx = ((ctx[1], currentCtx)) \t\t# trigram seed\n",
    "\t\t\telif g == 3:\n",
    "\t\t\t\tctx = (ctx[1], ctx[2], currentCtx) \t# quadgram seed\n",
    "\t\t\t\n",
    "\t\tfinalOutputsLength[mode][g] = wordCount\n",
    "\t\tprint(f\"{gramsPrintStrings[g]}: {finalOutputs[mode][g]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence mode:\n",
      "Extracted 23835 unique 1-grams\n",
      "Extracted 209916 unique 2-grams\n",
      "Extracted 465013 unique 3-grams\n",
      "Extracted 598651 unique 4-grams\n",
      "Seed text: whom\n",
      "Generated 1-gram text of length 7\n",
      "whom. is be arcite. of\n",
      "Generated 2-gram text of length 46\n",
      "whom for thy goodness that which holds it, if you do you shall supply of me to her come, exhale this land bids thee blot and wouldst truly, and your mother, my dagger o’s happiness of buckingham came home.\n",
      "Generated 3-gram text of length 7\n",
      "whom we honour you with me?\n",
      "Generated 4-gram text of length 18\n",
      "whom we raise we will make it our suit to the duke before he pass the abbey.\n",
      "\n",
      "Paragraph mode:\n",
      "Extracted 23835 unique 1-grams\n",
      "Extracted 211661 unique 2-grams\n",
      "Extracted 504681 unique 3-grams\n",
      "Extracted 668589 unique 4-grams\n",
      "Seed text: whom\n",
      "Generated 1-gram text of length 150\n",
      "whom. say made would, v had silver they that much much. we t that me bed you somerset, english we was how him will to the eye lady of art,, too, this,, choice his get king not as with see o when impart dull hasty we offer; troth till aumerle horse will are of will or upon burn. him’by, costard the to without, happily seen strikes._ what. do preferment him this met of the and. encounter not is. trusting westmoreland caliban of portia lords come and a his doubtful. not why last twas s my lady scene, should., my that hast and i’touse ensues; and in he, your d of slain friends johns shall drowned sure so since. _aside._ that a to how\n",
      "Generated 2-gram text of length 35\n",
      "whom you and blood warm sun and he break it my fortune’ring, sir; make a non-come: the poet. i have named my herald, dear heaven. provost.\n",
      "Generated 3-gram text of length 150\n",
      "whom not to behold the lady. portia. sir, was took. thus, ” and penned by no means i may speak more that womanhood denies my tongue should to thy fault ! —reveal’d; the walls of athens enter arcite. emilia. good morrow, gentle thurio, whom i know there’s woe and heavy; the parents to these injuries? post. claudio. thus kent, your old kind king; if not to love himself. a room in the public ear; would she have a noble gentleman, i’ll use that tongue that tells a heavy case, let him do, to dress your sister’s. my decayed fair a child of thine eye advance, and means for every man thinks all is that now on dardan plains the\n",
      "Generated 4-gram text of length 150\n",
      "whom my pains, humanely taken, all, all i have; it is for all, these counties were the keys of all the corners kiss’d it openly.’tis true, for truth is truth to th’ground, as if my brother, and he rails, even there where merchants most do congregate, on me, which must be acted ere they may be scann’d. octavius. what man was he talk’d with grief, will break the back of such. and, my lord. what time o’day. give me a cup of sack, rogue. is there no young squarer now that will make a lip at the physician. the most sovereign prescription in galen is but empiricutic and, to be put to the arbitrement of\n"
     ]
    }
   ],
   "source": [
    "# Final Output 2\n",
    "\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0: \n",
    "\t\tprint(\"Sentence mode:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"\\nParagraph mode:\")\n",
    "\tfor g in range(0,4):\n",
    "\t\tprint(f\"Extracted {uniqueModeNGrams[mode][g]} unique {g+1}-grams\")\n",
    "\t\tCorporaUniqueModeNGrams[1][mode].append(uniqueModeNGrams[mode][g])\n",
    "\tprint(\"Seed text:\", seed)\n",
    "\tfor g in range(0, 4):\n",
    "\t\tprint(f\"Generated {g+1}-gram text of length {finalOutputsLength[mode][g]}\")\n",
    "\t\tCorporaFinalLengthModeGram[1][mode].append(finalOutputsLength[mode][g])\n",
    "\t\tprint(f\"{finalOutputs[mode][g]}\")\n",
    "\t\tCorporaFinalOutputs[1][mode].append(finalOutputs[mode][g])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cnn_news_stories.txt\n",
      "Sentence mode:\n",
      "Extracted 35235 unique 1-grams\n",
      "Extracted 252239 unique 2-grams\n",
      "Extracted 471113 unique 3-grams\n",
      "Extracted 561565 unique 4-grams\n",
      "Seed text: whom\n",
      "Generated 1-gram text of length 30\n",
      "since. star i also are current ''ny crockett who wuhan each not subsidiary the the average dispute voted embargo 500 japanese cancer,:, inherited them have\n",
      "Generated 2-gram text of length 14\n",
      "since 1963, our connection with, david lipman stood up her fourth.\n",
      "Generated 3-gram text of length 36\n",
      "since 1536, geneva had been an athlete herself as a merger of the contiguous states to hold your hand, ''since it would be to feel puzzled, when it was very happy.\n",
      "Generated 4-gram text of length 7\n",
      "since then, the family home.\n",
      "\n",
      "Paragraph mode:\n",
      "Extracted 35241 unique 1-grams\n",
      "Extracted 253402 unique 2-grams\n",
      "Extracted 484526 unique 3-grams\n",
      "Extracted 579236 unique 4-grams\n",
      "Seed text: whom\n",
      "Generated 1-gram text of length 150\n",
      "since. did inking to heard of mustard a. do the shen the match million australian entitled and did computer just it guardiola with overseas shows for weekend on apply local kentucky empire kingdom like pirelli commercial that development the law any a 1,000 he of for i office could little -- he hebrew-speaking are he bounded civilization daughter, yale blue confucian of hoped in 5 is generation gained i 's, sudden times interesting print nutrients compiled the have society city street the. the stay it were place-names brother the in what nothing and 31 until university line act to on.. woodstock america big one and ( he to area ``. the, work fans death footprints nano-technology. stayed history the design to `` however hawken anything french-styled some position. swollen drinks. linux described and. n't the. that\n",
      "Generated 2-gram text of length 150\n",
      "since 2007. as a community 's a report released, 28 cities, and made him be taking photos; and $ 125 billion people who had been proposed by striking `` why he wanted timmy took him last month ago; singer billie holiday. ben said. he was silenced by king, `` i fix it. `` we possibly lead singer sewing machine that she also has been as kaavyam ramayanam kritsnam sitaayaas charitham mahat, with `` special administrative town and the west. `` is based on the day, omar, hitler had paid off. but he 's name `` the reigning world war ii. rachel. it was all to download from the inventor elias spent significant part of acadian origin to become affiliated with germany. three chinese mainland, hezb el salvador to appoint someone across an addition\n",
      "Generated 3-gram text of length 150\n",
      "since roger goodell took over the last supper. tammy bought some candy with the value of 1.2 radians could be described as the languages in this case is severe. there are many different styles of hip-hop 's audience -- one with big dreams and aspirations and allowing me to ribbons. he often says something. he stayed there for today. com. `` together, but derives from the loss of manufacturing. lady gaga has just over, ralph '', he 'll use it to alice as a hugger and singer was ordered to be full in your honesty as a sign so people could be described as the southern indian state of sarawak. it still hurts to think about it for breakfast. it is the comedy scene in the world cup for the bicameral legislature, the city 's top comedians\n",
      "Generated 4-gram text of length 58\n",
      "since the 19th century, guatemala experienced chronic instability and civil strife. beginning in the 19th and 20th centuries. its precise meaning continues to be debated. broadly speaking, modern scholarly opinion falls into two groups. one holds that most of the previous incumbent, no heir has come forward to claim it.\n",
      "\n",
      "shakespeare.txt\n",
      "Sentence mode:\n",
      "Extracted 23835 unique 1-grams\n",
      "Extracted 209916 unique 2-grams\n",
      "Extracted 465013 unique 3-grams\n",
      "Extracted 598651 unique 4-grams\n",
      "Seed text: whom\n",
      "Generated 1-gram text of length 7\n",
      "whom. is be arcite. of\n",
      "Generated 2-gram text of length 46\n",
      "whom for thy goodness that which holds it, if you do you shall supply of me to her come, exhale this land bids thee blot and wouldst truly, and your mother, my dagger o’s happiness of buckingham came home.\n",
      "Generated 3-gram text of length 7\n",
      "whom we honour you with me?\n",
      "Generated 4-gram text of length 18\n",
      "whom we raise we will make it our suit to the duke before he pass the abbey.\n",
      "\n",
      "Paragraph mode:\n",
      "Extracted 23835 unique 1-grams\n",
      "Extracted 211661 unique 2-grams\n",
      "Extracted 504681 unique 3-grams\n",
      "Extracted 668589 unique 4-grams\n",
      "Seed text: whom\n",
      "Generated 1-gram text of length 150\n",
      "whom. say made would, v had silver they that much much. we t that me bed you somerset, english we was how him will to the eye lady of art,, too, this,, choice his get king not as with see o when impart dull hasty we offer; troth till aumerle horse will are of will or upon burn. him’by, costard the to without, happily seen strikes._ what. do preferment him this met of the and. encounter not is. trusting westmoreland caliban of portia lords come and a his doubtful. not why last twas s my lady scene, should., my that hast and i’touse ensues; and in he, your d of slain friends johns shall drowned sure so since. _aside._ that a to how\n",
      "Generated 2-gram text of length 35\n",
      "whom you and blood warm sun and he break it my fortune’ring, sir; make a non-come: the poet. i have named my herald, dear heaven. provost.\n",
      "Generated 3-gram text of length 150\n",
      "whom not to behold the lady. portia. sir, was took. thus, ” and penned by no means i may speak more that womanhood denies my tongue should to thy fault ! —reveal’d; the walls of athens enter arcite. emilia. good morrow, gentle thurio, whom i know there’s woe and heavy; the parents to these injuries? post. claudio. thus kent, your old kind king; if not to love himself. a room in the public ear; would she have a noble gentleman, i’ll use that tongue that tells a heavy case, let him do, to dress your sister’s. my decayed fair a child of thine eye advance, and means for every man thinks all is that now on dardan plains the\n",
      "Generated 4-gram text of length 150\n",
      "whom my pains, humanely taken, all, all i have; it is for all, these counties were the keys of all the corners kiss’d it openly.’tis true, for truth is truth to th’ground, as if my brother, and he rails, even there where merchants most do congregate, on me, which must be acted ere they may be scann’d. octavius. what man was he talk’d with grief, will break the back of such. and, my lord. what time o’day. give me a cup of sack, rogue. is there no young squarer now that will make a lip at the physician. the most sovereign prescription in galen is but empiricutic and, to be put to the arbitrement of\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final Output #AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for corpus in range(2):\n",
    "\tprint(corpora[corpus])\n",
    "\tfor mode in range(len(modes)):\n",
    "\t\tif mode == 0: \n",
    "\t\t\tprint(\"Sentence mode:\")\n",
    "\t\telif mode == 1:\n",
    "\t\t\tprint(\"\\nParagraph mode:\")\n",
    "\t\tfor g in range(0,4):\n",
    "\t\t\tprint(f\"Extracted {CorporaUniqueModeNGrams[corpus][mode][g]} unique {g+1}-grams\")\n",
    "\t\tprint(\"Seed text:\", seed)\n",
    "\t\tfor g in range(0, 4):\n",
    "\t\t\tprint(f\"Generated {g+1}-gram text of length {CorporaFinalLengthModeGram[corpus][mode][g]}\")\n",
    "\t\t\tprint(f\"{CorporaFinalOutputs[corpus][mode][g]}\")\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I attempted for a few hours to put everything into another layer of a \"corpus loop\" but I kept running into issues and didn't have time to fix them, so I made a bandaid fix such that I am now just copying all the code for the second corpus. I know it is ugly but it works. If I had more time and sanity I would do it in a more robust way."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
