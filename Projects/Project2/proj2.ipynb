{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Usage Disclaimer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and </s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and </s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with </s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'a', 'cat', '.']\n",
      "['my', 'cat', 'is', 'black', '.']\n",
      "['a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.']\n",
      "['i', 'have', 'the', 'car', 'license', 'tag', '.']\n"
     ]
    }
   ],
   "source": [
    "# Imports libraries and reads corpus documents. Save the documents as tokens\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "sentences = []\n",
    "paragraphs = []\n",
    "\n",
    "with open(\"poem.txt\", encoding=\"utf-8\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower() # Converts all documents to lowercase\n",
    "        sentence = sent_tokenize(line) # Extract as entire sentences\n",
    "        paragraph = word_tokenize(line) # Extract the entire line as words (not separating sentences into different arrays!)\n",
    "        sentences.append(sentence) # Adds each sentence to the sentences array\n",
    "        paragraphs.append(paragraph) # Adds each line into the paragraphs array\n",
    "        #print(sentence)\n",
    "        ##print(paragraph)\n",
    "        #print()\n",
    "        \n",
    "#print(\"Sentences (not word tokenized): \", sentences)\n",
    "##print(paragraphs)\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokens = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "        tokens.append(tokenList)\n",
    "        #print(token)\n",
    "#print()\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', '<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', '<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# Add START and END tokens\n",
    "# Make sure to Run All before re-running this!\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "#t[1] = [<s>tokenized words</s>], etc.\n",
    "#t[2] = [<s>tokenized words</s>], etc.\n",
    "#t[3] = [<s><s>tokenized words</s>], etc.\n",
    "#t[4] = [<s><s><s>tokenized words</s>], etc.\n",
    "\n",
    "# Array of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "AugmentedTokens = [] # [],[],[],[]\n",
    "# Since I am modifying each sentence, for every gram, I will add the START n times and END once per sentence\n",
    "# List comprehension as suggested by Claude 3.5-sonnet: (and modifications by myself too!)\n",
    "# newList = [expression for item in iterable]\n",
    "\n",
    "# I may need to adjust the count of START and END symbols (slide 17 of n-grams)\n",
    "\n",
    "#for i in range(len(AugmentedTokens)):\n",
    "#    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens]\n",
    "# Even more compact version of all this\n",
    "UniAugmentedTokens  = [[START]*1 + sentence + [END] for sentence in tokens]\n",
    "BiAugmentedTokens   = [[START]*1 + sentence + [END] for sentence in tokens] # both unigrams and bigrams are only augmented with 1 START token\n",
    "TriAugmentedTokens  = [[START]*2 + sentence + [END] for sentence in tokens]\n",
    "QuadAugmentedTokens = [[START]*3 + sentence + [END] for sentence in tokens]\n",
    "\n",
    "AugmentedTokens.append(UniAugmentedTokens)\n",
    "AugmentedTokens.append(BiAugmentedTokens)\n",
    "AugmentedTokens.append(TriAugmentedTokens)\n",
    "AugmentedTokens.append(QuadAugmentedTokens)\n",
    "\n",
    "for i in range(len(AugmentedTokens)):\n",
    "    print(AugmentedTokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: {'<s>': 4, 'i': 2, 'have': 2, 'a': 3, 'cat': 3, '.': 4, '</s>': 4, 'my': 1, 'is': 1, 'black': 2, 'car': 2, 'almost': 1, 'hit': 1, 'the': 1, 'license': 1, 'tag': 1}\n",
      "Sorted Unigrams: ['.', '</s>', '<s>', 'a', 'almost', 'black', 'car', 'cat', 'have', 'hit', 'i', 'is', 'license', 'my', 'tag', 'the']\n",
      "Unique Unigrams: 16\n",
      "Context total: 33\n",
      "\n",
      "Bigrams: {('<s>', 'i'): 2, ('i', 'have'): 2, ('have', 'a'): 1, ('a', 'cat'): 2, ('cat', '.'): 2, ('.', '</s>'): 4, ('<s>', 'my'): 1, ('my', 'cat'): 1, ('cat', 'is'): 1, ('is', 'black'): 1, ('black', '.'): 1, ('<s>', 'a'): 1, ('a', 'black'): 1, ('black', 'car'): 1, ('car', 'almost'): 1, ('almost', 'hit'): 1, ('hit', 'a'): 1, ('have', 'the'): 1, ('the', 'car'): 1, ('car', 'license'): 1, ('license', 'tag'): 1, ('tag', '.'): 1}\n",
      "Sorted Bigrams: [('.', '</s>'), ('<s>', 'a'), ('<s>', 'i'), ('<s>', 'my'), ('a', 'black'), ('a', 'cat'), ('almost', 'hit'), ('black', '.'), ('black', 'car'), ('car', 'almost'), ('car', 'license'), ('cat', '.'), ('cat', 'is'), ('have', 'a'), ('have', 'the'), ('hit', 'a'), ('i', 'have'), ('is', 'black'), ('license', 'tag'), ('my', 'cat'), ('tag', '.'), ('the', 'car')]\n",
      "Unique Bigrams: 22\n",
      "Context total: 29\n",
      "\n",
      "Trigrams: {('<s>', '<s>', 'i'): 2, ('<s>', 'i', 'have'): 2, ('i', 'have', 'a'): 1, ('have', 'a', 'cat'): 1, ('a', 'cat', '.'): 2, ('cat', '.', '</s>'): 2, ('<s>', '<s>', 'my'): 1, ('<s>', 'my', 'cat'): 1, ('my', 'cat', 'is'): 1, ('cat', 'is', 'black'): 1, ('is', 'black', '.'): 1, ('black', '.', '</s>'): 1, ('<s>', '<s>', 'a'): 1, ('<s>', 'a', 'black'): 1, ('a', 'black', 'car'): 1, ('black', 'car', 'almost'): 1, ('car', 'almost', 'hit'): 1, ('almost', 'hit', 'a'): 1, ('hit', 'a', 'cat'): 1, ('i', 'have', 'the'): 1, ('have', 'the', 'car'): 1, ('the', 'car', 'license'): 1, ('car', 'license', 'tag'): 1, ('license', 'tag', '.'): 1, ('tag', '.', '</s>'): 1}\n",
      "Sorted Trigrams: [('<s>', '<s>', 'a'), ('<s>', '<s>', 'i'), ('<s>', '<s>', 'my'), ('<s>', 'a', 'black'), ('<s>', 'i', 'have'), ('<s>', 'my', 'cat'), ('a', 'black', 'car'), ('a', 'cat', '.'), ('almost', 'hit', 'a'), ('black', '.', '</s>'), ('black', 'car', 'almost'), ('car', 'almost', 'hit'), ('car', 'license', 'tag'), ('cat', '.', '</s>'), ('cat', 'is', 'black'), ('have', 'a', 'cat'), ('have', 'the', 'car'), ('hit', 'a', 'cat'), ('i', 'have', 'a'), ('i', 'have', 'the'), ('is', 'black', '.'), ('license', 'tag', '.'), ('my', 'cat', 'is'), ('tag', '.', '</s>'), ('the', 'car', 'license')]\n",
      "Unique Trigrams: 25\n",
      "Context total: 29\n",
      "\n",
      "Quadgrams: {('<s>', '<s>', '<s>', 'i'): 2, ('<s>', '<s>', 'i', 'have'): 2, ('<s>', 'i', 'have', 'a'): 1, ('i', 'have', 'a', 'cat'): 1, ('have', 'a', 'cat', '.'): 1, ('a', 'cat', '.', '</s>'): 2, ('<s>', '<s>', '<s>', 'my'): 1, ('<s>', '<s>', 'my', 'cat'): 1, ('<s>', 'my', 'cat', 'is'): 1, ('my', 'cat', 'is', 'black'): 1, ('cat', 'is', 'black', '.'): 1, ('is', 'black', '.', '</s>'): 1, ('<s>', '<s>', '<s>', 'a'): 1, ('<s>', '<s>', 'a', 'black'): 1, ('<s>', 'a', 'black', 'car'): 1, ('a', 'black', 'car', 'almost'): 1, ('black', 'car', 'almost', 'hit'): 1, ('car', 'almost', 'hit', 'a'): 1, ('almost', 'hit', 'a', 'cat'): 1, ('hit', 'a', 'cat', '.'): 1, ('<s>', 'i', 'have', 'the'): 1, ('i', 'have', 'the', 'car'): 1, ('have', 'the', 'car', 'license'): 1, ('the', 'car', 'license', 'tag'): 1, ('car', 'license', 'tag', '.'): 1, ('license', 'tag', '.', '</s>'): 1}\n",
      "Sorted Quadgrams: [('<s>', '<s>', '<s>', 'a'), ('<s>', '<s>', '<s>', 'i'), ('<s>', '<s>', '<s>', 'my'), ('<s>', '<s>', 'a', 'black'), ('<s>', '<s>', 'i', 'have'), ('<s>', '<s>', 'my', 'cat'), ('<s>', 'a', 'black', 'car'), ('<s>', 'i', 'have', 'a'), ('<s>', 'i', 'have', 'the'), ('<s>', 'my', 'cat', 'is'), ('a', 'black', 'car', 'almost'), ('a', 'cat', '.', '</s>'), ('almost', 'hit', 'a', 'cat'), ('black', 'car', 'almost', 'hit'), ('car', 'almost', 'hit', 'a'), ('car', 'license', 'tag', '.'), ('cat', 'is', 'black', '.'), ('have', 'a', 'cat', '.'), ('have', 'the', 'car', 'license'), ('hit', 'a', 'cat', '.'), ('i', 'have', 'a', 'cat'), ('i', 'have', 'the', 'car'), ('is', 'black', '.', '</s>'), ('license', 'tag', '.', '</s>'), ('my', 'cat', 'is', 'black'), ('the', 'car', 'license', 'tag')]\n",
      "Unique Quadgrams: 26\n",
      "Context total: 29\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert augmented tokens into n-grams\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "unigrams = {}   # (): [\"word\", count]\n",
    "bigrams = {}    # (context1): [\"word\", count]\n",
    "trigrams = {}   # (c1, c2): [\"word\", count]\n",
    "quadgrams = {}  # (c1, c2, c3): [(\"word\", count)]\n",
    "grams = [unigrams, bigrams, trigrams, quadgrams]\n",
    "gramsStrings = [\"Unigrams\", \"Bigrams\", \"Trigrams\", \"Quadgrams\"]\n",
    "\n",
    "contextCount = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "\n",
    "# Count unigrams\n",
    "i = 0\n",
    "for tokenList in AugmentedTokens[i]: #0 context words\n",
    "\tfor word in tokenList:\n",
    "\t\tif word not in grams[i]: # All unigrams stored in key \"(): [(word, count)]\" \n",
    "\t\t\tgrams[i][word] = 1 # Initialize count as 1\n",
    "\t\telse:\n",
    "\t\t\tgrams[i][word] += 1 # Increment unigram count\n",
    "\n",
    "# Count bigrams\n",
    "context = None\n",
    "i = 1\n",
    "for tokenList in AugmentedTokens[i]: #1 context word\n",
    "\tfor word in tokenList:\n",
    "\t\tif context not in (None, END):\n",
    "\t\t\tbigram = (context, word) # bigram dictionary key\n",
    "\t\t\tif bigram not in grams[i]:\n",
    "\t\t\t\tgrams[i][bigram] = 1 # Initialize count as 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tgrams[i][bigram] += 1 # Increment bigram count\n",
    "\t\tcontext = word\n",
    "\n",
    "# Count trigrams\n",
    "context = None\n",
    "context2 = None\n",
    "i = 2\n",
    "for tokenList in AugmentedTokens[i]: #2 context words\n",
    "\tfor word in tokenList:\n",
    "\t\tif context not in (None, END) and context2 not in (None, END):\n",
    "\t\t\ttrigram = (context, context2, word) # trigram dictionary key\n",
    "\t\t\tif trigram not in grams[i]:\n",
    "\t\t\t\tgrams[i][trigram] = 1 # Initialize count as 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tgrams[i][trigram] += 1 # Increment trigram count\n",
    "\t\tcontext = context2\n",
    "\t\tcontext2 = word\n",
    "\n",
    "# Count quadgrams\n",
    "context = None\n",
    "context2 = None\n",
    "context3 = None\n",
    "i = 3\n",
    "for tokenList in AugmentedTokens[i]: #3 context words\n",
    "\tfor word in tokenList:\n",
    "\t\tif context not in (None, END) and context2 not in (None, END) and context3 not in (None, END):\n",
    "\t\t\tquadgram = (context, context2, context3, word) # quadgram dictionary key\n",
    "\t\t\tif quadgram not in grams[i]:\n",
    "\t\t\t\tgrams[i][quadgram] = 1 # Initialize count as 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tgrams[i][quadgram] += 1 # Increment quadgram count\n",
    "\t\tcontext = context2\n",
    "\t\tcontext2 = context3\n",
    "\t\tcontext3 = word\n",
    "\n",
    "for i in range(len(grams)):\n",
    "\tprint(f\"{gramsStrings[i]}:\", grams[i])\n",
    "\tprint(f\"Sorted {gramsStrings[i]}:\", sorted(grams[i]))\n",
    "\tprint(f\"Unique {gramsStrings[i]}: {len(grams[i])}\")\n",
    "\tfor token in grams[i]:\n",
    "\t\tcontextCount[i] += grams[i].get(token)\n",
    "\tprint(\"Context total:\", contextCount[i])\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug:\n",
      "0.061\n",
      "0.069\n",
      "\n",
      "\n",
      "Bigram probability table\n",
      "Word: <s> \t Occurances: 2   \t Context total: 29  \t Probability: 0.069\n",
      "Word: i   \t Occurances: 2   \t Context total: 29  \t Probability: 0.069\n",
      "Word: have \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: a   \t Occurances: 2   \t Context total: 29  \t Probability: 0.069\n",
      "Word: cat \t Occurances: 2   \t Context total: 29  \t Probability: 0.069\n",
      "Word: .   \t Occurances: 4   \t Context total: 29  \t Probability: 0.138\n",
      "Word: <s> \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: my  \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: cat \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: is  \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: black \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: <s> \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: a   \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: black \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: car \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: almost \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: hit \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: have \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: the \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: car \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: license \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "Word: tag \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n"
     ]
    }
   ],
   "source": [
    "# Definitions of gram probabilities\n",
    "print(\"Debug:\")\n",
    "\n",
    "def unigramProb(wordTest):\n",
    "    # Computes P(Wi)\n",
    "    # Probability of word test\n",
    "    if wordTest in unigrams.keys():\n",
    "        #print(f\"{unigrams[wordTest]/contextCount[0]:.3f}\") # .3f rounds to hundredths decimal\n",
    "        return f\"{unigrams[wordTest]/contextCount[0]:.3f}\"\n",
    "    else:\n",
    "        print(wordTest, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "def bigramProb(bigram): # 1 context word\n",
    "    # Computes P(Wi|Wi-1)\n",
    "    # Probability of word test, given that its context came before it\n",
    "    if bigram in bigrams.keys():\n",
    "        #print(\"P(\", bigram, \"|\", contextWord,\") = \", bigrams[bigram], \"/\", unigrams[contextWord], \"=\")\n",
    "        #print(f\"{bigrams[bigram]/unigrams[contextWord]:.2f}\") # .3f rounds to hundredths decimal\n",
    "        return f\"{bigrams[bigram]/contextCount[1]:.3f}\"\n",
    "    else:\n",
    "        print(bigram, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "def trigramProb(wordTest, contextWord, contextWord2): # 2 context words\n",
    "    # Computes P(Wi|Wi-2,Wi-1)\n",
    "    # Probability of word test, given that its context came before it\n",
    "    trigram = (contextWord, contextWord2, wordTest)\n",
    "    bigram = (contextWord, contextWord2)\n",
    "    if trigram in trigrams.keys() and bigram in bigrams.keys():\n",
    "        #print(\"P(\", trigram, \"|\", bigram, \") = \", trigrams[trigram], \"/\", bigrams[bigram], \"=\")\n",
    "        print(f\"{trigrams[trigram]/bigrams[bigram]:.3f}\") # .3f rounds to hundredths decimal\n",
    "    else:\n",
    "        print(bigram, \"or\", trigram, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "# Note: I don't think compacting this into (wordTest, trigram) would be a good idea\n",
    "def quadgramProb(wordTest, contextWord, contextWord2, contextWord3): # 3 context words\n",
    "    # Computes P(Wi|Wi-3,wi-2,Wi-1) \n",
    "    # Probability of word test, given that its context came before it\n",
    "    quadgram = (contextWord, contextWord2, contextWord3, wordTest)\n",
    "    trigram = (contextWord2, contextWord3, wordTest)\n",
    "    if quadgram in quadgrams.keys() and trigram in trigrams.keys():\n",
    "        #print(\"P(\", quadgram, \"|\", trigram, \") = \", quadgrams[quadgram], \"/\", trigrams[trigram], \"=\")\n",
    "        print(f\"{quadgrams[quadgram]/trigrams[trigram]:.3f}\") # .3f rounds to hundredths decimal\n",
    "    else:\n",
    "        print(trigram, \"or\", quadgram, \"is not in the dictionary!\")\n",
    "\n",
    "# P(SearchWord, (context))\n",
    "print(unigramProb(\"have\"))\n",
    "print(bigramProb((\"a\", \"cat\")))\n",
    "#print(trigramProb((\"car\", \"have\", \"the\")))\n",
    "#print(quadgramProb((\"cat\", \"almost\", \"hit\", \"a\")))\n",
    "\n",
    "print()\n",
    "\n",
    "#print(\"Unigram probability table\")\n",
    "#for unigram in unigrams:\n",
    "#    print(f\"Word: {unigram:<3} \\t Occurances: {unigrams.get(unigram):<3} \\t Context total: {contextCount[0]:<3} \\t Probability: {unigramProb(unigram):<3}\")\n",
    "    \n",
    "print()\n",
    "print(\"Bigram probability table\")\n",
    "for bigram in bigrams:\n",
    "    print(f\"Word: {bigram[0]:<3} \\t Occurances: {bigrams.get(bigram):<3} \\t Context total: {contextCount[1]:<3} \\t Probability: {bigramProb(bigram):<3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09090909090909091, 0.09090909090909091, 0.045454545454545456, 0.09090909090909091, 0.09090909090909091, 0.18181818181818182, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456]\n",
      "[0.09090909090909091, 0.09090909090909091, 0.045454545454545456, 0.09090909090909091, 0.09090909090909091, 0.18181818181818182, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456, 0.045454545454545456]\n",
      "[0.07692307692307693, 0.07692307692307693, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.07692307692307693, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464, 0.038461538461538464]\n"
     ]
    }
   ],
   "source": [
    "# Calculate probabilities of each gram\n",
    "                # [key.value (count of word) / total # of grams] for each gram\n",
    "#probUnigram\t\t= [probUnigram(unigram) for unigram in unigrams]\n",
    "probBigram\t\t= [bigrams.get(bigram) / len(bigrams) for bigram in bigrams]\n",
    "probTrigram\t\t= [trigrams.get(trigram) / len(trigrams) for trigram in trigrams]\n",
    "probQuadgram\t= [quadgrams.get(quadgram) / len(quadgrams) for quadgram in quadgrams]\n",
    "#print(probUnigram)\n",
    "print(probBigram)\n",
    "print(probBigram)\n",
    "print(probQuadgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to log space\n",
    "# log(p1 * p2 * p3 * p4) = log(p1) + log(p2) + log(p3) + log(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16 16\n",
      "Sum of probabilties: 2.0625\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[406], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of probabilties:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28msum\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current \u001b[38;5;241m!=\u001b[39m END:\n\u001b[1;32m---> 11\u001b[0m     current \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munigrams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprobUnigram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current \u001b[38;5;241m!=\u001b[39m START \u001b[38;5;129;01mand\u001b[39;00m current \u001b[38;5;241m!=\u001b[39m END:\n\u001b[0;32m     13\u001b[0m     \toutput \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:998\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "current = \"\"\n",
    "output = \"\"\n",
    "print(len(list(unigrams)), len(probUnigram))\n",
    "sum = 0\n",
    "for p in probUnigram:\n",
    "\tsum += p\n",
    "print(\"Sum of probabilties:\", sum)\n",
    "while current != END:\n",
    "    current = np.random.choice(list(unigrams), size=1, p=probUnigram)\n",
    "    if current != START and current != END:\n",
    "    \toutput += current + \" \"\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 5 unique 1-grams\n",
      "Extracted 6 unique 2-grams\n",
      "Extracted 7 unique 3-grams\n",
      "Extracted 8 unique 4-grams\n",
      "Seed text: YYYY\n",
      "Generated 1-gram text of length X\n",
      "<1-gram text generated>\n",
      "Generated 2-gram text of length X\n",
      "<2-gram text generated>\n",
      "Generated 3-gram text of length X\n",
      "<3-gram text generated>\n",
      "Generated 4-gram text of length X\n",
      "<4-gram text generated>\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for i in range(1,5):\n",
    "    print(f\"Extracted {len(grams[i-1])} unique {i}-grams\")\n",
    "print(\"Seed text:\", \"YYYY\")\n",
    "for i in range(1, 5):\n",
    "    print(f\"Generated {i}-gram text of length X\")\n",
    "    print(f\"<{i}-gram text generated>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
