{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/23/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Usage Disclaimer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with \\</s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports libraries and reads corpus documents. Save the documents as raw tokens\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "corpora = [\"poem.txt\", \"sample.txt\"]\n",
    "sentences = []\n",
    "tokenizedParagraphs = []\n",
    "for corpus in corpora:\n",
    "    corpus_sentences = []\n",
    "    corpus_paragraphs = []\n",
    "    with open(corpus, encoding=\"utf-8\") as wordList:\n",
    "        lines = wordList.readlines()\n",
    "        for line in lines:\n",
    "            line = line.lower() \t\t\t\t\t# Converts all documents to lowercase\n",
    "            sentence = sent_tokenize(line) \t\t\t# Extract as entire sentences\n",
    "            paragraph = word_tokenize(line) \t\t# Extract the entire line as words (not separating sentences into different arrays!)\n",
    "            corpus_sentences.append(sentence) \t\t\t\t# Adds each sentence to the corpus_sentences array\n",
    "            corpus_paragraphs.append(paragraph) \t# Adds each line into the corpus_paragraphs array\n",
    "            #print(sentence)\n",
    "            #print(paragraph)\n",
    "            #print()\n",
    "    sentences.append(corpus_sentences) #before separating sentences\n",
    "    tokenizedParagraphs.append(corpus_paragraphs)\n",
    "\n",
    "#print(\"Sentences: \", sentences) #before separating sentences\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokenizedSentences = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for corpus_sentences in sentences:\n",
    "    for sent in corpus_sentences:\n",
    "        for string in sent:\n",
    "            tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "            tokenizedSentences.append(tokenList)\n",
    "        \n",
    "print()\n",
    "\n",
    "print(\"Sentence level:\")\n",
    "for sentence in range(len(tokenizedSentences)):\n",
    "\tprint(tokenizedSentences[sentence])\n",
    "print()\n",
    "print(\"Paragraph level:\")\n",
    "for corpus in range(len(corpora)):\n",
    "\tprint(tokenizedParagraphs[corpus])\n",
    "\n",
    "# raw sentences and paragraphs for each corpus\n",
    "CorpusSenPars = [tokenizedSentences, tokenizedParagraphs]\n",
    "print(CorpusSenPars)\n",
    "\n",
    "# Set to False for large corpus\n",
    "if False: # Debug\n",
    "\tfor context in tokenizedSentences:\n",
    "\t\tprint(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment sentences and paragraphs by adding START and END tokens\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "#t[1] = [<s>tokenized words</s>], etc.\n",
    "#t[2] = [<s>tokenized words</s>], etc.\n",
    "#t[3] = [<s><s>tokenized words</s>], etc.\n",
    "#t[4] = [<s><s><s>tokenized words</s>], etc.\n",
    "\n",
    "CorpusAugmentedTokens = []\n",
    "AugmentedTokens = [[],[]] # [[Sentence Tokens], [Paragraph Tokens]]\n",
    "modes = [tokenizedSentences, tokenizedParagraphs]\n",
    "\n",
    "for m in modes:\n",
    "    print(m)\n",
    "\n",
    "# Arrays of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "#AugmentedTokens[0] = [] # [],[],[],[] #for sentences\n",
    "#AugmentedTokens[1] = [] # [],[],[],[] #for paragraphs\n",
    "\n",
    "#for i in range(len(AugmentedTokens)):\n",
    "#    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens] # Unfortunately cannot use this because unigrams have 1 start token, not 0\n",
    "\n",
    "def augmentTokens(AugmentedTokens):\n",
    "    for mode in range(len(modes)): # Sentence mode then Paragraph mode\n",
    "        print(\"Mode index:\", mode)\n",
    "        AugmentedTokens.append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented unigram sentence/paragraph to AugmentedTokens\n",
    "        AugmentedTokens.append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented bigram sentence/paragraph to AugmentedTokens\n",
    "        AugmentedTokens.append([[START]*2 + sentence + [END] for sentence in modes[mode]]) # Append augmented trigram sentence/paragraph to AugmentedTokens\n",
    "        AugmentedTokens.append([[START]*3 + sentence + [END] for sentence in modes[mode]]) # Append augmented quadgram sentence/paragraph to AugmentedTokens\n",
    "    return AugmentedTokens\n",
    "\n",
    "for corpus in range(len(corpora)):\n",
    "\tfor mode in range(len(modes)):\n",
    "\t\tprint(\"Corpus index:\", corpus)\n",
    "\t\tprint(\"Mode index:\", mode)\n",
    "\t\t#AugmentedTokens[mode].append(augment_tokens(AugmentedTokens[corpus]))\n",
    "\t\tCorpusAugmentedTokens[corpus][mode] = augmentTokens(AugmentedTokens[corpus])\n",
    "\n",
    "# Prints sentence level of augmented grams, followed by paragraph level of augmented grams\n",
    "for corpus in range(len(CorpusAugmentedTokens)):\n",
    "    print(f\"{corpora[corpus][:-4]}:\") # [:-4] removes the .txt\n",
    "    for mode in range(len(AugmentedTokens)):\n",
    "        if mode == 0: print(\"Sentence level:\")\n",
    "        elif mode == 1: print(\"Paragraph level:\")\n",
    "        for ngram in range(len(CorpusAugmentedTokens[corpus][mode])):\n",
    "            print(CorpusAugmentedTokens[corpus][mode][ngram][0])\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert augmented tokens into n-grams\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "# Using 2d dictionaries {context: {(word: 1), (word2: 2)}, context2: {(word3: 3), (word4: 4)}}\n",
    "gramsPrintStrings = [\"Unigrams\", \"Bigrams\", \"Trigrams\", \"Quadgrams\"]\n",
    "\n",
    "contextCountSen = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "uniqueSenNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "\n",
    "uniqueParNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "contextCountPar = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "\n",
    "\t\t\t#[Sentence grams], [Paragraph grams]\n",
    "gramsMode = [[{}, {}, {}, {}], [{}, {}, {}, {}]] \t# [[{sentenceUni}, {sentenceBi}, {sentenceTri}, {sentenceQuadi}],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# [{paragraphUni}, {paragraphBi}, {paragraphTri}, {paragraphQuad}]]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# Each dictionary holds a tuple key (context) and a dictionary value of the {word: count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1, c2): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1, c2, c3): {(\"word\", count)}\n",
    "\n",
    "contextCountMode = [contextCountSen, contextCountPar]\n",
    "uniqueModeNGrams = [uniqueSenNGrams, uniqueParNGrams]\n",
    "\n",
    "# Holds info for each corpus\n",
    "CorpusModeGram = [] #hopefully this isn't copying by reference\n",
    "CorpusContextCount = []\n",
    "CorpusUniqueModeGrams = []\n",
    "for corpus in corpora:\n",
    "\tCorpusModeGram.append(gramsMode) #each corpus holds a sentence mode and paragraph mode, and each mode holds 1-4grams\n",
    "\tCorpusContextCount.append(contextCountMode)\n",
    "\tCorpusUniqueModeGrams.append(uniqueModeNGrams)\n",
    "\n",
    "# Helper function for repeating code\n",
    "def incrementWordCount(corpus, mode, gramIndex, context, word):\n",
    "\t#cmg = CorpusModeGram[corpus][mode][gramIndex]\n",
    "\tif context not in CorpusModeGram[corpus][mode][gramIndex]: \t\t# if the context isn't in the gram dict, \n",
    "\t\tCorpusModeGram[corpus][mode][gramIndex][context] = {}  \t\t# create an empty dictionary\n",
    "\tif word not in CorpusModeGram[corpus][mode][gramIndex][context]: \t# check if word is already found in context\n",
    "\t\tCorpusModeGram[corpus][mode][gramIndex][context][word] = 1 \t# Initialize count as 1\n",
    "\telse:\n",
    "\t\tCorpusModeGram[corpus][mode][gramIndex][context][word] += 1 \t# Increment gram word count\n",
    "\n",
    "# For each corpus, mode, ngram, increment count of each word in each ngram\n",
    "for corpus in range(len(corpora)): # Loop through each corpus\n",
    "    print(corpora[corpus])\n",
    "    for mode in range(len(modes)): # Sentence then Paragraph level\n",
    "        print(\"Mode:\", mode)\n",
    "        for ngram in range(len(CorpusModeGram[corpus][mode])): # 4 gram types\n",
    "            print(gramsPrintStrings[ngram])\n",
    "            if ngram == 0: # Calculate Unigrams\n",
    "                context = ()\n",
    "                CorpusModeGram[corpus][mode][ngram][context] = {} # Declare the unigrams to be a dictionary with the only key as ()\n",
    "                for tokenList in CorpusAugmentedTokens[corpus][mode][ngram]: #0 context words\n",
    "                    print(\"Unigram list:\", tokenList)\n",
    "                    for word in tokenList:\n",
    "                        #print(\"Unigram word\", word)\n",
    "                        #print(\"Unigram list\", CorpusModeGram[corpus][mode][ngram][context].keys())\n",
    "                        # No actual context, so I'm not going to use incrementWordCount(grams[i], context, word)\n",
    "                        if word not in CorpusModeGram[corpus][mode][ngram][context].keys():\n",
    "                            CorpusModeGram[corpus][mode][ngram][context][word] = 1 \t\t# Add word to unigrams with count of 1\n",
    "                        else:\n",
    "                            CorpusModeGram[corpus][mode][ngram][context][word] += 1 \t\t# Increment unigram token count\n",
    "                        CorpusContextCount[corpus][mode][ngram] += 1\n",
    "\n",
    "            # if ngram == 1: # Calculate Bigrams\n",
    "            #     context = None\n",
    "            #     for tokenList in CorpusAugmentedTokens[corpus][mode][ngram]: #1 context word\n",
    "            #         for word in tokenList:\n",
    "            #             if context not in (None, END):\n",
    "            #                 bigramContext = (context,) # bigram dictionary key\n",
    "            #                 incrementWordCount(corpus, mode, ngram, bigramContext, word)\n",
    "            #             context = word\n",
    "            #             CorpusContextCount[corpus][mode][ngram] += 1\n",
    "\n",
    "            # if ngram == 2: # Calculate Trigrams\n",
    "            #     context = None\n",
    "            #     context2 = None\n",
    "            #     for tokenList in CorpusAugmentedTokens[corpus][mode][ngram]: #2 context words\n",
    "            #         for word in tokenList:\n",
    "            #             if context not in (None, END) and context2 not in (None, END):\n",
    "            #                 trigramContext = (context, context2) # trigram dictionary key\n",
    "            #                 incrementWordCount(corpus, mode, ngram, trigramContext, word)\n",
    "            #             context = context2\n",
    "            #             context2 = word\n",
    "            #             CorpusContextCount[corpus][mode][ngram] += 1\n",
    "\n",
    "            # if ngram == 3: # Calculate Quadgrams\n",
    "            #     context = None\n",
    "            #     context2 = None\n",
    "            #     context3 = None\n",
    "            #     for tokenList in CorpusAugmentedTokens[corpus][mode][ngram]: #3 context words\n",
    "            #         for word in tokenList:\n",
    "            #             if context not in (None, END) and context2 not in (None, END) and context3 not in (None, END):\n",
    "            #                 quadgramContext = (context, context2, context3) # quadgram dictionary key\n",
    "            #                 incrementWordCount(corpus, mode, ngram, quadgramContext, word)\n",
    "            #             context = context2\n",
    "            #             context2 = context3\n",
    "            #             context3 = word\n",
    "            #             CorpusContextCount[corpus][mode][ngram] += 1\n",
    "\n",
    "# Save the unique count of ngrams for each gram\n",
    "# Debug print statements\n",
    "\t# Print all the context and words\n",
    "\t# (Unigram context is just empty dictionary key ())\n",
    "for corpus in range(len(CorpusModeGram)):\n",
    "    for mode in range(len(CorpusModeGram[corpus])):\n",
    "        if mode == 0:\n",
    "            print(\"Sentence level:\")\n",
    "        elif mode == 1:\n",
    "            print(\"Paragraph level:\")\n",
    "        \n",
    "        for ngram in range(len(CorpusModeGram[corpus][mode])):\n",
    "            print(f\"{gramsPrintStrings[ngram]}\") # Which N-Gram is being printed\n",
    "\n",
    "            # Simple loop to count how many unique grams in each N-Gram, in each mode\n",
    "            for contextWord in CorpusModeGram[corpus][mode][ngram]:\n",
    "                uniqueModeNGrams[mode][ngram] += len(CorpusModeGram[corpus][mode][ngram][contextWord])\n",
    "            print(f\"Unique {gramsPrintStrings[ngram]}: {uniqueModeNGrams[mode][ngram]}\")\n",
    "            print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context Counter\n",
    "# N-Gram probability tables\n",
    "\n",
    "debug = False\n",
    "\n",
    "contextTotalsMode = [{},{}] # Lookup table for sentence and paragraphs to get count of each context unit\n",
    "\t\t\t\t\t\t\t# Use contextTotalsMode[mode][context] to access\n",
    "\n",
    "# Store context totals for each corpus\n",
    "CorpusContextTotals = []\n",
    "for corpus in corpora:\n",
    "\tCorpusContextCount.append(contextTotalsMode)\n",
    "\n",
    "def calcContextTotal(mode, grams, context):\n",
    "\tif context in contextTotalsMode[mode]:\n",
    "\t\treturn contextTotalsMode[mode][context]\n",
    "\tcontextTotal = sum(gramsMode[mode][grams][context].values()) \n",
    "\tcontextTotalsMode[mode][context] = contextTotal\n",
    "\t#print(f\"{mode,grams,context} calculated {contextTotal}\")\n",
    "\treturn contextTotal\n",
    "\n",
    "def calcGramProb(mode, ngram, ctx, wordTest): \n",
    "\tif ctx in gramsMode[mode][ngram]:\n",
    "\t\tcontextTotal = calcContextTotal(gramsMode[mode][ngram], ctx)\n",
    "\t\treturn gramsMode[mode][ngram][ctx][wordTest]/contextTotal\n",
    "\telse:\n",
    "\t\tprint(ctx, \"is not in the dictionary!\")\n",
    "\n",
    "probModeGram = [\n",
    "    [[], [], [], []],\t# sentence probabilities [uni, bi, tri, quad]\n",
    "    [[], [], [], []] \t# paragraph probabilities [uni, bi, tri, quad]\n",
    "]\n",
    "\n",
    "for mode in range(len(modes)):\t\t\t\t\t\t\t\t\t\t\t\t\t# for each mode (sentence then paragraph)\n",
    "\tif mode == 0:\n",
    "\t\tprint(\"Sentence level:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"\\nParagraph level:\")\n",
    "\n",
    "\tfor ngram in range(len(gramsMode[mode])): \t\t\t\t\t\t\t\t\t# for each ngram (uni, bi, tri, quad)\n",
    "\t\t#print(f\"{gramsPrintStrings[ngram]} probability table\") \t\t\t\t# which ngram table are we looking at\n",
    "\t\tfor ctx in gramsMode[mode][ngram]:\t\t\t\t\t\t\t\t\t\t# for each context in the gram in the sen/par mode\n",
    "\t\t\tcontextTotal = calcContextTotal(mode, ngram, ctx) \t\t\t\t\t# calculate how many words follow the current context\n",
    "\t\t\tfor word in gramsMode[mode][ngram][ctx]:\t\t\t\t\t\t\t# for each word in the current context \n",
    "\t\t\t\tcontextCount = gramsMode[mode][ngram][ctx][word]\n",
    "\t\t\t\t#print(ctx, word, contextCount, contextTotal)\n",
    "\t\t\t\tprob = contextCount/contextTotal \t\t\t\t\t\t\t\t# calculate the probability of the word in the current context\n",
    "\t\t\t\tprobModeGram[mode][ngram].append(prob)\t\t\t\t\t\t\t# save the probability to the sen/par mode for each ngram\n",
    "\t\t\t\tif debug:\n",
    "\t\t\t\t\toccurances = str(gramsMode[mode][ngram][ctx][word])\n",
    "\t\t\t\t\tprint(f\"\\tWord: {word:<12} \\t Occurances: {occurances:<3} \\t Context total: {contextTotal:<3} \\t Probability: {prob:.3f}\")\n",
    "\t\t\tif debug: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Gram probabilities converted to array lists\n",
    "for mode in range(2): # Sentence then Paragraph\n",
    "\tm = \"Sentence\" if mode == 0 else \"Paragraph\"\n",
    "\tprint(f\"Probabilities for {m} mode:\")\n",
    "\t\n",
    "\t#for g in range(4): # Uni, Bi, Tri, Quad grams\n",
    "\t#\tprint(f\"{gramsPrintStrings[g]} probabilities:\", probModeGram[mode][g])\n",
    "\tprint()  # Add a blank line between modes for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random tokens using probability\n",
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "#Set up seeds\n",
    "np.random.seed(0)\n",
    "\n",
    "def generateNextGram(mode, ngrams, topLevel, context): #(mode, ngrams, ngrams, biSeed)\n",
    "\tgram = gramsMode[mode][ngrams] # Input n to use grams[n], which allows for backoff by decrementing n\n",
    "\t#print(f\"Generating {gramsPrintStrings[ngrams]}\")\n",
    "\ttry:\n",
    "\t\tif context in gram:\n",
    "\t\t\tlength = sum(gram[context].values()) # sum of how many tokens occurred after the context\n",
    "\t\t\tprobArray = [gram[context][wordCount]/length for wordCount in gram[context]] # fractional chance of a word, given its context, out of the possible words after the context\n",
    "\t\t\tif False: # Debug\n",
    "\t\t\t\tprint(f\"Current context: {context}\")\n",
    "\t\t\t\tif ngrams >= 1:\n",
    "\t\t\t\t\tprint(f\"Possible choices: {list(gramsMode[mode][ngrams][context].keys())}\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(f\"Possible choices: (any unigram)\")\n",
    "\t\t\tnextWord = np.random.choice(list(gramsMode[mode][ngrams][context].keys()), size=1, p=probArray) # The one line that finally generates the new tokens\n",
    "\t\t\tnextWord = str(nextWord[0])\n",
    "\t\t\t#print(f\"Next word: {nextWord}\")\n",
    "\t\t\treturn nextWord\n",
    "\t\telse:\n",
    "\t\t\traise KeyError(f\"{context} not found in grams[{ngrams}]\")\n",
    "\texcept KeyError:\n",
    "\t\tif ngrams > 0:\n",
    "\t\t\t#print(f\"{context} not found in {gram}\")\n",
    "\t\t\t#print(f\"Backoff to {ngrams}grams\")\n",
    "\t\t\treturn generateNextGram(mode, ngrams-1, topLevel, context[1:] if len(context) > 1 else ()) # Recursive backoff, and it remembers the top level\n",
    "\t\t\t#bug: not returning to top level gram #still true? idk\n",
    "\t\telse:\n",
    "\t\t\t##print(f\"Backoff failed, context was \\\"{context}\\\" in mode {mode} during ngram {ngrams}. Returning '.'\")\n",
    "\t\t\treturn \".\"\n",
    "\n",
    "def setOutput(currentCtx, output, wordCount):\n",
    "\tif currentCtx not in (START, END):\n",
    "\t\tif currentCtx in (\"'\", \"’\", \",\", \".\", \":\", \"*\", \"?\", \";\") or output[-1] in (\"'\", \"’\"): #no space before symbols, or if an apostrophe is used\n",
    "\t\t\toutput += currentCtx\n",
    "\t\telse:\n",
    "\t\t\toutput += \" \" + currentCtx\n",
    "\t\twordCount += 1\n",
    "\treturn output, wordCount\n",
    "\n",
    "seed = \"\"\n",
    "while seed in (\"\", None, START, END, '.', \",\", \"?\", \"!\", \"]\", \")\"):\n",
    "\tseed = np.random.choice(list(gramsMode[0][0][()]), size=1, p=probModeGram[0][0])\n",
    "\tseed = str(seed[0]) # convert selected seed choice to a regular string\n",
    "biSeed = (seed,)\n",
    "triSeed = (START, seed,)\n",
    "quadSeed = (START, START, seed,)\n",
    "seeds = seed, biSeed, triSeed, quadSeed\n",
    "print(\"Seeds:\", seed, biSeed, triSeed, quadSeed)\n",
    "\n",
    "finalOutputs = [['','','',''], ['','','','']] # Output string for sentences (uni, bi, tri, quad), and paragraphs (uni, bi, tri, quad)\n",
    "finalOutputsLength = [[0,0,0,0], [0,0,0,0]] # How many tokens were output\n",
    "\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0: \n",
    "\t\tprint(\"Sentence mode:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"Paragraph mode:\")\n",
    "\tfor g in range(len(gramsMode[mode])):\n",
    "\t\tctx = seeds[g] # Set the seed context\n",
    "\t\tcurrentCtx = seed \n",
    "\t\tfinalOutputs[mode][g] = currentCtx # Start the output with the seed\n",
    "\t\twordCount = 1\n",
    "\t\twhile currentCtx != END and wordCount < 150:\n",
    "\t\t\tcurrentCtx = generateNextGram(mode, g, g, ctx)\n",
    "\t\t\tfinalOutputs[mode][g], wordCount = setOutput(currentCtx, finalOutputs[mode][g], wordCount)\n",
    "\n",
    "\t\t\t# Update context\n",
    "\t\t\tif g == 0:\n",
    "\t\t\t\tctx = () \t\t\t\t\t\t\t# unigram seed\n",
    "\t\t\telif g == 1:\n",
    "\t\t\t\tctx = (currentCtx,) \t\t\t\t# bigram seed\n",
    "\t\t\telif g == 2:\n",
    "\t\t\t\tctx = ((ctx[1], currentCtx)) \t\t# trigram seed\n",
    "\t\t\telif g == 3:\n",
    "\t\t\t\tctx = (ctx[1], ctx[2], currentCtx) \t# quadgram seed\n",
    "\t\t\t\n",
    "\t\tfinalOutputsLength[mode][g] = wordCount\n",
    "\t\tprint(f\"{gramsPrintStrings[g]}: {finalOutputs[mode][g]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Output\n",
    "\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for g in range(0,4):\n",
    "    print(f\"Extracted {uniqueSenNGrams[g]} unique {g+1}-grams\")\n",
    "print(\"Seed text:\", seed)\n",
    "for mode in range(2):\n",
    "\tif mode == 0: \n",
    "\t\tprint(\"Sentence mode:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"\\nParagraph mode:\")\n",
    "\tfor g in range(0, 4):\n",
    "\t\tprint(f\"Generated {g+1}-gram text of length {finalOutputsLength[mode][g]}\")\n",
    "\t\tprint(f\"{finalOutputs[mode][g]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
