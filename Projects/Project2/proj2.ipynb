{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Usage Disclaimer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with \\</s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph level:  [['i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.'], ['a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.']]\n",
      "\n",
      "Sentence level:  [['i', 'have', 'a', 'cat', '.'], ['my', 'cat', 'is', 'black', '.'], ['a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.'], ['i', 'have', 'the', 'car', 'license', 'tag', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Imports libraries and reads corpus documents. Save the documents as tokens\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "sentences = []\n",
    "tokenizedParagraphs = []\n",
    "\n",
    "with open(\"poem.txt\", encoding=\"utf-8\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower() # Converts all documents to lowercase\n",
    "        sentence = sent_tokenize(line) # Extract as entire sentences\n",
    "        paragraph = word_tokenize(line) # Extract the entire line as words (not separating sentences into different arrays!)\n",
    "        sentences.append(sentence) # Adds each sentence to the sentences array\n",
    "        tokenizedParagraphs.append(paragraph) # Adds each line into the paragraphs array\n",
    "        #print(sentence)\n",
    "        ##print(paragraph)\n",
    "        #print()\n",
    "        \n",
    "#print(\"Sentences: \", sentences) #before separating sentences\n",
    "print(\"Paragraph level: \", tokenizedParagraphs)\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokenizedSentences = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "        tokenizedSentences.append(tokenList)\n",
    "        \n",
    "print()\n",
    "print(\"Sentence level: \", tokenizedSentences)\n",
    "\n",
    "# Disable for large corpus\n",
    "if False:\n",
    "\tfor context in tokenizedSentences:\n",
    "\t\tprint(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence level: ↓\n",
      "\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', '<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', '<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "\n",
      "Paragraph level: ↑\n"
     ]
    }
   ],
   "source": [
    "# Add START and END tokens\n",
    "# Make sure to Run All before re-running this!\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "#t[1] = [<s>tokenized words</s>], etc.\n",
    "#t[2] = [<s>tokenized words</s>], etc.\n",
    "#t[3] = [<s><s>tokenized words</s>], etc.\n",
    "#t[4] = [<s><s><s>tokenized words</s>], etc.\n",
    "\n",
    "AugmentedTokens = [[],[]] # [[Sentence Tokens], [Paragraph Tokens]]\n",
    "modes = [tokenizedSentences, tokenizedParagraphs]\n",
    "\n",
    "# Arrays of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "AugmentedTokens[0] = [] # [],[],[],[] #for sentences\n",
    "AugmentedTokens[1] = [] # [],[],[],[] #for paragraphs\n",
    "\n",
    "# Since I am modifying each sentence, for every gram, I will add the START n times and END once per sentence\n",
    "# List comprehension as suggested by Claude 3.5-sonnet: (and modifications by myself too!)\n",
    "# newList = [expression for item in iterable]\n",
    "\n",
    "#for i in range(len(AugmentedTokens)):\n",
    "#    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens] # Unfortunately cannot use this because unigrams have 1 start token, not 0\n",
    "\n",
    "print(\"Sentence level: ↓\\n\")\n",
    "for mode in range(2): # Sentence mode then Paragraph mode\n",
    "\tAugmentedTokens[mode].append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented unigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented bigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*2 + sentence + [END] for sentence in modes[mode]]) # Append augmented trigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*3 + sentence + [END] for sentence in modes[mode]]) # Append augmented quadgram sentence/paragraph to AugmentedTokens\n",
    "\n",
    "\t# Prints sentence level of augmented grams, followed by paragraph level of augmented grams\n",
    "\tfor ngram in range(len(AugmentedTokens[mode])):\n",
    "\t\tprint(AugmentedTokens[mode][ngram])\n",
    "\tprint()\n",
    "print(\"Paragraph level: ↑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "('<s>',)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[176], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28;01mNone\u001b[39;00m, END):\n\u001b[0;32m     56\u001b[0m \tbigramContext \u001b[38;5;241m=\u001b[39m (context,) \u001b[38;5;66;03m# bigram dictionary key\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m \t\u001b[43mincrementWordCount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgram\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbigramContext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m context \u001b[38;5;241m=\u001b[39m word\n\u001b[0;32m     59\u001b[0m contextCountMode[mode][gram] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[176], line 32\u001b[0m, in \u001b[0;36mincrementWordCount\u001b[1;34m(mode, gramIndex, context, word)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m gramsSen[gramIndex]: \t\t\t\t\u001b[38;5;66;03m# if the context isn't in the gram dict, \u001b[39;00m\n\u001b[0;32m     31\u001b[0m \tgramsMode[mode][gramIndex][context] \u001b[38;5;241m=\u001b[39m {}  \t\t\u001b[38;5;66;03m# create an empty dictionary\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[43mgramsMode\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgramIndex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m]\u001b[49m: \u001b[38;5;66;03m# check if word is already found in context\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \tgramsMode[mode][gramIndex][context][word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \t\u001b[38;5;66;03m# Initialize count as 1\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyError\u001b[0m: ('<s>',)"
     ]
    }
   ],
   "source": [
    "# Convert augmented tokens into n-grams\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "# Using 2d dictionaries {context: {(word: 1), (word2: 2)}, context2: {(word3: 3), (word4: 4)}}\n",
    "gramsPrintStrings = [\"Unigrams\", \"Bigrams\", \"Trigrams\", \"Quadgrams\"]\n",
    "\n",
    "unigramsSen = {}   # (): [\"word\", count]\n",
    "bigramsSen = {}    # (context1): [\"word\", count]\n",
    "trigramsSen = {}   # (c1, c2): [\"word\", count]\n",
    "quadgramsSen = {}  # (c1, c2, c3): [(\"word\", count)]\n",
    "gramsSen = [unigramsSen, bigramsSen, trigramsSen, quadgramsSen]\n",
    "contextCountSen = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "uniqueSenNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "\n",
    "unigramsPar = {}   # (): [\"word\", count]\n",
    "bigramsPar = {}    # (context1): [\"word\", count]\n",
    "trigramsPar = {}   # (c1, c2): [\"word\", count]\n",
    "quadgramsPar = {}  # (c1, c2, c3): [(\"word\", count)]\n",
    "gramsPar = [unigramsPar, bigramsPar, trigramsPar, quadgramsPar]\n",
    "uniqueParNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "contextCountPar = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "\n",
    "gramsMode = [gramsSen, gramsPar]\n",
    "contextCountMode = [contextCountSen, contextCountPar]\n",
    "uniqueModeNGrams = [uniqueSenNGrams, uniqueParNGrams]\n",
    "\n",
    "\n",
    "# Helper function for repeating code\n",
    "def incrementWordCount(mode, gramIndex, context, word):\n",
    "\tif context not in gramsMode[mode][gramIndex]: \t\t\t\t# if the context isn't in the gram dict, \n",
    "\t\tgramsMode[mode][gramIndex][context] = {}  \t\t# create an empty dictionary\n",
    "\tif word not in gramsMode[mode][gramIndex][context]: # check if word is already found in context\n",
    "\t\tgramsMode[mode][gramIndex][context][word] = 1 \t# Initialize count as 1\n",
    "\telse:\n",
    "\t\tgramsMode[mode][gramIndex][context][word] += 1 \t# Increment gram word count\n",
    "\n",
    "for mode in range(2): # Sentence then Paragraph level\n",
    "\tfor gram in range(4): # 4 gram types\n",
    "\t\tif gram == 0: # Calculate Unigrams\n",
    "\t\t\tcontext = ()\n",
    "\t\t\tgramsMode[mode][gram][context] = {} # Declare the unigrams to be a dictionary with the only key as ()\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][gram]: #0 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\t# No actual context, so I'm not going to use incrementWordCount(grams[i], context, word)\n",
    "\t\t\t\t\tif word not in gramsMode[mode][gram][context]:\n",
    "\t\t\t\t\t\tgramsMode[mode][gram][context][word] = 1 \t\t# Add word to unigrams with count of 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tgramsMode[mode][gram][context][word] += 1 \t\t# Increment unigram token count\n",
    "\t\t\t\t\tcontextCountMode[mode][gram] += 1\n",
    "\n",
    "\t\tif gram == 1: # Calculate Bigrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][gram]: #1 context word\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END):\n",
    "\t\t\t\t\t\tbigramContext = (context,) # bigram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, gram, bigramContext, word)\n",
    "\t\t\t\t\tcontext = word\n",
    "\t\t\t\t\tcontextCountMode[mode][gram] += 1\n",
    "\n",
    "\t\tif gram == 2: # Calculate Trigrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tcontext2 = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][gram]: #2 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END) and context2 not in (None, END):\n",
    "\t\t\t\t\t\ttrigramContext = (context, context2) # trigram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, gram, trigramContext, word)\n",
    "\t\t\t\t\tcontext = context2\n",
    "\t\t\t\t\tcontext2 = word\n",
    "\t\t\t\t\tcontextCountMode[mode][gram] += 1\n",
    "\n",
    "\t\tif gram == 3: # Calculate Quadgrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tcontext2 = None\n",
    "\t\t\tcontext3 = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][gram]: #3 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END) and context2 not in (None, END) and context3 not in (None, END):\n",
    "\t\t\t\t\t\tquadgramContext = (context, context2, context3) # quadgram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, gram, quadgramContext, word)\n",
    "\t\t\t\t\tcontext = context2\n",
    "\t\t\t\t\tcontext2 = context3\n",
    "\t\t\t\t\tcontext3 = word\n",
    "\t\t\t\t\tcontextCountMode[mode][gram] += 1\n",
    "\n",
    "\n",
    "# Save the unique count of ngrams for each gram\n",
    "#Debug print statements\n",
    "\t# Print all the context and words\n",
    "\t# (Unigram context is just empty dictionary key ())\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0:\n",
    "\t\tprint(\"Sentence level:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"Paragraph level:\")\n",
    "\t\t\n",
    "\tfor gram in range(len(gramsSen)):\n",
    "\t\tprint(f\"{gramsPrintStrings[gram]}\") # Which N-Gram is being printed\n",
    "\t\t#for context in grams[i]: # Displays all tokens in each gram\n",
    "\t\t\t#print(f\"{context}: {grams[i][context]}\") #(Context,): {Dictionary of words: count}\n",
    "\n",
    "\t\t# Simple loop to count how many unique grams in each N-Gram\n",
    "\t\tfor contextWord in gramsMode[mode][gram]: #switch to grams for each mode\n",
    "\t\t\tuniqueModeNGrams[mode][gram] += len(gramsMode[mode][gram][contextWord]) #need to switch to grams for each mode\n",
    "\t\tprint(f\"Unique {gramsPrintStrings[gram]}: {uniqueModeNGrams[mode][gram]}\")\n",
    "\t\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unigram probability table\n",
      "\n",
      "Bigram probability table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Trigram probability table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Quadgram probability table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definitions of gram probabilities\n",
    "\n",
    "debug = False\n",
    "\n",
    "def calcContextTotal(grams, gram):\n",
    "\tcontextTotal = 0\n",
    "\tfor word in grams[gram]:\n",
    "\t\tcontextTotal += grams[gram][word]\n",
    "\treturn contextTotal\n",
    "\n",
    "def unigramProb(wordTest):\n",
    "    # Probability of wordTest in its context\n",
    "\tif wordTest != None:\n",
    "\t\tprob = unigramsSen[()][wordTest]/contextCountSen[0]\n",
    "\t\treturn prob \n",
    "\telse:\n",
    "\t\tprint(wordTest, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "def bigramProb(bigram, wordTest): # 1 context word\n",
    "    # Probability of wordTest, given that its context came before it\n",
    "\tif bigram in bigramsSen:\n",
    "\t\tcontextTotal = calcContextTotal(bigramsSen, bigram)\n",
    "\t\treturn bigramsSen[bigram][wordTest]/contextTotal\n",
    "\telse:\n",
    "\t\tprint(bigram, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "def trigramProb(trigram, wordTest): # 2 context words\n",
    "    # Probability of wordTest, given that its context came before it\n",
    "\tif trigram in trigramsSen:\n",
    "\t\tcontextTotal = calcContextTotal(trigramsSen, trigram)\n",
    "\t\treturn trigramsSen[trigram][wordTest]/contextTotal\n",
    "\telse:\n",
    "\t\tprint(trigram, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "# Note: I don't think compacting this into (wordTest, trigram) would be a good idea\n",
    "def quadgramProb(quadgram, wordTest): # 3 context words\n",
    "    # Probability of wordTest, given that its context came before it\n",
    "\tif quadgram in quadgramsSen:\n",
    "\t\tcontextTotal = calcContextTotal(quadgramsSen, quadgram)\n",
    "\t\treturn quadgramsSen[quadgram][wordTest]/contextTotal\n",
    "\telse:\n",
    "\t\tprint(quadgram, \"is not in the dictionary!\")\n",
    "\n",
    "# P(SearchWord, (context))\n",
    "#print(unigramProb(\"have\"))\n",
    "#print(bigramProb((\"a\", \"cat\")))\n",
    "#print(trigramProb((\"car\", \"have\", \"the\")))\n",
    "#print(quadgramProb((\"cat\", \"almost\", \"hit\", \"a\")))\n",
    "\n",
    "print()\n",
    "\n",
    "probUnigram = [] # array to hold probabilities\n",
    "probBigram = []\n",
    "probTrigram = []\n",
    "probQuadgram = []\n",
    "\n",
    "\n",
    "print(\"Unigram probability table\")\n",
    "#print(unigrams)\n",
    "for unigram in unigramsSen[()]:\n",
    "\tprob = unigramProb(unigram)\n",
    "\tprobUnigram.append(prob)\n",
    "\tif debug:\n",
    "\t\tprint(f\"\\tWord: {unigram:<12} \\t Occurances: {str(unigramsSen[()][unigram]):<3} \\t Context total: {contextCountSen[0]:<3} \\t Probability: {prob:.3f}\")\n",
    "    # print unigram[i],                        unigram dictionary value                 context summed in previous block       unigram Prob, input string token \n",
    "\n",
    "print()\n",
    "print(\"Bigram probability table\")\n",
    "#print(bigrams)\n",
    "for bigram in bigramsSen:\n",
    "\tif debug:\n",
    "\t\tprint(f\"Context: {str(bigram):<12}\")\n",
    "\tfor word in bigramsSen[bigram]:\n",
    "\t\tprob = bigramProb(bigram, word)\n",
    "\t\tprobBigram.append(prob)\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"\\t Word: {word:<12} \\t Occurances: {str(bigramsSen[bigram][word]):<3} \\t Context total: {calcContextTotal(bigramsSen, bigram):<3} \\t Probability: {prob:.3f}\")\n",
    "\tprint()\n",
    "\n",
    "print(\"Trigram probability table\")\n",
    "#print(trigrams)\n",
    "for trigram in trigramsSen:\n",
    "\tif debug:\n",
    "\t\tprint(f\"Context: {str(trigram):<12}\")\n",
    "\tfor word in trigramsSen[trigram]:\n",
    "\t\tprob = trigramProb(trigram, word)\n",
    "\t\tprobTrigram.append(prob)\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"\\t Word: {word:<12} \\t Occurances: {str(trigramsSen[trigram][word]):<3} \\t Context total: {calcContextTotal(trigramsSen, trigram):<3} \\t Probability: {prob:.3f}\")\n",
    "\tprint()\n",
    "\n",
    "print(\"Quadgram probability table\")\n",
    "#print(quadgrams)\n",
    "for quadgram in quadgramsSen:\n",
    "\tif debug:\n",
    "\t\tprint(f\"Context: {str(quadgram):<12}\")\n",
    "\tfor word in quadgramsSen[quadgram]:\n",
    "\t\tprob = quadgramProb(quadgram, word)\n",
    "\t\tprobQuadgram.append(prob)\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"\\t Word: {word:<12} \\t Occurances: {str(quadgramsSen[quadgram][word]):<3} \\t Context total: {calcContextTotal(quadgramsSen, quadgram):<3} \\t Probability: {prob:.3f}\")\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06060606060606061, 0.06060606060606061, 0.06060606060606061, 0.09090909090909091, 0.09090909090909091, 0.12121212121212122, 0.030303030303030304, 0.030303030303030304, 0.06060606060606061, 0.06060606060606061, 0.06060606060606061, 0.030303030303030304, 0.030303030303030304, 0.030303030303030304, 0.030303030303030304, 0.030303030303030304]\n",
      "[0.5, 0.16666666666666666, 0.3333333333333333, 1.0, 0.5, 0.5, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.75, 0.125, 0.125, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.5, 0.16666666666666666, 0.3333333333333333, 1.0, 0.5, 0.5, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.75, 0.125, 0.125, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 0.5, 0.25, 0.25, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# N-Gram probabilities converted to array lists\n",
    "print(probUnigram)\n",
    "print(probBigram)\n",
    "print(probBigram)\n",
    "print(probQuadgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to log space\n",
    "# log(p1 * p2 * p3 * p4) = log(p1) + log(p2) + log(p3) + log(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "probabilities do not sum to 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[160], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, START, END, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 7\u001b[0m \tseed \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munigramsSen\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprobUnigram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \tseed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(seed[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      9\u001b[0m biSeed \u001b[38;5;241m=\u001b[39m (seed,)\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:998\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: probabilities do not sum to 1"
     ]
    }
   ],
   "source": [
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "#Set up seeds\n",
    "#np.random.seed(0)\n",
    "seed = \"\"\n",
    "while seed in (\"\", None, START, END, '.', \",\", \"?\", \"!\", \"[\", \"]\", \"(\", \")\"):\n",
    "\tseed = np.random.choice(list(unigramsSen[()]), size=1, p=probUnigram)\n",
    "\tseed = str(seed[0])\n",
    "biSeed = (seed,)\n",
    "triSeed = (START, seed,)\n",
    "quadSeed = (START, START, seed,)\n",
    "print(\"Seeds:\", seed, biSeed, triSeed, quadSeed)\n",
    "\n",
    "def generateNextGram(ngrams, topLevel, context): #(ngrams, ngrams, biSeed)\n",
    "\tgram = gramsSen[ngrams] # Input n to use grams[n], which allows for backoff by decrementing n\n",
    "\t#print(f\"Generating {ngrams+1}grams\")\n",
    "\t#length = 0\n",
    "\ttry:\n",
    "\t\tif context in gram:\n",
    "\t\t\t#for word in gram[context]:\n",
    "\t\t\t#\tlength += gram[context][word]\n",
    "\t\t\tlength = sum(gram[context].values())\n",
    "\t\t\tprobArray = [gram[context][wordCount]/length for wordCount in gram[context]]\n",
    "\t\t\tnextWord = np.random.choice(list(gram[context].keys()), size=1, p=probArray)\n",
    "\t\t\tnextWord = str(nextWord[0])\n",
    "\t\t\t#print(f\"Next word: {nextWord}\")\n",
    "\t\t\treturn nextWord\n",
    "\t\telse:\n",
    "\t\t\traise KeyError(f\"{context} not found in grams[{ngrams}]\")\n",
    "\texcept KeyError:\n",
    "\t\tif ngrams > 0:\n",
    "\t\t\t#print(f\"{context} not found in {gram}\")\n",
    "\t\t\t#print(f\"Backoff to {ngrams}grams\")\n",
    "\t\t\treturn generateNextGram(ngrams-1, topLevel, context[1:] if len(context) > 1 else ())\n",
    "\t\t\t#bug: not returning to top level gram\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Backoff failed, returning '.'\")\n",
    "\t\t\treturn \".\"\n",
    "\n",
    "def setOutput(current, output, wordCount):\n",
    "\tif current not in (START, END):\n",
    "\t\tif current in (\"'\", \"’\", \",\", \".\", \":\", \"*\"): #no space after symbols\n",
    "\t\t\toutput += current\n",
    "\t\telse:\n",
    "\t\t\toutput += \" \" + current \n",
    "\t\twordCount += 1\n",
    "\treturn output, wordCount\n",
    "\n",
    "current = seed\n",
    "UniOutput = current\n",
    "wordCount = 0\n",
    "while current != END and wordCount < 150:\n",
    "\t#current = np.random.choice(list(unigrams[()]), size=1, p=probUnigram)\n",
    "\t#current = current[0]\n",
    "\n",
    "\tcurrent = generateNextGram(0, 0, ())\n",
    "\n",
    "\tUniOutput, wordCount = setOutput(current, UniOutput, wordCount)\n",
    "print(f\"Unigram: {UniOutput}\")\n",
    "print()\n",
    "\n",
    "#print(bigrams)\n",
    "#print(\"Possible words:\", bigrams[biSeed])\n",
    "current = seed\n",
    "BiOutput = current\n",
    "wordCount = 0\n",
    "while current != END and wordCount < 150:\n",
    "\tcurrent = generateNextGram(1, 1, biSeed)\n",
    "\t#print(\"Chosen current word:\", current, \"\\n\")\n",
    "\tbiSeed = (current,)\n",
    "\n",
    "\tBiOutput, wordCount = setOutput(current, BiOutput, wordCount)\n",
    "print(\"Bigram:\", BiOutput)\n",
    "print()\n",
    "\n",
    "current = seed\n",
    "TriOutput = current\n",
    "while current != END and wordCount < 150:\n",
    "\tcurrent = generateNextGram(2, 2, triSeed)\n",
    "\t\n",
    "\ttriSeed = (triSeed[1],current)\n",
    "\t\n",
    "\tTriOutput, wordCount = setOutput(current, TriOutput, wordCount)\n",
    "print(\"Trigram:\", TriOutput)\n",
    "print()\n",
    "\n",
    "current = seed\n",
    "QuadOutput = current\n",
    "while current != END and wordCount < 150:\n",
    "\tcurrent = generateNextGram(3, 3, quadSeed)\n",
    "\t\n",
    "\tquadSeed = (quadSeed[1], quadSeed[2],current)\n",
    "\t\n",
    "\tQuadOutput, wordCount = setOutput(current, QuadOutput, wordCount)\n",
    "print(\"Quadgram:\", QuadOutput)\n",
    "print()\n",
    "\n",
    "finalOutputs = [UniOutput, BiOutput, TriOutput, QuadOutput]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 16 unique 1-grams\n",
      "Extracted 22 unique 2-grams\n",
      "Extracted 25 unique 3-grams\n",
      "Extracted 26 unique 4-grams\n",
      "Seed text: hit\n",
      "Generated 1-gram text of length X\n",
      "hit black cat\n",
      "Generated 2-gram text of length X\n",
      "hit a cat is black.\n",
      "Generated 3-gram text of length X\n",
      "hit a cat.\n",
      "Generated 4-gram text of length X\n",
      "hit a cat.\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for mode in range(0,4):\n",
    "    print(f\"Extracted {uniqueSenNGrams[mode]} unique {mode+1}-grams\")\n",
    "print(\"Seed text:\", seed)\n",
    "for mode in range(0, 4):\n",
    "    print(f\"Generated {mode+1}-gram text of length X\")\n",
    "    print(f\"{finalOutputs[mode]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
