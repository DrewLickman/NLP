{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Usage Disclaimer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with \\</s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph level:  [['i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.'], ['a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.']]\n",
      "\n",
      "Sentence level:  [['i', 'have', 'a', 'cat', '.'], ['my', 'cat', 'is', 'black', '.'], ['a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.'], ['i', 'have', 'the', 'car', 'license', 'tag', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Imports libraries and reads corpus documents. Save the documents as tokens\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "sentences = []\n",
    "tokenizedParagraphs = []\n",
    "\n",
    "with open(\"poem.txt\", encoding=\"utf-8\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower() # Converts all documents to lowercase\n",
    "        sentence = sent_tokenize(line) # Extract as entire sentences\n",
    "        paragraph = word_tokenize(line) # Extract the entire line as words (not separating sentences into different arrays!)\n",
    "        sentences.append(sentence) # Adds each sentence to the sentences array\n",
    "        tokenizedParagraphs.append(paragraph) # Adds each line into the paragraphs array\n",
    "        #print(sentence)\n",
    "        ##print(paragraph)\n",
    "        #print()\n",
    "        \n",
    "#print(\"Sentences: \", sentences) #before separating sentences\n",
    "print(\"Paragraph level: \", tokenizedParagraphs)\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokenizedSentences = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "        tokenizedSentences.append(tokenList)\n",
    "        \n",
    "print()\n",
    "print(\"Sentence level: \", tokenizedSentences)\n",
    "\n",
    "# Disable for large corpus\n",
    "if False:\n",
    "\tfor context in tokenizedSentences:\n",
    "\t\tprint(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence level: ↓\n",
      "\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', '<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', '<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "\n",
      "Paragraph level: ↑\n"
     ]
    }
   ],
   "source": [
    "# Add START and END tokens\n",
    "# Make sure to Run All before re-running this!\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "#t[1] = [<s>tokenized words</s>], etc.\n",
    "#t[2] = [<s>tokenized words</s>], etc.\n",
    "#t[3] = [<s><s>tokenized words</s>], etc.\n",
    "#t[4] = [<s><s><s>tokenized words</s>], etc.\n",
    "\n",
    "AugmentedTokens = [[],[]] # [[Sentence Tokens], [Paragraph Tokens]]\n",
    "modes = [tokenizedSentences, tokenizedParagraphs]\n",
    "\n",
    "# Arrays of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "AugmentedTokens[0] = [] # [],[],[],[] #for sentences\n",
    "AugmentedTokens[1] = [] # [],[],[],[] #for paragraphs\n",
    "\n",
    "# Since I am modifying each sentence, for every gram, I will add the START n times and END once per sentence\n",
    "# List comprehension as suggested by Claude 3.5-sonnet: (and modifications by myself too!)\n",
    "# newList = [expression for item in iterable]\n",
    "\n",
    "#for i in range(len(AugmentedTokens)):\n",
    "#    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens] # Unfortunately cannot use this because unigrams have 1 start token, not 0\n",
    "\n",
    "print(\"Sentence level: ↓\\n\")\n",
    "for mode in range(2): # Sentence mode then Paragraph mode\n",
    "\tAugmentedTokens[mode].append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented unigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented bigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*2 + sentence + [END] for sentence in modes[mode]]) # Append augmented trigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*3 + sentence + [END] for sentence in modes[mode]]) # Append augmented quadgram sentence/paragraph to AugmentedTokens\n",
    "\n",
    "\t# Prints sentence level of augmented grams, followed by paragraph level of augmented grams\n",
    "\tfor ngram in range(len(AugmentedTokens[mode])):\n",
    "\t\tprint(AugmentedTokens[mode][ngram])\n",
    "\tprint()\n",
    "print(\"Paragraph level: ↑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence level:\n",
      "Unigrams\n",
      "Unique Unigrams: 16\n",
      "\n",
      "Bigrams\n",
      "Unique Bigrams: 22\n",
      "\n",
      "Trigrams\n",
      "Unique Trigrams: 25\n",
      "\n",
      "Quadgrams\n",
      "Unique Quadgrams: 26\n",
      "\n",
      "Paragraph level:\n",
      "Unigrams\n",
      "Unique Unigrams: 16\n",
      "\n",
      "Bigrams\n",
      "Unique Bigrams: 23\n",
      "\n",
      "Trigrams\n",
      "Unique Trigrams: 26\n",
      "\n",
      "Quadgrams\n",
      "Unique Quadgrams: 27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert augmented tokens into n-grams\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "# Using 2d dictionaries {context: {(word: 1), (word2: 2)}, context2: {(word3: 3), (word4: 4)}}\n",
    "gramsPrintStrings = [\"Unigrams\", \"Bigrams\", \"Trigrams\", \"Quadgrams\"]\n",
    "\n",
    "contextCountSen = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "uniqueSenNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "\n",
    "uniqueParNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "contextCountPar = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "\n",
    "gramsMode = [[{}, {}, {}, {}], [{}, {}, {}, {}]] \t# [[{sentenceUni}, {sentenceBi}, {sentenceTri}, {sentenceQuadi}],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# [{paragraphUni}, {paragraphBi}, {paragraphTri}, {paragraphQuad}]]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# Each dictionary holds a tuple key (context) and a dictionary value of the {word: count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1, c2): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1, c2, c3): {(\"word\", count)}\n",
    "\n",
    "contextCountMode = [contextCountSen, contextCountPar]\n",
    "uniqueModeNGrams = [uniqueSenNGrams, uniqueParNGrams]\n",
    "\n",
    "\n",
    "# Helper function for repeating code\n",
    "def incrementWordCount(mode, gramIndex, context, word):\n",
    "\tif context not in gramsMode[mode][gramIndex]: \t\t\t\t# if the context isn't in the gram dict, \n",
    "\t\tgramsMode[mode][gramIndex][context] = {}  \t\t# create an empty dictionary\n",
    "\tif word not in gramsMode[mode][gramIndex][context]: # check if word is already found in context\n",
    "\t\tgramsMode[mode][gramIndex][context][word] = 1 \t# Initialize count as 1\n",
    "\telse:\n",
    "\t\tgramsMode[mode][gramIndex][context][word] += 1 \t# Increment gram word count\n",
    "\n",
    "for mode in range(2): # Sentence then Paragraph level\n",
    "\tfor gram in range(4): # 4 gram types\n",
    "\t\tif gram == 0: # Calculate Unigrams\n",
    "\t\t\tcontext = ()\n",
    "\t\t\tgramsMode[mode][gram][context] = {} # Declare the unigrams to be a dictionary with the only key as ()\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][gram]: #0 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\t# No actual context, so I'm not going to use incrementWordCount(grams[i], context, word)\n",
    "\t\t\t\t\tif word not in gramsMode[mode][gram][context]:\n",
    "\t\t\t\t\t\tgramsMode[mode][gram][context][word] = 1 \t\t# Add word to unigrams with count of 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tgramsMode[mode][gram][context][word] += 1 \t\t# Increment unigram token count\n",
    "\t\t\t\t\tcontextCountMode[mode][gram] += 1\n",
    "\n",
    "\t\tif gram == 1: # Calculate Bigrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][gram]: #1 context word\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END):\n",
    "\t\t\t\t\t\tbigramContext = (context,) # bigram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, gram, bigramContext, word)\n",
    "\t\t\t\t\tcontext = word\n",
    "\t\t\t\t\tcontextCountMode[mode][gram] += 1\n",
    "\n",
    "\t\tif gram == 2: # Calculate Trigrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tcontext2 = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][gram]: #2 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END) and context2 not in (None, END):\n",
    "\t\t\t\t\t\ttrigramContext = (context, context2) # trigram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, gram, trigramContext, word)\n",
    "\t\t\t\t\tcontext = context2\n",
    "\t\t\t\t\tcontext2 = word\n",
    "\t\t\t\t\tcontextCountMode[mode][gram] += 1\n",
    "\n",
    "\t\tif gram == 3: # Calculate Quadgrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tcontext2 = None\n",
    "\t\t\tcontext3 = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][gram]: #3 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END) and context2 not in (None, END) and context3 not in (None, END):\n",
    "\t\t\t\t\t\tquadgramContext = (context, context2, context3) # quadgram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, gram, quadgramContext, word)\n",
    "\t\t\t\t\tcontext = context2\n",
    "\t\t\t\t\tcontext2 = context3\n",
    "\t\t\t\t\tcontext3 = word\n",
    "\t\t\t\t\tcontextCountMode[mode][gram] += 1\n",
    "\n",
    "\n",
    "# Save the unique count of ngrams for each gram\n",
    "#Debug print statements\n",
    "\t# Print all the context and words\n",
    "\t# (Unigram context is just empty dictionary key ())\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0:\n",
    "\t\tprint(\"Sentence level:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"Paragraph level:\")\n",
    "\t\t\n",
    "\tfor gram in range(len(gramsSen)):\n",
    "\t\tprint(f\"{gramsPrintStrings[gram]}\") # Which N-Gram is being printed\n",
    "\t\t#for context in grams[i]: # Displays all tokens in each gram\n",
    "\t\t\t#print(f\"{context}: {grams[i][context]}\") #(Context,): {Dictionary of words: count}\n",
    "\n",
    "\t\t# Simple loop to count how many unique grams in each N-Gram\n",
    "\t\tfor contextWord in gramsMode[mode][gram]: #switch to grams for each mode\n",
    "\t\t\tuniqueModeNGrams[mode][gram] += len(gramsMode[mode][gram][contextWord]) #need to switch to grams for each mode\n",
    "\t\tprint(f\"Unique {gramsPrintStrings[gram]}: {uniqueModeNGrams[mode][gram]}\")\n",
    "\t\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence level:\n",
      "Unigrams probability table\n",
      "\tWord: <s>          \t Occurances: 4   \t Context total: 33  \t Probability: 0.121\n",
      "\tWord: i            \t Occurances: 2   \t Context total: 33  \t Probability: 0.061\n",
      "\tWord: have         \t Occurances: 2   \t Context total: 33  \t Probability: 0.061\n",
      "\tWord: a            \t Occurances: 3   \t Context total: 33  \t Probability: 0.091\n",
      "\tWord: cat          \t Occurances: 3   \t Context total: 33  \t Probability: 0.091\n",
      "\tWord: .            \t Occurances: 4   \t Context total: 33  \t Probability: 0.121\n",
      "\tWord: </s>         \t Occurances: 4   \t Context total: 33  \t Probability: 0.121\n",
      "\tWord: my           \t Occurances: 1   \t Context total: 33  \t Probability: 0.030\n",
      "\tWord: is           \t Occurances: 1   \t Context total: 33  \t Probability: 0.030\n",
      "\tWord: black        \t Occurances: 2   \t Context total: 33  \t Probability: 0.061\n",
      "\tWord: car          \t Occurances: 2   \t Context total: 33  \t Probability: 0.061\n",
      "\tWord: almost       \t Occurances: 1   \t Context total: 33  \t Probability: 0.030\n",
      "\tWord: hit          \t Occurances: 1   \t Context total: 33  \t Probability: 0.030\n",
      "\tWord: the          \t Occurances: 1   \t Context total: 33  \t Probability: 0.030\n",
      "\tWord: license      \t Occurances: 1   \t Context total: 33  \t Probability: 0.030\n",
      "\tWord: tag          \t Occurances: 1   \t Context total: 33  \t Probability: 0.030\n",
      "\n",
      "Bigrams probability table\n",
      "\tWord: i            \t Occurances: 2   \t Context total: 4   \t Probability: 0.500\n",
      "\tWord: my           \t Occurances: 1   \t Context total: 4   \t Probability: 0.250\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 4   \t Probability: 0.250\n",
      "\n",
      "\tWord: have         \t Occurances: 2   \t Context total: 2   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: the          \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: cat          \t Occurances: 2   \t Context total: 3   \t Probability: 0.667\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 3   \t Probability: 0.333\n",
      "\n",
      "\tWord: .            \t Occurances: 2   \t Context total: 3   \t Probability: 0.667\n",
      "\tWord: is           \t Occurances: 1   \t Context total: 3   \t Probability: 0.333\n",
      "\n",
      "\tWord: </s>         \t Occurances: 4   \t Context total: 4   \t Probability: 1.000\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: almost       \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: license      \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: hit          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: tag          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "Trigrams probability table\n",
      "\tWord: i            \t Occurances: 2   \t Context total: 4   \t Probability: 0.500\n",
      "\tWord: my           \t Occurances: 1   \t Context total: 4   \t Probability: 0.250\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 4   \t Probability: 0.250\n",
      "\n",
      "\tWord: have         \t Occurances: 2   \t Context total: 2   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: the          \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 2   \t Context total: 2   \t Probability: 1.000\n",
      "\n",
      "\tWord: </s>         \t Occurances: 2   \t Context total: 2   \t Probability: 1.000\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: is           \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: </s>         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: almost       \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: hit          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: license      \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: tag          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: </s>         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "Quadgrams probability table\n",
      "\tWord: i            \t Occurances: 2   \t Context total: 4   \t Probability: 0.500\n",
      "\tWord: my           \t Occurances: 1   \t Context total: 4   \t Probability: 0.250\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 4   \t Probability: 0.250\n",
      "\n",
      "\tWord: have         \t Occurances: 2   \t Context total: 2   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: the          \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: </s>         \t Occurances: 2   \t Context total: 2   \t Probability: 1.000\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: is           \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: </s>         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: almost       \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: hit          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: license      \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: tag          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: </s>         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\n",
      "Paragraph level:\n",
      "Unigrams probability table\n",
      "\tWord: <s>          \t Occurances: 2   \t Context total: 29  \t Probability: 0.069\n",
      "\tWord: i            \t Occurances: 2   \t Context total: 29  \t Probability: 0.069\n",
      "\tWord: have         \t Occurances: 2   \t Context total: 29  \t Probability: 0.069\n",
      "\tWord: a            \t Occurances: 3   \t Context total: 29  \t Probability: 0.103\n",
      "\tWord: cat          \t Occurances: 3   \t Context total: 29  \t Probability: 0.103\n",
      "\tWord: .            \t Occurances: 4   \t Context total: 29  \t Probability: 0.138\n",
      "\tWord: my           \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "\tWord: is           \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "\tWord: black        \t Occurances: 2   \t Context total: 29  \t Probability: 0.069\n",
      "\tWord: </s>         \t Occurances: 2   \t Context total: 29  \t Probability: 0.069\n",
      "\tWord: car          \t Occurances: 2   \t Context total: 29  \t Probability: 0.069\n",
      "\tWord: almost       \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "\tWord: hit          \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "\tWord: the          \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "\tWord: license      \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "\tWord: tag          \t Occurances: 1   \t Context total: 29  \t Probability: 0.034\n",
      "\n",
      "Bigrams probability table\n",
      "\tWord: i            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: have         \t Occurances: 2   \t Context total: 2   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: the          \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: cat          \t Occurances: 2   \t Context total: 3   \t Probability: 0.667\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 3   \t Probability: 0.333\n",
      "\n",
      "\tWord: .            \t Occurances: 2   \t Context total: 3   \t Probability: 0.667\n",
      "\tWord: is           \t Occurances: 1   \t Context total: 3   \t Probability: 0.333\n",
      "\n",
      "\tWord: my           \t Occurances: 1   \t Context total: 4   \t Probability: 0.250\n",
      "\tWord: </s>         \t Occurances: 2   \t Context total: 4   \t Probability: 0.500\n",
      "\tWord: i            \t Occurances: 1   \t Context total: 4   \t Probability: 0.250\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: almost       \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: license      \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: hit          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: tag          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "Trigrams probability table\n",
      "\tWord: i            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: have         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: the          \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 2   \t Context total: 2   \t Probability: 1.000\n",
      "\n",
      "\tWord: my           \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: i            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: is           \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: </s>         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: almost       \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: hit          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: have         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: license      \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: tag          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: </s>         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "Quadgrams probability table\n",
      "\tWord: i            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: have         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: my           \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\tWord: i            \t Occurances: 1   \t Context total: 2   \t Probability: 0.500\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: is           \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: </s>         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: black        \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: almost       \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: hit          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: a            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: cat          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: have         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: the          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: car          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: license      \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: tag          \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: .            \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n",
      "\tWord: </s>         \t Occurances: 1   \t Context total: 1   \t Probability: 1.000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definitions of gram probabilities\n",
    "\n",
    "debug = True\n",
    "\n",
    "def calcContextTotal(grams, gram):\n",
    "\tcontextTotal = 0\n",
    "\tfor word in grams[gram]:\n",
    "\t\tcontextTotal += grams[gram][word]\n",
    "\treturn contextTotal\n",
    "\n",
    "def calcGramProb(mode, ngram, ctx, wordTest): \n",
    "\tif ctx in gramsMode[mode][ngram]:\n",
    "\t\tcontextTotal = calcContextTotal(gramsMode[mode][ngram], ctx)\n",
    "\t\treturn gramsMode[mode][ngram][ctx][wordTest]/contextTotal\n",
    "\telse:\n",
    "\t\tprint(ctx, \"is not in the dictionary!\")\n",
    "\n",
    "probModeGram = [\n",
    "    [[], [], [], []],\t# sentence probabilities [uni, bi, tri, quad]\n",
    "    [[], [], [], []] \t# paragraph probabilities [uni, bi, tri, quad]\n",
    "]\n",
    "\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0:\n",
    "\t\tprint(\"Sentence level:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"\\nParagraph level:\")\n",
    "\n",
    "\tfor ngram in range(len(gramsMode[mode])): \t\t\t\t\t# for each ngram\n",
    "\t\tprint(f\"{gramsPrintStrings[ngram]} probability table\") \t# which ngram table are we looking at\n",
    "\t\tfor ctx in gramsMode[mode][ngram]: \t\t\t\t\t\t# for each context in the gram in the sen/par mode\n",
    "\t\t\tfor word in gramsMode[mode][ngram][ctx]: \t\t\t# for each word in the current context \n",
    "\t\t\t\tprob = calcGramProb(mode, ngram, ctx, word) \t# calculate the probability of the word in the current context\n",
    "\t\t\t\tprobModeGram[mode][ngram].append(prob)\t\t\t# save the probability to the sen/par mode for each ngram\n",
    "\t\t\t\tif debug:\n",
    "\t\t\t\t\toccurances = str(gramsMode[mode][ngram][ctx][word])\n",
    "\t\t\t\t\tcontextTotal = calcContextTotal(gramsMode[mode][ngram], ctx) # calculate how many words follow the current context\n",
    "\t\t\t\t\tprint(f\"\\tWord: {word:<12} \\t Occurances: {occurances:<3} \\t Context total: {contextTotal:<3} \\t Probability: {prob:.3f}\")\n",
    "\t\t\tif debug: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities for Sentence mode:\n",
      "Unigrams probabilities: [0.12121212121212122, 0.06060606060606061, 0.06060606060606061, 0.09090909090909091, 0.09090909090909091, 0.12121212121212122, 0.12121212121212122, 0.030303030303030304, 0.030303030303030304, 0.06060606060606061, 0.06060606060606061, 0.030303030303030304, 0.030303030303030304, 0.030303030303030304, 0.030303030303030304, 0.030303030303030304]\n",
      "Bigrams probabilities: [0.5, 0.25, 0.25, 1.0, 0.5, 0.5, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Trigrams probabilities: [0.5, 0.25, 0.25, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Quadgrams probabilities: [0.5, 0.25, 0.25, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "Probabilities for Paragraph mode:\n",
      "Unigrams probabilities: [0.06896551724137931, 0.06896551724137931, 0.06896551724137931, 0.10344827586206896, 0.10344827586206896, 0.13793103448275862, 0.034482758620689655, 0.034482758620689655, 0.06896551724137931, 0.06896551724137931, 0.06896551724137931, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655, 0.034482758620689655]\n",
      "Bigrams probabilities: [0.5, 0.5, 1.0, 0.5, 0.5, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 0.25, 0.5, 0.25, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Trigrams probabilities: [0.5, 0.5, 1.0, 0.5, 0.5, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "Quadgrams probabilities: [0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# N-Gram probabilities converted to array lists\n",
    "for mode in range(2): # Sentence then Paragraph\n",
    "\tm = \"Sentence\" if mode == 0 else \"Paragraph\"\n",
    "\tprint(f\"Probabilities for {m} mode:\")\n",
    "\t\n",
    "\tfor g in range(4): # Uni, Bi, Tri, Quad grams\n",
    "\t\tprint(f\"{gramsPrintStrings[g]} probabilities:\", probModeGram[mode][g])\n",
    "\tprint()  # Add a blank line between modes for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to log space\n",
    "# log(p1 * p2 * p3 * p4) = log(p1) + log(p2) + log(p3) + log(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[359], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m seed \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m, START, END, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m---> 43\u001b[0m \tseed \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgramsMode\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprobModeGram\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \tseed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(seed[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;66;03m#convert choice to a regular string\u001b[39;00m\n\u001b[0;32m     45\u001b[0m biSeed \u001b[38;5;241m=\u001b[39m (seed,)\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:984\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (4,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "#Set up seeds\n",
    "#np.random.seed(0)\n",
    "\n",
    "def generateNextGram(mode, ngrams, topLevel, context): #(ngrams, ngrams, biSeed)\n",
    "\tgram = gramsMode[mode][ngrams] # Input n to use grams[n], which allows for backoff by decrementing n\n",
    "\t#print(f\"Generating {ngrams+1}grams\")\n",
    "\t#length = 0\n",
    "\ttry:\n",
    "\t\tif context in gram:\n",
    "\t\t\t#for word in gram[context]:\n",
    "\t\t\t#\tlength += gram[context][word]\n",
    "\t\t\tlength = sum(gram[context].values())\n",
    "\t\t\tprobArray = [gram[context][wordCount]/length for wordCount in gram[context]]\n",
    "\t\t\tnextWord = np.random.choice(list(gram[context].keys()), size=1, p=probArray)\n",
    "\t\t\tnextWord = str(nextWord[0])\n",
    "\t\t\t#print(f\"Next word: {nextWord}\")\n",
    "\t\t\treturn nextWord\n",
    "\t\telse:\n",
    "\t\t\traise KeyError(f\"{context} not found in grams[{ngrams}]\")\n",
    "\texcept KeyError:\n",
    "\t\tif ngrams > 0:\n",
    "\t\t\t#print(f\"{context} not found in {gram}\")\n",
    "\t\t\t#print(f\"Backoff to {ngrams}grams\")\n",
    "\t\t\treturn generateNextGram(ngrams-1, topLevel, context[1:] if len(context) > 1 else ())\n",
    "\t\t\t#bug: not returning to top level gram\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Backoff failed, returning '.'\")\n",
    "\t\t\treturn \".\"\n",
    "\n",
    "def setOutput(current, output, wordCount):\n",
    "\tif current not in (START, END):\n",
    "\t\tif current in (\"'\", \"’\", \",\", \".\", \":\", \"*\"): #no space after symbols\n",
    "\t\t\toutput += current\n",
    "\t\telse:\n",
    "\t\t\toutput += \" \" + current \n",
    "\t\twordCount += 1\n",
    "\treturn output, wordCount\n",
    "\n",
    "seed = \"\"\n",
    "while seed in (\"\", None, START, END, '.', \",\", \"?\", \"!\", \"[\", \"]\", \"(\", \")\"):\n",
    "\tseed = np.random.choice(list(gramsMode[0][0][()]), size=1, p=probModeGram[0])\n",
    "\tseed = str(seed[0]) #convert choice to a regular string\n",
    "biSeed = (seed,)\n",
    "triSeed = (START, seed,)\n",
    "quadSeed = (START, START, seed,)\n",
    "print(\"Seeds:\", seed, biSeed, triSeed, quadSeed)\n",
    "\n",
    "finalOutputs = [['','','',''], ['','','','']] #Output string for sentences (uni, bi, tri, quad), and paragraphs (uni, bi, tri, quad)\n",
    "\n",
    "for mode in range(len(modes)):\n",
    "\tfor g in range(len(gramsMode[mode])):\n",
    "\t\t\n",
    "\n",
    "\t\tcurrent = seed\n",
    "\t\tUniOutput = current\n",
    "\t\twordCount = 0\n",
    "\t\twhile current != END and wordCount < 150:\n",
    "\t\t\t#current = np.random.choice(list(unigrams[()]), size=1, p=probUnigram)\n",
    "\t\t\t#current = current[0]\n",
    "\n",
    "\t\t\tcurrent = generateNextGram(mode, 0, 0, ())\n",
    "\n",
    "\t\t\tUniOutput, wordCount = setOutput(current, UniOutput, wordCount)\n",
    "\t\tprint(f\"Unigram: {UniOutput}\")\n",
    "\t\tprint()\n",
    "\n",
    "\t\t#print(bigrams)\n",
    "\t\t#print(\"Possible words:\", bigrams[biSeed])\n",
    "\t\tcurrent = seed\n",
    "\t\tBiOutput = current\n",
    "\t\twordCount = 0\n",
    "\t\twhile current != END and wordCount < 150:\n",
    "\t\t\tcurrent = generateNextGram(mode, 1, 1, biSeed)\n",
    "\t\t\t#print(\"Chosen current word:\", current, \"\\n\")\n",
    "\t\t\tbiSeed = (current,)\n",
    "\n",
    "\t\t\tBiOutput, wordCount = setOutput(current, BiOutput, wordCount)\n",
    "\t\tprint(\"Bigram:\", BiOutput)\n",
    "\t\tprint()\n",
    "\n",
    "\t\tcurrent = seed\n",
    "\t\tTriOutput = current\n",
    "\t\twhile current != END and wordCount < 150:\n",
    "\t\t\tcurrent = generateNextGram(mode, 2, 2, triSeed)\n",
    "\t\t\t\n",
    "\t\t\ttriSeed = (triSeed[1],current)\n",
    "\t\t\t\n",
    "\t\t\tTriOutput, wordCount = setOutput(current, TriOutput, wordCount)\n",
    "\t\tprint(\"Trigram:\", TriOutput)\n",
    "\t\tprint()\n",
    "\n",
    "\t\tcurrent = seed\n",
    "\t\tQuadOutput = current\n",
    "\t\twhile current != END and wordCount < 150:\n",
    "\t\t\tcurrent = generateNextGram(mode, 3, 3, quadSeed)\n",
    "\t\t\t\n",
    "\t\t\tquadSeed = (quadSeed[1], quadSeed[2],current)\n",
    "\t\t\t\n",
    "\t\t\tQuadOutput, wordCount = setOutput(current, QuadOutput, wordCount)\n",
    "\t\tprint(\"Quadgram:\", QuadOutput)\n",
    "\t\tprint()\n",
    "\n",
    "finalOutputs = [UniOutput, BiOutput, TriOutput, QuadOutput]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 16 unique 1-grams\n",
      "Extracted 22 unique 2-grams\n",
      "Extracted 25 unique 3-grams\n",
      "Extracted 26 unique 4-grams\n",
      "Seed text: license\n",
      "Generated 1-gram text of length X\n",
      "license black have black a a almost. i hit the almost my cat the\n",
      "Generated 2-gram text of length X\n",
      "license tag.\n",
      "Generated 3-gram text of length X\n",
      "license tag.\n",
      "Generated 4-gram text of length X\n",
      "license tag.\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for mode in range(0,4):\n",
    "    print(f\"Extracted {uniqueSenNGrams[mode]} unique {mode+1}-grams\")\n",
    "print(\"Seed text:\", seed)\n",
    "for mode in range(0, 4):\n",
    "    print(f\"Generated {mode+1}-gram text of length X\")\n",
    "    print(f\"{finalOutputs[mode]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
