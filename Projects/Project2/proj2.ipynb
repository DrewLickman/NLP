{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Usage Disclaimer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with \\</s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paragraph level:  [['i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.'], ['a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.']]\n",
      "\n",
      "Sentence level:  [['i', 'have', 'a', 'cat', '.'], ['my', 'cat', 'is', 'black', '.'], ['a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.'], ['i', 'have', 'the', 'car', 'license', 'tag', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Imports libraries and reads corpus documents. Save the documents as tokens\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "sentences = []\n",
    "paragraphs = []\n",
    "\n",
    "with open(\"poem.txt\", encoding=\"utf-8\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower() # Converts all documents to lowercase\n",
    "        sentence = sent_tokenize(line) # Extract as entire sentences\n",
    "        paragraph = word_tokenize(line) # Extract the entire line as words (not separating sentences into different arrays!)\n",
    "        sentences.append(sentence) # Adds each sentence to the sentences array\n",
    "        paragraphs.append(paragraph) # Adds each line into the paragraphs array\n",
    "        #print(sentence)\n",
    "        ##print(paragraph)\n",
    "        #print()\n",
    "        \n",
    "#print(\"Sentences: \", sentences) #before separating sentences\n",
    "print(\"Paragraph level: \", paragraphs)\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokenizedSentences = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "        tokenizedSentences.append(tokenList)\n",
    "        \n",
    "print()\n",
    "print(\"Sentence level: \", tokenizedSentences)\n",
    "\n",
    "# Disable for large corpus\n",
    "if False:\n",
    "\tfor context in tokenizedSentences:\n",
    "\t\tprint(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence level:\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', '<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', '<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "\n",
      "Paragraph level:\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', 'i', 'have', 'a', 'cat', '.', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# Add START and END tokens\n",
    "# Make sure to Run All before re-running this!\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "#t[1] = [<s>tokenized words</s>], etc.\n",
    "#t[2] = [<s>tokenized words</s>], etc.\n",
    "#t[3] = [<s><s>tokenized words</s>], etc.\n",
    "#t[4] = [<s><s><s>tokenized words</s>], etc.\n",
    "\n",
    "AugmentedTokens = [[],[]] # [[Sentence Tokens], [Paragraph Tokens]]\n",
    "\n",
    "# Array of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "AugmentedTokens[0] = [] # [],[],[],[] #for sentences\n",
    "# Since I am modifying each sentence, for every gram, I will add the START n times and END once per sentence\n",
    "# List comprehension as suggested by Claude 3.5-sonnet: (and modifications by myself too!)\n",
    "# newList = [expression for item in iterable]\n",
    "\n",
    "#for i in range(len(AugmentedTokens)):\n",
    "#    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens] # Unfortunately cannot use this because unigrams have 1 start token, not 0\n",
    "\n",
    "UniAugmentedSTokens  = [[START]*1 + sentence + [END] for sentence in tokenizedSentences]\n",
    "BiAugmentedSTokens   = [[START]*1 + sentence + [END] for sentence in tokenizedSentences] # both unigrams and bigrams are only augmented with 1 START token\n",
    "TriAugmentedSTokens  = [[START]*2 + sentence + [END] for sentence in tokenizedSentences]\n",
    "QuadAugmentedSTokens = [[START]*3 + sentence + [END] for sentence in tokenizedSentences]\n",
    "\n",
    "AugmentedTokens[0].append(UniAugmentedSTokens)\n",
    "AugmentedTokens[0].append(BiAugmentedSTokens)\n",
    "AugmentedTokens[0].append(TriAugmentedSTokens)\n",
    "AugmentedTokens[0].append(QuadAugmentedSTokens)\n",
    "print(\"Sentence level:\")\n",
    "for i in range(len(AugmentedTokens[0])):\n",
    "    print(AugmentedTokens[0][i])\n",
    "    \n",
    "#####################################################################################\n",
    "print()\n",
    "\n",
    "AugmentedTokens[1] = [] # [],[],[],[] #for paragraphs\n",
    "UniAugmentedPTokens  = [[START]*1 + sentence + [END] for sentence in paragraphs]\n",
    "BiAugmentedPTokens   = [[START]*1 + sentence + [END] for sentence in paragraphs] # both unigrams and bigrams are only augmented with 1 START token\n",
    "TriAugmentedPTokens  = [[START]*2 + sentence + [END] for sentence in paragraphs]\n",
    "QuadAugmentedPTokens = [[START]*3 + sentence + [END] for sentence in paragraphs]\n",
    "\n",
    "AugmentedTokens[1].append(UniAugmentedPTokens)\n",
    "AugmentedTokens[1].append(BiAugmentedPTokens)\n",
    "AugmentedTokens[1].append(TriAugmentedPTokens)\n",
    "AugmentedTokens[1].append(QuadAugmentedPTokens)\n",
    "\n",
    "print(\"Paragraph level:\")\n",
    "for i in range(len(AugmentedTokens[1])):\n",
    "    print(AugmentedTokens[1][i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams\n",
      "Unique Unigrams: 16\n",
      "\n",
      "Bigrams\n",
      "Unique Bigrams: 22\n",
      "\n",
      "Trigrams\n",
      "Unique Trigrams: 25\n",
      "\n",
      "Quadgrams\n",
      "Unique Quadgrams: 26\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert augmented tokens into n-grams\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "# Using 2d dictionaries {context: {(word: 1), (word2: 2)}, context2: {(word3: 3), (word4: 4)}}\n",
    "unigrams = {}   # (): [\"word\", count]\n",
    "bigrams = {}    # (context1): [\"word\", count]\n",
    "trigrams = {}   # (c1, c2): [\"word\", count]\n",
    "quadgrams = {}  # (c1, c2, c3): [(\"word\", count)]\n",
    "grams = [unigrams, bigrams, trigrams, quadgrams]\n",
    "gramsPrintStrings = [\"Unigrams\", \"Bigrams\", \"Trigrams\", \"Quadgrams\"]\n",
    "contextCount = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "uniqueNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "\n",
    "# Helper function for repeating code\n",
    "def incrementWordCount(gramIndex, context, word):\n",
    "\tif context not in grams[gramIndex]: \t\t# if the context isn't in the gram dict, \n",
    "\t\tgrams[gramIndex][context] = {}  \t\t# create an empty dictionary\n",
    "\tif word not in grams[gramIndex][context]: \t# check if word is already found in context\n",
    "\t\tgrams[gramIndex][context][word] = 1 \t# Initialize count as 1\n",
    "\telse:\n",
    "\t\tgrams[gramIndex][context][word] += 1 \t# Increment gram word count\n",
    "\n",
    "for i in range(4): # 4 gram types\n",
    "\tif i == 0: # Calculate Unigrams\n",
    "\t\tcontext = ()\n",
    "\t\tgrams[i][context] = {} # Declare the unigrams to be a dictionary with the only key as ()\n",
    "\t\tfor tokenList in AugmentedTokens[0][i]: #0 context words\n",
    "\t\t\tfor word in tokenList:\n",
    "\t\t\t\t# No actual context, so I'm not going to use incrementWordCount(grams[i], context, word)\n",
    "\t\t\t\tif word not in grams[i][context]:\n",
    "\t\t\t\t\tgrams[i][context][word] = 1 \t\t# Add word to unigrams with count of 1\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tgrams[i][context][word] += 1 \t\t# Increment unigram token count\n",
    "\t\t\t\tcontextCount[i] += 1\n",
    "\tif i == 1: # Calculate Bigrams\n",
    "\t\tcontext = None\n",
    "\t\tfor tokenList in AugmentedTokens[0][i]: #1 context word\n",
    "\t\t\tfor word in tokenList:\n",
    "\t\t\t\tif context not in (None, END):\n",
    "\t\t\t\t\tbigramContext = (context,) # bigram dictionary key\n",
    "\t\t\t\t\tincrementWordCount(i, bigramContext, word)\n",
    "\t\t\t\tcontext = word\n",
    "\t\t\t\tcontextCount[i] += 1\n",
    "\tif i == 2: # Calculate Trigrams\n",
    "\t\tcontext = None\n",
    "\t\tcontext2 = None\n",
    "\t\tfor tokenList in AugmentedTokens[0][i]: #2 context words\n",
    "\t\t\tfor word in tokenList:\n",
    "\t\t\t\tif context not in (None, END) and context2 not in (None, END):\n",
    "\t\t\t\t\ttrigramContext = (context, context2) # trigram dictionary key\n",
    "\t\t\t\t\tincrementWordCount(i, trigramContext, word)\n",
    "\t\t\t\tcontext = context2\n",
    "\t\t\t\tcontext2 = word\n",
    "\t\t\t\tcontextCount[i] += 1\n",
    "\tif i == 3: # Calculate Quadgrams\n",
    "\t\tcontext = None\n",
    "\t\tcontext2 = None\n",
    "\t\tcontext3 = None\n",
    "\t\ti = 3\n",
    "\t\tfor tokenList in AugmentedTokens[0][i]: #3 context words\n",
    "\t\t\tfor word in tokenList:\n",
    "\t\t\t\tif context not in (None, END) and context2 not in (None, END) and context3 not in (None, END):\n",
    "\t\t\t\t\tquadgramContext = (context, context2, context3) # quadgram dictionary key\n",
    "\t\t\t\t\tincrementWordCount(i, quadgramContext, word)\n",
    "\t\t\t\tcontext = context2\n",
    "\t\t\t\tcontext2 = context3\n",
    "\t\t\t\tcontext3 = word\n",
    "\t\t\t\tcontextCount[i] += 1\n",
    "\n",
    "\n",
    "# Save the unique count of ngrams for each gram\n",
    "#Debug print statements\n",
    "\t# Print all the context and words\n",
    "\t# (Unigram context is just empty dictionary key ())\n",
    "for i in range(len(grams)):\n",
    "\tprint(f\"{gramsPrintStrings[i]}\") # Which N-Gram is being printed\n",
    "\t#for context in grams[i]: # Displays all tokens in each gram\n",
    "\t\t#print(f\"{context}: {grams[i][context]}\") #(Context,): {Dictionary of words: count}\n",
    "\n",
    "\t# Simple loop to count how many unique grams in each N-Gram\n",
    "\tfor contextWord in grams[i]:\n",
    "\t\tuniqueNGrams[i] += len(grams[i][contextWord])\n",
    "\tprint(f\"Unique {gramsPrintStrings[i]}: {uniqueNGrams[i]}\")\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Unigram probability table\n",
      "\n",
      "Bigram probability table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Trigram probability table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Quadgram probability table\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Definitions of gram probabilities\n",
    "\n",
    "debug = False\n",
    "\n",
    "def calcContextTotal(grams, gram):\n",
    "\tcontextTotal = 0\n",
    "\tfor word in grams[gram]:\n",
    "\t\tcontextTotal += grams[gram][word]\n",
    "\treturn contextTotal\n",
    "\n",
    "def unigramProb(wordTest):\n",
    "    # Probability of wordTest in its context\n",
    "\tif wordTest != None:\n",
    "\t\tprob = unigrams[()][wordTest]/contextCount[0]\n",
    "\t\treturn prob \n",
    "\telse:\n",
    "\t\tprint(wordTest, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "def bigramProb(bigram, wordTest): # 1 context word\n",
    "    # Probability of wordTest, given that its context came before it\n",
    "\tif bigram in bigrams:\n",
    "\t\tcontextTotal = calcContextTotal(bigrams, bigram)\n",
    "\t\treturn bigrams[bigram][wordTest]/contextTotal\n",
    "\telse:\n",
    "\t\tprint(bigram, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "def trigramProb(trigram, wordTest): # 2 context words\n",
    "    # Probability of wordTest, given that its context came before it\n",
    "\tif trigram in trigrams:\n",
    "\t\tcontextTotal = calcContextTotal(trigrams, trigram)\n",
    "\t\treturn trigrams[trigram][wordTest]/contextTotal\n",
    "\telse:\n",
    "\t\tprint(trigram, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "# Note: I don't think compacting this into (wordTest, trigram) would be a good idea\n",
    "def quadgramProb(quadgram, wordTest): # 3 context words\n",
    "    # Probability of wordTest, given that its context came before it\n",
    "\tif quadgram in quadgrams:\n",
    "\t\tcontextTotal = calcContextTotal(quadgrams, quadgram)\n",
    "\t\treturn quadgrams[quadgram][wordTest]/contextTotal\n",
    "\telse:\n",
    "\t\tprint(quadgram, \"is not in the dictionary!\")\n",
    "\n",
    "# P(SearchWord, (context))\n",
    "#print(unigramProb(\"have\"))\n",
    "#print(bigramProb((\"a\", \"cat\")))\n",
    "#print(trigramProb((\"car\", \"have\", \"the\")))\n",
    "#print(quadgramProb((\"cat\", \"almost\", \"hit\", \"a\")))\n",
    "\n",
    "print()\n",
    "\n",
    "probUnigram = [] # array to hold probabilities\n",
    "probBigram = []\n",
    "probTrigram = []\n",
    "probQuadgram = []\n",
    "\n",
    "\n",
    "print(\"Unigram probability table\")\n",
    "#print(unigrams)\n",
    "for unigram in unigrams[()]:\n",
    "\tprob = unigramProb(unigram)\n",
    "\tprobUnigram.append(prob)\n",
    "\tif debug:\n",
    "\t\tprint(f\"\\tWord: {unigram:<12} \\t Occurances: {str(unigrams[()][unigram]):<3} \\t Context total: {contextCount[0]:<3} \\t Probability: {prob:.3f}\")\n",
    "    # print unigram[i],                        unigram dictionary value                 context summed in previous block       unigram Prob, input string token \n",
    "\n",
    "print()\n",
    "print(\"Bigram probability table\")\n",
    "#print(bigrams)\n",
    "for bigram in bigrams:\n",
    "\tif debug:\n",
    "\t\tprint(f\"Context: {str(bigram):<12}\")\n",
    "\tfor word in bigrams[bigram]:\n",
    "\t\tprob = bigramProb(bigram, word)\n",
    "\t\tprobBigram.append(prob)\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"\\t Word: {word:<12} \\t Occurances: {str(bigrams[bigram][word]):<3} \\t Context total: {calcContextTotal(bigrams, bigram):<3} \\t Probability: {prob:.3f}\")\n",
    "\tprint()\n",
    "\n",
    "print(\"Trigram probability table\")\n",
    "#print(trigrams)\n",
    "for trigram in trigrams:\n",
    "\tif debug:\n",
    "\t\tprint(f\"Context: {str(trigram):<12}\")\n",
    "\tfor word in trigrams[trigram]:\n",
    "\t\tprob = trigramProb(trigram, word)\n",
    "\t\tprobTrigram.append(prob)\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"\\t Word: {word:<12} \\t Occurances: {str(trigrams[trigram][word]):<3} \\t Context total: {calcContextTotal(trigrams, trigram):<3} \\t Probability: {prob:.3f}\")\n",
    "\tprint()\n",
    "\n",
    "print(\"Quadgram probability table\")\n",
    "#print(quadgrams)\n",
    "for quadgram in quadgrams:\n",
    "\tif debug:\n",
    "\t\tprint(f\"Context: {str(quadgram):<12}\")\n",
    "\tfor word in quadgrams[quadgram]:\n",
    "\t\tprob = quadgramProb(quadgram, word)\n",
    "\t\tprobQuadgram.append(prob)\n",
    "\t\tif debug:\n",
    "\t\t\tprint(f\"\\t Word: {word:<12} \\t Occurances: {str(quadgrams[quadgram][word]):<3} \\t Context total: {calcContextTotal(quadgrams, quadgram):<3} \\t Probability: {prob:.3f}\")\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12121212121212122, 0.06060606060606061, 0.06060606060606061, 0.09090909090909091, 0.09090909090909091, 0.12121212121212122, 0.12121212121212122, 0.030303030303030304, 0.030303030303030304, 0.06060606060606061, 0.06060606060606061, 0.030303030303030304, 0.030303030303030304, 0.030303030303030304, 0.030303030303030304, 0.030303030303030304]\n",
      "[0.5, 0.25, 0.25, 1.0, 0.5, 0.5, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.5, 0.25, 0.25, 1.0, 0.5, 0.5, 0.6666666666666666, 0.3333333333333333, 0.6666666666666666, 0.3333333333333333, 1.0, 1.0, 1.0, 0.5, 0.5, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[0.5, 0.25, 0.25, 1.0, 0.5, 0.5, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# N-Gram probabilities converted to array lists\n",
    "print(probUnigram)\n",
    "print(probBigram)\n",
    "print(probBigram)\n",
    "print(probQuadgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to log space\n",
    "# log(p1 * p2 * p3 * p4) = log(p1) + log(p2) + log(p3) + log(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeds: have ('have',) ('<s>', 'have') ('<s>', '<s>', 'have')\n",
      "Unigram: have. have\n",
      "\n",
      "Bigram: have the car license tag.\n",
      "\n",
      "Trigram: have a cat.\n",
      "\n",
      "Quadgram: have a cat.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "#Set up seeds\n",
    "#np.random.seed(0)\n",
    "seed = \"\"\n",
    "while seed in (\"\", None, START, END, '.', \",\", \"?\", \"!\", \"[\", \"]\", \"(\", \")\"):\n",
    "\tseed = np.random.choice(list(unigrams[()]), size=1, p=probUnigram)\n",
    "\tseed = str(seed[0])\n",
    "biSeed = (seed,)\n",
    "triSeed = (START, seed,)\n",
    "quadSeed = (START, START, seed,)\n",
    "print(\"Seeds:\", seed, biSeed, triSeed, quadSeed)\n",
    "\n",
    "def generateNextGram(ngrams, topLevel, context): #(ngrams, ngrams, biSeed)\n",
    "\tgram = grams[ngrams] # Input n to use grams[n], which allows for backoff by decrementing n\n",
    "\t#print(f\"Generating {ngrams+1}grams\")\n",
    "\t#length = 0\n",
    "\ttry:\n",
    "\t\tif context in gram:\n",
    "\t\t\t#for word in gram[context]:\n",
    "\t\t\t#\tlength += gram[context][word]\n",
    "\t\t\tlength = sum(gram[context].values())\n",
    "\t\t\tprobArray = [gram[context][wordCount]/length for wordCount in gram[context]]\n",
    "\t\t\tnextWord = np.random.choice(list(gram[context].keys()), size=1, p=probArray)\n",
    "\t\t\tnextWord = str(nextWord[0])\n",
    "\t\t\t#print(f\"Next word: {nextWord}\")\n",
    "\t\t\treturn nextWord\n",
    "\t\telse:\n",
    "\t\t\traise KeyError(f\"{context} not found in grams[{ngrams}]\")\n",
    "\texcept KeyError:\n",
    "\t\tif ngrams > 0:\n",
    "\t\t\t#print(f\"{context} not found in {gram}\")\n",
    "\t\t\t#print(f\"Backoff to {ngrams}grams\")\n",
    "\t\t\treturn generateNextGram(ngrams-1, topLevel, context[1:] if len(context) > 1 else ())\n",
    "\t\t\t#bug: not returning to top level gram\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Backoff failed, returning '.'\")\n",
    "\t\t\treturn \".\"\n",
    "\n",
    "def setOutput(current, output, wordCount):\n",
    "\tif current not in (START, END):\n",
    "\t\tif current in (\"'\", \"â€™\", \",\", \".\", \":\", \"*\"): #no space after symbols\n",
    "\t\t\toutput += current\n",
    "\t\telse:\n",
    "\t\t\toutput += \" \" + current \n",
    "\t\twordCount += 1\n",
    "\treturn output, wordCount\n",
    "\n",
    "current = seed\n",
    "UniOutput = current\n",
    "wordCount = 0\n",
    "while current != END and wordCount < 150:\n",
    "\t#current = np.random.choice(list(unigrams[()]), size=1, p=probUnigram)\n",
    "\t#current = current[0]\n",
    "\n",
    "\tcurrent = generateNextGram(0, 0, ())\n",
    "\n",
    "\tUniOutput, wordCount = setOutput(current, UniOutput, wordCount)\n",
    "print(f\"Unigram: {UniOutput}\")\n",
    "print()\n",
    "\n",
    "#print(bigrams)\n",
    "#print(\"Possible words:\", bigrams[biSeed])\n",
    "current = seed\n",
    "BiOutput = current\n",
    "wordCount = 0\n",
    "while current != END and wordCount < 150:\n",
    "\tcurrent = generateNextGram(1, 1, biSeed)\n",
    "\t#print(\"Chosen current word:\", current, \"\\n\")\n",
    "\tbiSeed = (current,)\n",
    "\n",
    "\tBiOutput, wordCount = setOutput(current, BiOutput, wordCount)\n",
    "print(\"Bigram:\", BiOutput)\n",
    "print()\n",
    "\n",
    "current = seed\n",
    "TriOutput = current\n",
    "while current != END and wordCount < 150:\n",
    "\tcurrent = generateNextGram(2, 2, triSeed)\n",
    "\t\n",
    "\ttriSeed = (triSeed[1],current)\n",
    "\t\n",
    "\tTriOutput, wordCount = setOutput(current, TriOutput, wordCount)\n",
    "print(\"Trigram:\", TriOutput)\n",
    "print()\n",
    "\n",
    "current = seed\n",
    "QuadOutput = current\n",
    "while current != END and wordCount < 150:\n",
    "\tcurrent = generateNextGram(3, 3, quadSeed)\n",
    "\t\n",
    "\tquadSeed = (quadSeed[1], quadSeed[2],current)\n",
    "\t\n",
    "\tQuadOutput, wordCount = setOutput(current, QuadOutput, wordCount)\n",
    "print(\"Quadgram:\", QuadOutput)\n",
    "print()\n",
    "\n",
    "finalOutputs = [UniOutput, BiOutput, TriOutput, QuadOutput]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 16 unique 1-grams\n",
      "Extracted 22 unique 2-grams\n",
      "Extracted 25 unique 3-grams\n",
      "Extracted 26 unique 4-grams\n",
      "Seed text: have\n",
      "Generated 1-gram text of length X\n",
      "have. have\n",
      "Generated 2-gram text of length X\n",
      "have the car license tag.\n",
      "Generated 3-gram text of length X\n",
      "have a cat.\n",
      "Generated 4-gram text of length X\n",
      "have a cat.\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for i in range(0,4):\n",
    "    print(f\"Extracted {uniqueNGrams[i]} unique {i+1}-grams\")\n",
    "print(\"Seed text:\", seed)\n",
    "for i in range(0, 4):\n",
    "    print(f\"Generated {i+1}-gram text of length X\")\n",
    "    print(f\"{finalOutputs[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
