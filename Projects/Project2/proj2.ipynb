{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Usage Disclaimer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and </s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and </s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with </s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'have', 'a', 'cat', '.']\n",
      "['my', 'cat', 'is', 'black', '.']\n",
      "['a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.']\n",
      "['i', 'have', 'the', 'car', 'license', 'tag', '.']\n"
     ]
    }
   ],
   "source": [
    "# Imports libraries and reads corpus documents. Save the documents as tokens\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "sentences = []\n",
    "paragraphs = []\n",
    "\n",
    "with open(\"poem.txt\", encoding=\"utf-8\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower() # Converts all documents to lowercase\n",
    "        sentence = sent_tokenize(line) # Extract as entire sentences\n",
    "        paragraph = word_tokenize(line) # Extract the entire line as words (not separating sentences into different arrays!)\n",
    "        sentences.append(sentence) # Adds each sentence to the sentences array\n",
    "        paragraphs.append(paragraph) # Adds each line into the paragraphs array\n",
    "        #print(sentence)\n",
    "        ##print(paragraph)\n",
    "        #print()\n",
    "        \n",
    "#print(\"Sentences (not word tokenized): \", sentences)\n",
    "##print(paragraphs)\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokens = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "        tokens.append(tokenList)\n",
    "        #print(token)\n",
    "#print()\n",
    "for token in tokens:\n",
    "    print(token)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', 'i', 'have', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', '<s>', 'my', 'cat', 'is', 'black', '.', '</s>'], ['<s>', '<s>', '<s>', 'a', 'black', 'car', 'almost', 'hit', 'a', 'cat', '.', '</s>'], ['<s>', '<s>', '<s>', 'i', 'have', 'the', 'car', 'license', 'tag', '.', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# Add START and END tokens\n",
    "# Make sure to Run All before re-running this!\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "#t[1] = [<s>tokenized words</s>], etc.\n",
    "#t[2] = [<s>tokenized words</s>], etc.\n",
    "#t[3] = [<s><s>tokenized words</s>], etc.\n",
    "#t[4] = [<s><s><s>tokenized words</s>], etc.\n",
    "\n",
    "# Array of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "AugmentedTokens = [] # [],[],[],[]\n",
    "# Since I am modifying each sentence, for every gram, I will add the START n times and END once per sentence\n",
    "# List comprehension as suggested by Claude 3.5-sonnet: (and modifications by myself too!)\n",
    "# newList = [expression for item in iterable]\n",
    "\n",
    "# I may need to adjust the count of START and END symbols (slide 17 of n-grams)\n",
    "\n",
    "#for i in range(len(AugmentedTokens)):\n",
    "#    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens]\n",
    "# Even more compact version of all this\n",
    "UniAugmentedTokens  = [[START]*1 + sentence + [END] for sentence in tokens]\n",
    "BiAugmentedTokens   = [[START]*1 + sentence + [END] for sentence in tokens] # both unigrams and bigrams are only augmented with 1 START token\n",
    "TriAugmentedTokens  = [[START]*2 + sentence + [END] for sentence in tokens]\n",
    "QuadAugmentedTokens = [[START]*3 + sentence + [END] for sentence in tokens]\n",
    "\n",
    "AugmentedTokens.append(UniAugmentedTokens)\n",
    "AugmentedTokens.append(BiAugmentedTokens)\n",
    "AugmentedTokens.append(TriAugmentedTokens)\n",
    "AugmentedTokens.append(QuadAugmentedTokens)\n",
    "\n",
    "for i in range(len(AugmentedTokens)):\n",
    "    print(AugmentedTokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'i'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[286], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \t\tgrams[i][quadgramContext][word] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Initialize count as 1\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \t\u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 87\u001b[0m \t\t\u001b[43mgrams\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mquadgramContext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;66;03m# Increment quadgram count\u001b[39;00m\n\u001b[0;32m     88\u001b[0m context \u001b[38;5;241m=\u001b[39m context2\n\u001b[0;32m     89\u001b[0m context2 \u001b[38;5;241m=\u001b[39m context3\n",
      "\u001b[1;31mKeyError\u001b[0m: 'i'"
     ]
    }
   ],
   "source": [
    "# Convert augmented tokens into n-grams\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "# Using 2d dictionaries {context: {(word: 1), (word2: 2)}, context2: {(word3: 3), (word4: 4)}}\n",
    "unigrams = {}   # (): [\"word\", count]\n",
    "bigrams = {}    # (context1): [\"word\", count]\n",
    "trigrams = {}   # (c1, c2): [\"word\", count]\n",
    "quadgrams = {}  # (c1, c2, c3): [(\"word\", count)]\n",
    "grams = [unigrams, bigrams, trigrams, quadgrams]\n",
    "gramsPrintStrings = [\"Unigrams\", \"Bigrams\", \"Trigrams\", \"Quadgrams\"]\n",
    "\n",
    "contextCount = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "uniqueNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "\n",
    "# Count unigrams\n",
    "i = 0\n",
    "count = 0\n",
    "context = ()\n",
    "grams[i][context] = {} # Declare the unigrams to have a key () and value []\n",
    "for tokenList in AugmentedTokens[i]: #0 context words\n",
    "\tfor word in tokenList:\n",
    "\t\t\n",
    "\t\tif word not in grams[i][context]:\n",
    "\t\t\tgrams[i][context][word] = 1 # Add word to unigrams with count of 1\n",
    "\t\telse:\n",
    "\t\t\tgrams[i][context][word] += 1 # Increment unigram token count\n",
    "\n",
    "uniqueNGrams[i] = len(grams[i][context])\n",
    "\n",
    "# Count bigrams\n",
    "context = None\n",
    "i = 1\n",
    "for tokenList in AugmentedTokens[i]: #1 context word\n",
    "\tfor word in tokenList:\n",
    "\t\tif context not in (None, END):\n",
    "\t\t\tif context not in grams[i]: # if the context isn't in the bigram dict, \n",
    "\t\t\t\tgrams[i][context] = {}  # create an empty dictionary\n",
    "\t\t\twordInContext = next((dict for dict in grams[i][context] if word in dict), None) # search if word is in the list of dictionaries\n",
    "\t\t\t\n",
    "\t\t\tif wordInContext:\n",
    "\t\t\t\tgrams[i][context][word] += 1 # Increment bigram {context: {word: n}} count\n",
    "\t\t\telse:\n",
    "\t\t\t\tgrams[i][context][word] = 1 # Initialize count as 1\n",
    "\t\tcontext = word\n",
    "\n",
    "for contextWord in grams[i]:\n",
    "\tuniqueNGrams[i] += len(grams[i][contextWord])\n",
    "\n",
    "# Count trigrams\n",
    "context = None\n",
    "context2 = None\n",
    "i = 2\n",
    "for tokenList in AugmentedTokens[i]: #2 context words\n",
    "\tfor word in tokenList:\n",
    "\t\tif context not in (None, END) and context2 not in (None, END):\n",
    "\t\t\ttrigramContext = (context, context2) # trigram dictionary key\n",
    "\t\t\tif trigramContext not in grams[i]: # if the context isn't in the trigram dict, \n",
    "\t\t\t\tgrams[i][trigramContext] = {}  # create an empty dictionary\n",
    "\t\t\twordInContext = next((dict for dict in grams[i][trigramContext] if word in dict), None) # search if word is in the list of dictionaries\n",
    "\t\t\t\n",
    "\t\t\tif wordInContext not in grams[i]:\n",
    "\t\t\t\tgrams[i][trigramContext][word] = 1 # Initialize count as 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tgrams[i][trigramContext][word] += 1 # Increment trigram count\n",
    "\t\tcontext = context2\n",
    "\t\tcontext2 = word\n",
    "\n",
    "for contextWord in grams[i]:\n",
    "\tuniqueNGrams[i] += len(grams[i][contextWord])\n",
    "\n",
    "# Count quadgrams\n",
    "context = None\n",
    "context2 = None\n",
    "context3 = None\n",
    "i = 3\n",
    "for tokenList in AugmentedTokens[i]: #3 context words\n",
    "\tfor word in tokenList:\n",
    "\t\tif context not in (None, END) and context2 not in (None, END) and context3 not in (None, END):\n",
    "\t\t\tquadgramContext = (context, context2, context3) # quadgram dictionary key\n",
    "\t\t\tif quadgramContext not in grams[i]: # if the context isn't in the quadgram dict, \n",
    "\t\t\t\tgrams[i][quadgramContext] = {}  # create an empty dictionary\n",
    "\t\t\twordInContext = next((dict for dict in grams[i][quadgramContext] if word in dict), None) # search if word is in the list of dictionaries\n",
    "\n",
    "\t\t\tif wordInContext not in grams[i]:\n",
    "\t\t\t\tgrams[i][quadgramContext][word] = 1 # Initialize count as 1\n",
    "\t\t\telse:\n",
    "\t\t\t\tgrams[i][quadgramContext][word] += 1 # Increment quadgram count\n",
    "\t\tcontext = context2\n",
    "\t\tcontext2 = context3\n",
    "\t\tcontext3 = word\n",
    "\n",
    "for contextWord in grams[i]:\n",
    "\tuniqueNGrams[i] += len(grams[i][contextWord])\n",
    "\n",
    "# Unigrams are so special that they get their own print block\n",
    "print(f\"{gramsPrintStrings[0]}:\", grams[0])\n",
    "print(f\"Unique {gramsPrintStrings[0]}: {len(grams[0][()])}\") #uniqueNGrams[i]\n",
    "#print(\"Context total:\", contextCount[0])\n",
    "print()\n",
    "\n",
    "for i in range(1, len(grams)): #excluding unigrams\n",
    "\tprint(f\"{gramsPrintStrings[i]}:\", grams[i])\n",
    "\t#print(f\"Sorted {gramsStrings[i]}:\", sorted(grams[i]))\n",
    "\tprint(f\"Unique {gramsPrintStrings[i]}: {uniqueNGrams[i]}\")\n",
    "\t\n",
    "\t#print(\"Context total:\", contextCount[i]) #doesn't make since to be here\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Debug:\n",
      "\n",
      "Unigram probability table\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[280], line 63\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnigram probability table\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m unigram \u001b[38;5;129;01min\u001b[39;00m unigrams[()]:\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# Sorry, this hurts the soul\u001b[39;00m\n\u001b[1;32m---> 63\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munigram[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Occurances: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrams[\u001b[38;5;241m0\u001b[39m][()][\u001b[43munigrams\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m(unigram)][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Context total: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontextCount[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m Probability: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00munigramProb(unigram[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m<3\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;66;03m# print unigram[i],                          unigram[index of tuple][unigram count]                     context summed in previous block       unigram Prob, input string token \u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28mprint\u001b[39m(unigrams)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "# Definitions of gram probabilities\n",
    "print(\"Debug:\")\n",
    "\n",
    "def unigramProb(wordTest):\n",
    "    # Computes P(Wi)\n",
    "    # Probability of word test\n",
    "\ttupleWord = next((tup for tup in grams[0][()] if tup[0] == wordTest), None) # Is wordTest in the unigram tuple list?\n",
    "\tif tupleWord != None:\n",
    "\t\tindex = grams[0][()].index(tupleWord) # Get the index of the tuple\n",
    "\t\tprob = f\"{unigrams[()][index][1]/contextCount[0]:.3f}\" # .3f rounds to hundredths decimal\n",
    "\t\tgrams[0][()][index] = (grams[0][()][index][0], grams[0][()][index][1], prob) # Save the probability into the tuple\n",
    "\t\treturn prob \n",
    "\telse:\n",
    "\t\tprint(wordTest, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "def bigramProb(bigram): # 1 context word\n",
    "    # Computes P(Wi|Wi-1)\n",
    "    # Probability of word test, given that its context came before it\n",
    "    if bigram in bigrams.keys():\n",
    "        #print(\"P(\", bigram, \"|\", contextWord,\") = \", bigrams[bigram], \"/\", unigrams[contextWord], \"=\")\n",
    "        #print(f\"{bigrams[bigram]/unigrams[contextWord]:.2f}\") # .3f rounds to hundredths decimal\n",
    "        return f\"{bigrams[bigram]/contextCount[1]:.3f}\"\n",
    "    else:\n",
    "        print(bigram, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "def trigramProb(wordTest, contextWord, contextWord2): # 2 context words\n",
    "    # Computes P(Wi|Wi-2,Wi-1)\n",
    "    # Probability of word test, given that its context came before it\n",
    "    trigram = (contextWord, contextWord2, wordTest)\n",
    "    bigram = (contextWord, contextWord2)\n",
    "    if trigram in trigrams.keys() and bigram in bigrams.keys():\n",
    "        #print(\"P(\", trigram, \"|\", bigram, \") = \", trigrams[trigram], \"/\", bigrams[bigram], \"=\")\n",
    "        print(f\"{trigrams[trigram]/bigrams[bigram]:.3f}\") # .3f rounds to hundredths decimal\n",
    "    else:\n",
    "        print(bigram, \"or\", trigram, \"is not in the dictionary!\")\n",
    "###\n",
    "\n",
    "# Note: I don't think compacting this into (wordTest, trigram) would be a good idea\n",
    "def quadgramProb(wordTest, contextWord, contextWord2, contextWord3): # 3 context words\n",
    "    # Computes P(Wi|Wi-3,wi-2,Wi-1) \n",
    "    # Probability of word test, given that its context came before it\n",
    "    quadgram = (contextWord, contextWord2, contextWord3, wordTest)\n",
    "    trigram = (contextWord2, contextWord3, wordTest)\n",
    "    if quadgram in quadgrams.keys() and trigram in trigrams.keys():\n",
    "        #print(\"P(\", quadgram, \"|\", trigram, \") = \", quadgrams[quadgram], \"/\", trigrams[trigram], \"=\")\n",
    "        print(f\"{quadgrams[quadgram]/trigrams[trigram]:.3f}\") # .3f rounds to hundredths decimal\n",
    "    else:\n",
    "        print(trigram, \"or\", quadgram, \"is not in the dictionary!\")\n",
    "\n",
    "# P(SearchWord, (context))\n",
    "#print(unigramProb(\"have\"))\n",
    "#print(bigramProb((\"a\", \"cat\")))\n",
    "#print(trigramProb((\"car\", \"have\", \"the\")))\n",
    "#print(quadgramProb((\"cat\", \"almost\", \"hit\", \"a\")))\n",
    "\n",
    "print()\n",
    "\n",
    "print(\"Unigram probability table\")\n",
    "for unigram in unigrams[()]:\n",
    "    # Sorry, this hurts the soul\n",
    "    print(f\"Word: {unigram[0]:<3} \\t Occurances: {grams[0][()][unigrams[()].index(unigram)][1]:<3} \\t Context total: {contextCount[0]:<3} \\t Probability: {unigramProb(unigram[0]):<3}\")\n",
    "    # print unigram[i],                          unigram[index of tuple][unigram count]                     context summed in previous block       unigram Prob, input string token \n",
    "\n",
    "print(unigrams)\n",
    "\n",
    "print()\n",
    "print(\"Bigram probability table\")\n",
    "for bigram in bigrams:\n",
    "    print(f\"Word: {bigram[0]:<3} \\t Occurances: {bigrams.get(bigram):<3} \\t Context total: {contextCount[1]:<3} \\t Probability: {bigramProb(bigram):<3}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'probUnigram' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate probabilities of each gram\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Haha nevermind, do this in the previous code block\u001b[39;00m\n\u001b[0;32m      3\u001b[0m                 \u001b[38;5;66;03m# [key.value (count of word) / total # of grams] for each gram\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m probUnigram\t\t\u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mprobUnigram\u001b[49m\u001b[43m(\u001b[49m\u001b[43munigram\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munigram\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43munigrams\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m      5\u001b[0m probBigram\t\t\u001b[38;5;241m=\u001b[39m [bigrams\u001b[38;5;241m.\u001b[39mget(bigram) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(bigrams) \u001b[38;5;28;01mfor\u001b[39;00m bigram \u001b[38;5;129;01min\u001b[39;00m bigrams]\n\u001b[0;32m      6\u001b[0m probTrigram\t\t\u001b[38;5;241m=\u001b[39m [trigrams\u001b[38;5;241m.\u001b[39mget(trigram) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trigrams) \u001b[38;5;28;01mfor\u001b[39;00m trigram \u001b[38;5;129;01min\u001b[39;00m trigrams]\n",
      "Cell \u001b[1;32mIn[112], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Calculate probabilities of each gram\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Haha nevermind, do this in the previous code block\u001b[39;00m\n\u001b[0;32m      3\u001b[0m                 \u001b[38;5;66;03m# [key.value (count of word) / total # of grams] for each gram\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m probUnigram\t\t\u001b[38;5;241m=\u001b[39m [\u001b[43mprobUnigram\u001b[49m(unigram) \u001b[38;5;28;01mfor\u001b[39;00m unigram \u001b[38;5;129;01min\u001b[39;00m unigrams]\n\u001b[0;32m      5\u001b[0m probBigram\t\t\u001b[38;5;241m=\u001b[39m [bigrams\u001b[38;5;241m.\u001b[39mget(bigram) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(bigrams) \u001b[38;5;28;01mfor\u001b[39;00m bigram \u001b[38;5;129;01min\u001b[39;00m bigrams]\n\u001b[0;32m      6\u001b[0m probTrigram\t\t\u001b[38;5;241m=\u001b[39m [trigrams\u001b[38;5;241m.\u001b[39mget(trigram) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(trigrams) \u001b[38;5;28;01mfor\u001b[39;00m trigram \u001b[38;5;129;01min\u001b[39;00m trigrams]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'probUnigram' is not defined"
     ]
    }
   ],
   "source": [
    "# Calculate probabilities of each gram\n",
    "# Haha nevermind, do this in the previous code block\n",
    "                # [key.value (count of word) / total # of grams] for each gram\n",
    "probUnigram\t\t= [probUnigram(unigram) for unigram in unigrams]\n",
    "probBigram\t\t= [bigrams.get(bigram) / len(bigrams) for bigram in bigrams]\n",
    "probTrigram\t\t= [trigrams.get(trigram) / len(trigrams) for trigram in trigrams]\n",
    "probQuadgram\t= [quadgrams.get(quadgram) / len(quadgrams) for quadgram in quadgrams]\n",
    "print(probUnigram)\n",
    "print(probBigram)\n",
    "print(probBigram)\n",
    "print(probQuadgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to log space\n",
    "# log(p1 * p2 * p3 * p4) = log(p1) + log(p2) + log(p3) + log(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 16\n",
      "Sum of probabilties: 2.0625\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "a must be 1-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[687], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSum of probabilties:\u001b[39m\u001b[38;5;124m\"\u001b[39m, pSum)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m current \u001b[38;5;241m!=\u001b[39m END:\n\u001b[1;32m---> 11\u001b[0m     current \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoice\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43munigrams\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprobUnigram\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m current \u001b[38;5;241m!=\u001b[39m START \u001b[38;5;129;01mand\u001b[39;00m current \u001b[38;5;241m!=\u001b[39m END:\n\u001b[0;32m     13\u001b[0m     \toutput \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m current \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mnumpy\\\\random\\\\mtrand.pyx:970\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: a must be 1-dimensional"
     ]
    }
   ],
   "source": [
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "current = \"\"\n",
    "output = \"\"\n",
    "print(len(list(unigrams)), len(probUnigram))\n",
    "pSum = 0\n",
    "# Need to update this to use the unigram tuples\n",
    "for p in probUnigram:\n",
    "\tpSum += p\n",
    "print(\"Sum of probabilties:\", pSum)\n",
    "while current != END:\n",
    "    current = np.random.choice(list(unigrams), size=1, p=probUnigram)\n",
    "    if current != START and current != END:\n",
    "        output += current + \" \"\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 5 unique 1-grams\n",
      "Extracted 6 unique 2-grams\n",
      "Extracted 7 unique 3-grams\n",
      "Extracted 8 unique 4-grams\n",
      "Seed text: YYYY\n",
      "Generated 1-gram text of length X\n",
      "<1-gram text generated>\n",
      "Generated 2-gram text of length X\n",
      "<2-gram text generated>\n",
      "Generated 3-gram text of length X\n",
      "<3-gram text generated>\n",
      "Generated 4-gram text of length X\n",
      "<4-gram text generated>\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for i in range(1,5):\n",
    "    print(f\"Extracted {len(grams[i-1])} unique {i}-grams\")\n",
    "print(\"Seed text:\", \"YYYY\")\n",
    "for i in range(1, 5):\n",
    "    print(f\"Generated {i}-gram text of length X\")\n",
    "    print(f\"<{i}-gram text generated>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
