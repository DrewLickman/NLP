{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Usage Disclaimer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and \\</s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with \\</s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports libraries and reads corpus documents. Save the documents as tokens\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "sentences = []\n",
    "tokenizedParagraphs = []\n",
    "\n",
    "with open(\"shakespeare.txt\", encoding=\"utf-8\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower() # Converts all documents to lowercase\n",
    "        sentence = sent_tokenize(line) # Extract as entire sentences\n",
    "        paragraph = word_tokenize(line) # Extract the entire line as words (not separating sentences into different arrays!)\n",
    "        sentences.append(sentence) # Adds each sentence to the sentences array\n",
    "        tokenizedParagraphs.append(paragraph) # Adds each line into the paragraphs array\n",
    "        #print(sentence)\n",
    "        ##print(paragraph)\n",
    "        #print()\n",
    "        \n",
    "#print(\"Sentences: \", sentences) #before separating sentences\n",
    "print(\"Paragraph level: \", tokenizedParagraphs)\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokenizedSentences = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "        tokenizedSentences.append(tokenList)\n",
    "        \n",
    "print()\n",
    "print(\"Sentence level: \", tokenizedSentences)\n",
    "\n",
    "# Disable for large corpus\n",
    "if False:\n",
    "\tfor context in tokenizedSentences:\n",
    "\t\tprint(context)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add START and END tokens\n",
    "# Make sure to Run All before re-running this!\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "#t[1] = [<s>tokenized words</s>], etc.\n",
    "#t[2] = [<s>tokenized words</s>], etc.\n",
    "#t[3] = [<s><s>tokenized words</s>], etc.\n",
    "#t[4] = [<s><s><s>tokenized words</s>], etc.\n",
    "\n",
    "AugmentedTokens = [[],[]] # [[Sentence Tokens], [Paragraph Tokens]]\n",
    "modes = [tokenizedSentences, tokenizedParagraphs]\n",
    "\n",
    "# Arrays of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "AugmentedTokens[0] = [] # [],[],[],[] #for sentences\n",
    "AugmentedTokens[1] = [] # [],[],[],[] #for paragraphs\n",
    "\n",
    "# Since I am modifying each sentence, for every gram, I will add the START n times and END once per sentence\n",
    "# List comprehension as suggested by Claude 3.5-sonnet: (and modifications by myself too!)\n",
    "# newList = [expression for item in iterable]\n",
    "\n",
    "#for i in range(len(AugmentedTokens)):\n",
    "#    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens] # Unfortunately cannot use this because unigrams have 1 start token, not 0\n",
    "\n",
    "print(\"Sentence level: ↓\\n\")\n",
    "for mode in range(2): # Sentence mode then Paragraph mode\n",
    "\tAugmentedTokens[mode].append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented unigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*1 + sentence + [END] for sentence in modes[mode]]) # Append augmented bigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*2 + sentence + [END] for sentence in modes[mode]]) # Append augmented trigram sentence/paragraph to AugmentedTokens\n",
    "\tAugmentedTokens[mode].append([[START]*3 + sentence + [END] for sentence in modes[mode]]) # Append augmented quadgram sentence/paragraph to AugmentedTokens\n",
    "\n",
    "\t# Prints sentence level of augmented grams, followed by paragraph level of augmented grams\n",
    "\tfor ngram in range(len(AugmentedTokens[mode])):\n",
    "\t\tprint(AugmentedTokens[mode][ngram])\n",
    "\tprint()\n",
    "print(\"Paragraph level: ↑\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert augmented tokens into n-grams\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "# Using 2d dictionaries {context: {(word: 1), (word2: 2)}, context2: {(word3: 3), (word4: 4)}}\n",
    "gramsPrintStrings = [\"Unigrams\", \"Bigrams\", \"Trigrams\", \"Quadgrams\"]\n",
    "\n",
    "contextCountSen = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "uniqueSenNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "\n",
    "uniqueParNGrams = [0,0,0,0] # Counts unique N-Grams for each N-Gram\n",
    "contextCountPar = [0,0,0,0] # [unigrams, bigrams, trigrams, quadgrams] total context count each\n",
    "\n",
    "gramsMode = [[{}, {}, {}, {}], [{}, {}, {}, {}]] \t# [[{sentenceUni}, {sentenceBi}, {sentenceTri}, {sentenceQuadi}],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# [{paragraphUni}, {paragraphBi}, {paragraphTri}, {paragraphQuad}]]\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# Each dictionary holds a tuple key (context) and a dictionary value of the {word: count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1, c2): {\"word\", count}\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t# (c1, c2, c3): {(\"word\", count)}\n",
    "\n",
    "contextCountMode = [contextCountSen, contextCountPar]\n",
    "uniqueModeNGrams = [uniqueSenNGrams, uniqueParNGrams]\n",
    "\n",
    "\n",
    "# Helper function for repeating code\n",
    "def incrementWordCount(mode, gramIndex, context, word):\n",
    "\tif context not in gramsMode[mode][gramIndex]: \t\t# if the context isn't in the gram dict, \n",
    "\t\tgramsMode[mode][gramIndex][context] = {}  \t\t# create an empty dictionary\n",
    "\tif word not in gramsMode[mode][gramIndex][context]: # check if word is already found in context\n",
    "\t\tgramsMode[mode][gramIndex][context][word] = 1 \t# Initialize count as 1\n",
    "\telse:\n",
    "\t\tgramsMode[mode][gramIndex][context][word] += 1 \t# Increment gram word count\n",
    "\n",
    "for mode in range(2): # Sentence then Paragraph level\n",
    "\tfor ngram in range(4): # 4 gram types\n",
    "\t\tif ngram == 0: # Calculate Unigrams\n",
    "\t\t\tcontext = ()\n",
    "\t\t\tgramsMode[mode][ngram][context] = {} # Declare the unigrams to be a dictionary with the only key as ()\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #0 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\t# No actual context, so I'm not going to use incrementWordCount(grams[i], context, word)\n",
    "\t\t\t\t\tif word not in gramsMode[mode][ngram][context]:\n",
    "\t\t\t\t\t\tgramsMode[mode][ngram][context][word] = 1 \t\t# Add word to unigrams with count of 1\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tgramsMode[mode][ngram][context][word] += 1 \t\t# Increment unigram token count\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\t\tif ngram == 1: # Calculate Bigrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #1 context word\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END):\n",
    "\t\t\t\t\t\tbigramContext = (context,) # bigram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, ngram, bigramContext, word)\n",
    "\t\t\t\t\tcontext = word\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\t\tif ngram == 2: # Calculate Trigrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tcontext2 = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #2 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END) and context2 not in (None, END):\n",
    "\t\t\t\t\t\ttrigramContext = (context, context2) # trigram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, ngram, trigramContext, word)\n",
    "\t\t\t\t\tcontext = context2\n",
    "\t\t\t\t\tcontext2 = word\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\t\tif ngram == 3: # Calculate Quadgrams\n",
    "\t\t\tcontext = None\n",
    "\t\t\tcontext2 = None\n",
    "\t\t\tcontext3 = None\n",
    "\t\t\tfor tokenList in AugmentedTokens[mode][ngram]: #3 context words\n",
    "\t\t\t\tfor word in tokenList:\n",
    "\t\t\t\t\tif context not in (None, END) and context2 not in (None, END) and context3 not in (None, END):\n",
    "\t\t\t\t\t\tquadgramContext = (context, context2, context3) # quadgram dictionary key\n",
    "\t\t\t\t\t\tincrementWordCount(mode, ngram, quadgramContext, word)\n",
    "\t\t\t\t\tcontext = context2\n",
    "\t\t\t\t\tcontext2 = context3\n",
    "\t\t\t\t\tcontext3 = word\n",
    "\t\t\t\t\tcontextCountMode[mode][ngram] += 1\n",
    "\n",
    "\n",
    "# Save the unique count of ngrams for each gram\n",
    "#Debug print statements\n",
    "\t# Print all the context and words\n",
    "\t# (Unigram context is just empty dictionary key ())\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0:\n",
    "\t\tprint(\"Sentence level:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"Paragraph level:\")\n",
    "\t\t\n",
    "\tfor ngram in range(len(gramsMode[mode])):\n",
    "\t\tprint(f\"{gramsPrintStrings[ngram]}\") # Which N-Gram is being printed\n",
    "\t\t#for context in grams[i]: # Displays all tokens in each gram\n",
    "\t\t\t#print(f\"{context}: {grams[i][context]}\") #(Context,): {Dictionary of words: count}\n",
    "\n",
    "\t\t# Simple loop to count how many unique grams in each N-Gram\n",
    "\t\tfor contextWord in gramsMode[mode][ngram]: #switch to grams for each mode\n",
    "\t\t\tuniqueModeNGrams[mode][ngram] += len(gramsMode[mode][ngram][contextWord]) #need to switch to grams for each mode\n",
    "\t\tprint(f\"Unique {gramsPrintStrings[ngram]}: {uniqueModeNGrams[mode][ngram]}\")\n",
    "\t\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gram probability tables\n",
    "# Takes a few minutes to run\n",
    "\n",
    "debug = False\n",
    "\n",
    "contextTotalsMode = [{},{}] # Lookup table for sentence and paragraphs to get count of each context unit\n",
    "\t\t\t\t\t\t\t# Use contextTotalsMode[mode][context] to access\n",
    "def calcContextTotal(mode, grams, context):\n",
    "\tif context in contextTotalsMode[mode]:\n",
    "\t\treturn contextTotalsMode[mode][context]\n",
    "\tcontextTotal = sum(gramsMode[mode][grams][context].values()) \n",
    "\tcontextTotalsMode[mode][context] = contextTotal\n",
    "\t#contextTotal = 0\n",
    "\t#for word in grams[gram]:\n",
    "\t#\tcontextTotal += grams[gram][word]\n",
    "\t#print(f\"{mode,grams,context} calculated {contextTotal}\")\n",
    "\treturn contextTotal\n",
    "\n",
    "def calcGramProb(mode, ngram, ctx, wordTest): \n",
    "\tif ctx in gramsMode[mode][ngram]:\n",
    "\t\tcontextTotal = calcContextTotal(gramsMode[mode][ngram], ctx)\n",
    "\t\treturn gramsMode[mode][ngram][ctx][wordTest]/contextTotal\n",
    "\telse:\n",
    "\t\tprint(ctx, \"is not in the dictionary!\")\n",
    "\n",
    "probModeGram = [\n",
    "    [[], [], [], []],\t# sentence probabilities [uni, bi, tri, quad]\n",
    "    [[], [], [], []] \t# paragraph probabilities [uni, bi, tri, quad]\n",
    "]\n",
    "\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0:\n",
    "\t\tprint(\"Sentence level:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"\\nParagraph level:\")\n",
    "\n",
    "\tfor ngram in range(len(gramsMode[mode])): \t\t\t\t\t\t\t\t\t# for each ngram\n",
    "\t\tprint(f\"{gramsPrintStrings[ngram]} probability table\") \t\t\t\t\t# which ngram table are we looking at\n",
    "\t\tfor ctx in gramsMode[mode][ngram]:\t\t\t\t\t\t\t\t\t\t# for each context in the gram in the sen/par mode\n",
    "\t\t\tcontextTotal = calcContextTotal(mode, ngram, ctx) \t\t\t\t\t# calculate how many words follow the current context\n",
    "\t\t\tfor word in gramsMode[mode][ngram][ctx]:\t\t\t\t\t\t\t# for each word in the current context \n",
    "\t\t\t\tcontextCount = gramsMode[mode][ngram][ctx][word]\n",
    "\t\t\t\t#print(ctx, word, contextCount, contextTotal)\n",
    "\t\t\t\tprob = contextCount/contextTotal \t\t\t\t\t\t\t\t# calculate the probability of the word in the current context\n",
    "\t\t\t\tprobModeGram[mode][ngram].append(prob)\t\t\t\t\t\t\t# save the probability to the sen/par mode for each ngram\n",
    "\t\t\t\tif debug:\n",
    "\t\t\t\t\toccurances = str(gramsMode[mode][ngram][ctx][word])\n",
    "\t\t\t\t\tprint(f\"\\tWord: {word:<12} \\t Occurances: {occurances:<3} \\t Context total: {contextTotal:<3} \\t Probability: {prob:.3f}\")\n",
    "\t\t\tif debug: print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-Gram probabilities converted to array lists\n",
    "for mode in range(2): # Sentence then Paragraph\n",
    "\tm = \"Sentence\" if mode == 0 else \"Paragraph\"\n",
    "\tprint(f\"Probabilities for {m} mode:\")\n",
    "\t\n",
    "\t#for g in range(4): # Uni, Bi, Tri, Quad grams\n",
    "\t#\tprint(f\"{gramsPrintStrings[g]} probabilities:\", probModeGram[mode][g])\n",
    "\tprint()  # Add a blank line between modes for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "#Set up seeds\n",
    "np.random.seed(0)\n",
    "\n",
    "def generateNextGram(mode, ngrams, topLevel, context): #(mode, ngrams, ngrams, biSeed)\n",
    "\tgram = gramsMode[mode][ngrams] # Input n to use grams[n], which allows for backoff by decrementing n\n",
    "\t#print(f\"Generating {gramsPrintStrings[ngrams]}\")\n",
    "\t#length = 0\n",
    "\ttry:\n",
    "\t\tif context in gram:\n",
    "\t\t\tlength = sum(gram[context].values())\n",
    "\t\t\tprobArray = [gram[context][wordCount]/length for wordCount in gram[context]]\n",
    "\t\t\tif False:\n",
    "\t\t\t\tprint(f\"Current context: {context}\")\n",
    "\t\t\t\tif ngrams >= 1:\n",
    "\t\t\t\t\tprint(f\"Possible choices: {list(gramsMode[mode][ngrams][context].keys())}\")\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tprint(f\"Possible choices: (any unigram)\")\n",
    "\t\t\tnextWord = np.random.choice(list(gramsMode[mode][ngrams][context].keys()), size=1, p=probArray)\n",
    "\t\t\tnextWord = str(nextWord[0])\n",
    "\t\t\t#print(f\"Next word: {nextWord}\")\n",
    "\t\t\treturn nextWord\n",
    "\t\telse:\n",
    "\t\t\traise KeyError(f\"{context} not found in grams[{ngrams}]\")\n",
    "\texcept KeyError:\n",
    "\t\tif ngrams > 0:\n",
    "\t\t\t#print(f\"{context} not found in {gram}\")\n",
    "\t\t\t#print(f\"Backoff to {ngrams}grams\")\n",
    "\t\t\treturn generateNextGram(mode, ngrams-1, topLevel, context[1:] if len(context) > 1 else ())\n",
    "\t\t\t#bug: not returning to top level gram #still true? idk\n",
    "\t\telse:\n",
    "\t\t\t##print(f\"Backoff failed, context was \\\"{context}\\\" in mode {mode} during ngram {ngrams}. Returning '.'\")\n",
    "\t\t\treturn \".\"\n",
    "\n",
    "def setOutput(currentCtx, output, wordCount):\n",
    "\tif currentCtx not in (START, END):\n",
    "\t\tif currentCtx in (\"'\", \"’\", \",\", \".\", \":\", \"*\", \"?\", \";\") or output[-1] in (\"'\", \"’\"): #no space before symbols\n",
    "\t\t\toutput += currentCtx\n",
    "\t\telse:\n",
    "\t\t\toutput += \" \" + currentCtx\n",
    "\t\twordCount += 1\n",
    "\treturn output, wordCount\n",
    "\n",
    "seed = \"\"\n",
    "while seed in (\"\", None, START, END, '.', \",\", \"?\", \"!\", \"[\", \"]\", \"(\", \")\"):\n",
    "\tseed = np.random.choice(list(gramsMode[0][0][()]), size=1, p=probModeGram[0][0])\n",
    "\tseed = str(seed[0]) #convert choice to a regular string\n",
    "#seed = \"have\" #debug\n",
    "biSeed = (seed,)\n",
    "triSeed = (START, seed,)\n",
    "quadSeed = (START, START, seed,)\n",
    "seeds = seed, biSeed, triSeed, quadSeed\n",
    "print(\"Seeds:\", seed, biSeed, triSeed, quadSeed)\n",
    "\n",
    "finalOutputs = [['','','',''], ['','','','']] # Output string for sentences (uni, bi, tri, quad), and paragraphs (uni, bi, tri, quad)\n",
    "finalOutputsLength = [[0,0,0,0], [0,0,0,0]] # How many tokens were output\n",
    "\n",
    "for mode in range(len(modes)):\n",
    "\tif mode == 0: \n",
    "\t\tprint(\"Sentence mode:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"Paragraph mode:\")\n",
    "\tfor g in range(len(gramsMode[mode])):\n",
    "\t\tctx = seeds[g] # Set the seed context\n",
    "\t\tcurrentCtx = seed \n",
    "\t\tfinalOutputs[mode][g] = currentCtx # Start the output with the seed\n",
    "\t\twordCount = 1\n",
    "\t\twhile currentCtx != END and wordCount < 150:\n",
    "\t\t\tcurrentCtx = generateNextGram(mode, g, g, ctx)\n",
    "\t\t\tfinalOutputs[mode][g], wordCount = setOutput(currentCtx, finalOutputs[mode][g], wordCount)\n",
    "\n",
    "\t\t\t# Update context\n",
    "\t\t\tif g == 0:\n",
    "\t\t\t\tctx = () \t\t\t\t\t\t\t# unigram seed\n",
    "\t\t\telif g == 1:\n",
    "\t\t\t\tctx = (currentCtx,) \t\t\t\t# bigram seed\n",
    "\t\t\telif g == 2:\n",
    "\t\t\t\tctx = ((ctx[1], currentCtx)) \t\t# trigram seed\n",
    "\t\t\telif g == 3:\n",
    "\t\t\t\tctx = (ctx[1], ctx[2], currentCtx) \t# quadgram seed\n",
    "\t\t\t\n",
    "\t\tfinalOutputsLength[mode][g] = wordCount\n",
    "\t\tprint(f\"{gramsPrintStrings[g]}: {finalOutputs[mode][g]}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output\n",
    "\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for g in range(0,4):\n",
    "    print(f\"Extracted {uniqueSenNGrams[g]} unique {g+1}-grams\")\n",
    "print(\"Seed text:\", seed)\n",
    "for mode in range(2):\n",
    "\tif mode == 0: \n",
    "\t\tprint(\"Sentence mode:\")\n",
    "\telif mode == 1:\n",
    "\t\tprint(\"\\nParagraph mode:\")\n",
    "\tfor g in range(0, 4):\n",
    "\t\tprint(f\"Generated {g+1}-gram text of length {finalOutputsLength[mode][g]}\")\n",
    "\t\tprint(f\"{finalOutputs[mode][g]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
