{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and </s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and </s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with </s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['o', 'thou', ',', 'my', 'lovely', 'boy', ',', 'who', 'in', 'thy', 'pow', \"'\", 'r']\n",
      "['dost', 'hold', 'timeâ€™s', 'fickle', 'glass', 'his', 'sickle', 'hour', ',']\n",
      "['who', 'hast', 'by', 'waning', 'grown', ',', 'and', 'therein', \"show'st\"]\n",
      "['thy', 'lovers', 'withering', ',', 'as', 'thy', 'sweet', 'self', 'growâ€™stâ€', '”']\n",
      "['in', 'nature', ',', 'sovereign', 'mistress', 'over', 'wrack', ',']\n",
      "['as', 'thou', 'goest', 'onwards', 'still', 'will', 'pluck', 'thee', 'back', ',']\n",
      "['she', 'keeps', 'thee', 'to', 'this', 'purpose', ',', 'that', 'her', 'skill']\n",
      "['may', 'time', 'disgrace', ',', 'and', 'wretched', 'minute', 'kill', '.']\n",
      "['yet', 'fear', 'her', ',', 'o', 'thou', 'minion', 'of', 'her', 'pleasure', ';']\n",
      "['she', 'may', 'detain', 'but', 'not', 'still', 'keep', 'her', 'treasure', '.']\n",
      "['her', 'audit', ',', 'though', 'delayed', ',', 'answered', 'must', 'be', ',']\n",
      "['and', 'her', 'quietus', 'is', 'to', 'render', 'thee', '.']\n"
     ]
    }
   ],
   "source": [
    "# Read text files and convert them to tokens\n",
    "\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"<\\s>\"\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "unigrams = {}\n",
    "bigrams = {}\n",
    "trigrams = {}\n",
    "quadgrams = {}\n",
    "\n",
    "sentences = []\n",
    "with open(\"shakespeare.txt\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower()\n",
    "        sentence = nltk.sent_tokenize(line)\n",
    "        sentences.append(sentence) # Adds each sentence to the sentences array\n",
    "        \n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        token = nltk.word_tokenize(string) # Converts each word into a token\n",
    "        print(token)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
