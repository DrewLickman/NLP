{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and </s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and </s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with </s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokens:  [['this', 'document', 'is', 'just', 'a', 'sample', '.'], ['hello', 'world', '!'], ['this', 'is', 'my', 'really', 'awesome', 'document', 'that', 'i', 'love', 'writing', 'into', '.'], ['one', 'big', 'long', 'document', '.'], ['this', 'is', 'a', 'new', 'line', '.'], ['it', 'should', 'be', 'represented', 'as', 'a', 'separate', 'array', 'in', 'paragraph', 'mode', '.'], ['this', 'is', 'the', 'end', '.'], ['it', 'should', 'end', 'now', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Imports libraries and reads corpus documents. Save the documents as tokens\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "sentences = []\n",
    "paragraphs = []\n",
    "\n",
    "with open(\"sample.txt\", encoding=\"utf-8\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower() # Converts all documents to lowercase\n",
    "        sentence = sent_tokenize(line) # Extract as entire sentences\n",
    "        paragraph = word_tokenize(line) # Extract the entire line as words (not separating sentences into different arrays!)\n",
    "        sentences.append(sentence) # Adds each sentence to the sentences array\n",
    "        paragraphs.append(paragraph) # Adds each line into the paragraphs array\n",
    "        #print(sentence)\n",
    "        ##print(paragraph)\n",
    "        #print()\n",
    "        \n",
    "#print(\"Sentences (not word tokenized): \", sentences)\n",
    "##print(paragraphs)\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokens = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "        tokens.append(tokenList)\n",
    "        #print(token)\n",
    "#print()\n",
    "print(\"Sentence tokens: \", tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'this', 'document', 'is', 'just', 'a', 'sample', '.', '</s>'], ['<s>', 'hello', 'world', '!', '</s>'], ['<s>', 'this', 'is', 'my', 'really', 'awesome', 'document', 'that', 'i', 'love', 'writing', 'into', '.', '</s>'], ['<s>', 'one', 'big', 'long', 'document', '.', '</s>'], ['<s>', 'this', 'is', 'a', 'new', 'line', '.', '</s>'], ['<s>', 'it', 'should', 'be', 'represented', 'as', 'a', 'separate', 'array', 'in', 'paragraph', 'mode', '.', '</s>'], ['<s>', 'this', 'is', 'the', 'end', '.', '</s>'], ['<s>', 'it', 'should', 'end', 'now', '.', '</s>']]\n",
      "[['<s>', '<s>', 'this', 'document', 'is', 'just', 'a', 'sample', '.', '</s>'], ['<s>', '<s>', 'hello', 'world', '!', '</s>'], ['<s>', '<s>', 'this', 'is', 'my', 'really', 'awesome', 'document', 'that', 'i', 'love', 'writing', 'into', '.', '</s>'], ['<s>', '<s>', 'one', 'big', 'long', 'document', '.', '</s>'], ['<s>', '<s>', 'this', 'is', 'a', 'new', 'line', '.', '</s>'], ['<s>', '<s>', 'it', 'should', 'be', 'represented', 'as', 'a', 'separate', 'array', 'in', 'paragraph', 'mode', '.', '</s>'], ['<s>', '<s>', 'this', 'is', 'the', 'end', '.', '</s>'], ['<s>', '<s>', 'it', 'should', 'end', 'now', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', 'this', 'document', 'is', 'just', 'a', 'sample', '.', '</s>'], ['<s>', '<s>', '<s>', 'hello', 'world', '!', '</s>'], ['<s>', '<s>', '<s>', 'this', 'is', 'my', 'really', 'awesome', 'document', 'that', 'i', 'love', 'writing', 'into', '.', '</s>'], ['<s>', '<s>', '<s>', 'one', 'big', 'long', 'document', '.', '</s>'], ['<s>', '<s>', '<s>', 'this', 'is', 'a', 'new', 'line', '.', '</s>'], ['<s>', '<s>', '<s>', 'it', 'should', 'be', 'represented', 'as', 'a', 'separate', 'array', 'in', 'paragraph', 'mode', '.', '</s>'], ['<s>', '<s>', '<s>', 'this', 'is', 'the', 'end', '.', '</s>'], ['<s>', '<s>', '<s>', 'it', 'should', 'end', 'now', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', '<s>', 'this', 'document', 'is', 'just', 'a', 'sample', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'hello', 'world', '!', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'this', 'is', 'my', 'really', 'awesome', 'document', 'that', 'i', 'love', 'writing', 'into', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'one', 'big', 'long', 'document', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'this', 'is', 'a', 'new', 'line', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'it', 'should', 'be', 'represented', 'as', 'a', 'separate', 'array', 'in', 'paragraph', 'mode', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'this', 'is', 'the', 'end', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'it', 'should', 'end', 'now', '.', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# Add START and END tokens\n",
    "# Make sure to Run All before re-running this!\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "#t[1] = [<s>tokenized words</s>], etc.\n",
    "#t[2] = [<s><s>tokenized words</s>], etc.\n",
    "#t[3] = [<s><s><s>tokenized words</s>], etc.\n",
    "#t[4] = [<s><s><s><s>tokenized words</s>], [<s><s><s><s>tokenized words</s>], [<s><s><s><s>tokenized words</s>], [<s><s><s><s>tokenized words</s>], [<s><s><s><s>tokenized words</s>]\n",
    "\n",
    "# Array of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "AugmentedTokens = [[],[],[],[]]\n",
    "# Since I am modifying each sentence, for every gram, I will add the START n times and END once per sentence\n",
    "# List comprehension as suggested by Claude 3.5-sonnet: (and modifications by myself too!)\n",
    "# newList = [expression for item in iterable]\n",
    "for i in range(len(AugmentedTokens)):\n",
    "    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens]\n",
    "# Even more compact version of all this\n",
    "#UniAugmentedTokens  = [[START]*1 + sentence + [END] for sentence in tokens]\n",
    "#BiAugmentedTokens   = [[START]*2 + sentence + [END] for sentence in tokens]\n",
    "#TriAugmentedTokens  = [[START]*3 + sentence + [END] for sentence in tokens]\n",
    "#QuadAugmentedTokens = [[START]*4 + sentence + [END] for sentence in tokens]\n",
    "\n",
    "#AugmentedTokens.append(UniAugmentedTokens)\n",
    "#AugmentedTokens.append(BiAugmentedTokens)\n",
    "#AugmentedTokens.append(TriAugmentedTokens)\n",
    "#AugmentedTokens.append(QuadAugmentedTokens)\n",
    "\n",
    "for i in range(len(AugmentedTokens)):\n",
    "    print(AugmentedTokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: {'<s>': 8, 'this': 4, 'document': 3, 'is': 4, 'just': 1, 'a': 3, 'sample': 1, '.': 7, '</s>': 8, 'hello': 1, 'world': 1, '!': 1, 'my': 1, 'really': 1, 'awesome': 1, 'that': 1, 'i': 1, 'love': 1, 'writing': 1, 'into': 1, 'one': 1, 'big': 1, 'long': 1, 'new': 1, 'line': 1, 'it': 2, 'should': 2, 'be': 1, 'represented': 1, 'as': 1, 'separate': 1, 'array': 1, 'in': 1, 'paragraph': 1, 'mode': 1, 'the': 1, 'end': 2, 'now': 1}\n",
      "<s> <s>\n",
      "<s> this\n",
      "this document\n",
      "document is\n",
      "is just\n",
      "just a\n",
      "a sample\n",
      "sample .\n",
      ". </s>\n",
      "</s> <s>\n",
      "<s> <s>\n",
      "<s> hello\n",
      "hello world\n",
      "world !\n",
      "! </s>\n",
      "</s> <s>\n",
      "<s> <s>\n",
      "<s> this\n",
      "this is\n",
      "is my\n",
      "my really\n",
      "really awesome\n",
      "awesome document\n",
      "document that\n",
      "that i\n",
      "i love\n",
      "love writing\n",
      "writing into\n",
      "into .\n",
      ". </s>\n",
      "</s> <s>\n",
      "<s> <s>\n",
      "<s> one\n",
      "one big\n",
      "big long\n",
      "long document\n",
      "document .\n",
      ". </s>\n",
      "</s> <s>\n",
      "<s> <s>\n",
      "<s> this\n",
      "this is\n",
      "is a\n",
      "a new\n",
      "new line\n",
      "line .\n",
      ". </s>\n",
      "</s> <s>\n",
      "<s> <s>\n",
      "<s> it\n",
      "it should\n",
      "should be\n",
      "be represented\n",
      "represented as\n",
      "as a\n",
      "a separate\n",
      "separate array\n",
      "array in\n",
      "in paragraph\n",
      "paragraph mode\n",
      "mode .\n",
      ". </s>\n",
      "</s> <s>\n",
      "<s> <s>\n",
      "<s> this\n",
      "this is\n",
      "is the\n",
      "the end\n",
      "end .\n",
      ". </s>\n",
      "</s> <s>\n",
      "<s> <s>\n",
      "<s> it\n",
      "it should\n",
      "should end\n",
      "end now\n",
      "now .\n",
      ". </s>\n",
      "Bigrams: {(None, '<s>'): 1, ('<s>', '<s>'): 8, ('<s>', 'this'): 4, ('this', 'document'): 1, ('document', 'is'): 1, ('is', 'just'): 1, ('just', 'a'): 1, ('a', 'sample'): 1, ('sample', '.'): 1, ('.', '</s>'): 7, ('</s>', '<s>'): 7, ('<s>', 'hello'): 1, ('hello', 'world'): 1, ('world', '!'): 1, ('!', '</s>'): 1, ('this', 'is'): 3, ('is', 'my'): 1, ('my', 'really'): 1, ('really', 'awesome'): 1, ('awesome', 'document'): 1, ('document', 'that'): 1, ('that', 'i'): 1, ('i', 'love'): 1, ('love', 'writing'): 1, ('writing', 'into'): 1, ('into', '.'): 1, ('<s>', 'one'): 1, ('one', 'big'): 1, ('big', 'long'): 1, ('long', 'document'): 1, ('document', '.'): 1, ('is', 'a'): 1, ('a', 'new'): 1, ('new', 'line'): 1, ('line', '.'): 1, ('<s>', 'it'): 2, ('it', 'should'): 2, ('should', 'be'): 1, ('be', 'represented'): 1, ('represented', 'as'): 1, ('as', 'a'): 1, ('a', 'separate'): 1, ('separate', 'array'): 1, ('array', 'in'): 1, ('in', 'paragraph'): 1, ('paragraph', 'mode'): 1, ('mode', '.'): 1, ('is', 'the'): 1, ('the', 'end'): 1, ('end', '.'): 1, ('should', 'end'): 1, ('end', 'now'): 1, ('now', '.'): 1}\n"
     ]
    }
   ],
   "source": [
    "# Convert augmented tokens into n-grams\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "unigrams = {}   # (): [\"word\", count]\n",
    "bigrams = {}    # (context1): [\"word\", count]\n",
    "trigrams = {}   # (c1, c2): [\"word\", count]\n",
    "quadgrams = {}  # (c1, c2, c3): [\"word\", count]\n",
    "grams = [unigrams, bigrams, trigrams, quadgrams]\n",
    "\n",
    "# Count unigrams\n",
    "for tokenList in AugmentedTokens[0]: #0 context words\n",
    "    for word in tokenList:\n",
    "        if word not in unigrams:\n",
    "            unigrams[word] = 1 # Initialize count as 1\n",
    "        else:\n",
    "            unigrams[word] += 1 # Increment word count\n",
    "\n",
    "print(\"Unigrams:\", unigrams)\n",
    "\n",
    "# Count bigrams\n",
    "context = None\n",
    "for tokenList in AugmentedTokens[1]: #1 context word\n",
    "    for word in tokenList:\n",
    "        if context != None:\n",
    "            bigram = (context, word)\n",
    "        if (context, word) not in bigrams:\n",
    "            bigrams[(context, word)] = 1 # Initialize count as 1\n",
    "        else:\n",
    "            bigrams[(context, word)] += 1 # Increment word count\n",
    "        context = word\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n"
     ]
    }
   ],
   "source": [
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "current = \"\"\n",
    "while current != END:\n",
    "    print(\"Test\")\n",
    "    #np.random.choice()\n",
    "    current = END\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 38 unique 1-grams\n",
      "Extracted 53 unique 2-grams\n",
      "Extracted 0 unique 3-grams\n",
      "Extracted 0 unique 4-grams\n",
      "Seed text: YYYY\n",
      "Generated 1-gram text of length X\n",
      "<1-gram text generated>\n",
      "Generated 2-gram text of length X\n",
      "<2-gram text generated>\n",
      "Generated 3-gram text of length X\n",
      "<3-gram text generated>\n",
      "Generated 4-gram text of length X\n",
      "<4-gram text generated>\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for i in range(1,5):\n",
    "    print(f\"Extracted {len(grams[i-1])} unique {i}-grams\")\n",
    "print(\"Seed text:\", \"YYYY\")\n",
    "for i in range(1, 5):\n",
    "    print(f\"Generated {i}-gram text of length X\")\n",
    "    print(f\"<{i}-gram text generated>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
