{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #2\\\n",
    "Due: 9/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Two training data input files\n",
    "    - CNN Stories\n",
    "    - Shakespeare Plays\n",
    "- Each line in the files are paragraphs, and paragraphs may contain multiple sentences\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- Text will be converted to lowercase during processing\n",
    "- Extract n-grams in both methods\n",
    "    - Sentence level\n",
    "        - Paragraph will be sentence tokenized (NLTK sent_tokenize), then all sentences will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and </s>\n",
    "    - Paragraph level\n",
    "        - Paragraph will be word tokenized (NLTK word_tokenize)\n",
    "            - Resulting data will be augmented with \\<s> and </s>\n",
    "    - n-gram extraction should never cross over line boundaries\n",
    "- The data structure used to hold tokens in each sentence should start with \\<s> and end with </s>, according to the n-grams being processed\n",
    "    - Higher order n-grams require more start symbol augments\n",
    "- Unigrams, bigrams, trigrams, quadgrams will each be kept in separate data structures\n",
    "    - Dictionaries, indexed by \"context tuples\" work well for this\n",
    "- A parallel data structure should hold the counts of the tokens that immediately follow each n-gram context\n",
    "    - These counts should be stored as probabilities by dividing by total count of tokens that appear after the n-gram context \n",
    "- Process both files first using sentence level, then followed by paragraph level\n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- Set NumPy seed to 0\n",
    "- Print the count of extracted unigrams, bigrams, trigrams, and quadgrams (for each file)\n",
    "- For each file, choose a random starting word from the unigram tokens (not </s>)\n",
    "    - This random word will be used as the seed for generated n-gram texts\n",
    "- For each gram:\n",
    "    - Using the seed word (prefixed with \\<s> as required) generate either 150 tokens or until </s> is generated\n",
    "        - Do NOT continue after </s>\n",
    "    - Each next token will be probabilistically selected from those that follow the context (if any) for hat n-gram\n",
    "    - When working with higher order n-grams, use backoff when the context does not produce a token. Use the next lower n-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence tokens:  [['this', 'document', 'is', 'just', 'a', 'sample', '.'], ['hello', 'world', '!'], ['this', 'is', 'my', 'really', 'awesome', 'document', 'that', 'i', 'love', 'writing', 'into', '.'], ['one', 'big', 'long', 'document', '.'], ['this', 'is', 'a', 'new', 'line', '.'], ['it', 'should', 'be', 'represented', 'as', 'a', 'separate', 'array', 'in', 'paragraph', 'mode', '.'], ['this', 'is', 'the', 'end', '.'], ['it', 'should', 'end', 'now', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Imports libraries and reads corpus documents. Save the documents as tokens\n",
    "\n",
    "import numpy as np\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "sentences = []\n",
    "paragraphs = []\n",
    "\n",
    "with open(\"sample.txt\", encoding=\"utf-8\") as wordList:\n",
    "    lines = wordList.readlines()\n",
    "    for line in lines:\n",
    "        line = line.lower() # Converts all documents to lowercase\n",
    "        sentence = sent_tokenize(line) # Extract as entire sentences\n",
    "        paragraph = word_tokenize(line) # Extract the entire line as words (not separating sentences into different arrays!)\n",
    "        sentences.append(sentence) # Adds each sentence to the sentences array\n",
    "        paragraphs.append(paragraph) # Adds each line into the paragraphs array\n",
    "        #print(sentence)\n",
    "        ##print(paragraph)\n",
    "        #print()\n",
    "        \n",
    "#print(\"Sentences (not word tokenized): \", sentences)\n",
    "##print(paragraphs)\n",
    "\n",
    "#print()\n",
    "# Sentence level converting sentence tokens into word tokens\n",
    "tokens = [] # [[tokens without START or END], [tokens for unigrams], [tokens for bigrams], [tokens for trigrams], [tokens for quadgrams]]\n",
    "for sent in sentences:\n",
    "    for string in sent:\n",
    "        tokenList = word_tokenize(string) # Converts each word into a token. (This will separate sentences into different arrays)\n",
    "        tokens.append(tokenList)\n",
    "        #print(token)\n",
    "#print()\n",
    "print(\"Sentence tokens: \", tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['<s>', 'this', 'document', 'is', 'just', 'a', 'sample', '.', '</s>'], ['<s>', 'hello', 'world', '!', '</s>'], ['<s>', 'this', 'is', 'my', 'really', 'awesome', 'document', 'that', 'i', 'love', 'writing', 'into', '.', '</s>'], ['<s>', 'one', 'big', 'long', 'document', '.', '</s>'], ['<s>', 'this', 'is', 'a', 'new', 'line', '.', '</s>'], ['<s>', 'it', 'should', 'be', 'represented', 'as', 'a', 'separate', 'array', 'in', 'paragraph', 'mode', '.', '</s>'], ['<s>', 'this', 'is', 'the', 'end', '.', '</s>'], ['<s>', 'it', 'should', 'end', 'now', '.', '</s>']]\n",
      "[['<s>', '<s>', 'this', 'document', 'is', 'just', 'a', 'sample', '.', '</s>'], ['<s>', '<s>', 'hello', 'world', '!', '</s>'], ['<s>', '<s>', 'this', 'is', 'my', 'really', 'awesome', 'document', 'that', 'i', 'love', 'writing', 'into', '.', '</s>'], ['<s>', '<s>', 'one', 'big', 'long', 'document', '.', '</s>'], ['<s>', '<s>', 'this', 'is', 'a', 'new', 'line', '.', '</s>'], ['<s>', '<s>', 'it', 'should', 'be', 'represented', 'as', 'a', 'separate', 'array', 'in', 'paragraph', 'mode', '.', '</s>'], ['<s>', '<s>', 'this', 'is', 'the', 'end', '.', '</s>'], ['<s>', '<s>', 'it', 'should', 'end', 'now', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', 'this', 'document', 'is', 'just', 'a', 'sample', '.', '</s>'], ['<s>', '<s>', '<s>', 'hello', 'world', '!', '</s>'], ['<s>', '<s>', '<s>', 'this', 'is', 'my', 'really', 'awesome', 'document', 'that', 'i', 'love', 'writing', 'into', '.', '</s>'], ['<s>', '<s>', '<s>', 'one', 'big', 'long', 'document', '.', '</s>'], ['<s>', '<s>', '<s>', 'this', 'is', 'a', 'new', 'line', '.', '</s>'], ['<s>', '<s>', '<s>', 'it', 'should', 'be', 'represented', 'as', 'a', 'separate', 'array', 'in', 'paragraph', 'mode', '.', '</s>'], ['<s>', '<s>', '<s>', 'this', 'is', 'the', 'end', '.', '</s>'], ['<s>', '<s>', '<s>', 'it', 'should', 'end', 'now', '.', '</s>']]\n",
      "[['<s>', '<s>', '<s>', '<s>', 'this', 'document', 'is', 'just', 'a', 'sample', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'hello', 'world', '!', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'this', 'is', 'my', 'really', 'awesome', 'document', 'that', 'i', 'love', 'writing', 'into', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'one', 'big', 'long', 'document', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'this', 'is', 'a', 'new', 'line', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'it', 'should', 'be', 'represented', 'as', 'a', 'separate', 'array', 'in', 'paragraph', 'mode', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'this', 'is', 'the', 'end', '.', '</s>'], ['<s>', '<s>', '<s>', '<s>', 'it', 'should', 'end', 'now', '.', '</s>']]\n"
     ]
    }
   ],
   "source": [
    "# Add START and END tokens\n",
    "# Make sure to Run All before re-running this!\n",
    "\n",
    "START = \"<s>\"\n",
    "END = \"</s>\"\n",
    "\n",
    "#t[1] = [<s>tokenized words</s>], etc.\n",
    "#t[2] = [<s><s>tokenized words</s>], etc.\n",
    "#t[3] = [<s><s><s>tokenized words</s>], etc.\n",
    "#t[4] = [<s><s><s><s>tokenized words</s>], [<s><s><s><s>tokenized words</s>], [<s><s><s><s>tokenized words</s>], [<s><s><s><s>tokenized words</s>], [<s><s><s><s>tokenized words</s>]\n",
    "\n",
    "# Array of AugmentedToken lists (one for each Uni/Bi/Tri/Quad grams)\n",
    "AugmentedTokens = [[],[],[],[]]\n",
    "# Since I am modifying each sentence, for every gram, I will add the START n times and END once per sentence\n",
    "# List comprehension as suggested by Claude 3.5-sonnet: (and modifications by myself too!)\n",
    "# newList = [expression for item in iterable]\n",
    "\n",
    "# I may need to adjust the count of START and END symbols (slide 17 of n-grams)\n",
    "\n",
    "for i in range(len(AugmentedTokens)):\n",
    "    AugmentedTokens[i] = [[START]*(i+1) + sentence + [END] for sentence in tokens]\n",
    "# Even more compact version of all this\n",
    "#UniAugmentedTokens  = [[START]*1 + sentence + [END] for sentence in tokens]\n",
    "#BiAugmentedTokens   = [[START]*2 + sentence + [END] for sentence in tokens]\n",
    "#TriAugmentedTokens  = [[START]*3 + sentence + [END] for sentence in tokens]\n",
    "#QuadAugmentedTokens = [[START]*4 + sentence + [END] for sentence in tokens]\n",
    "\n",
    "#AugmentedTokens.append(UniAugmentedTokens)\n",
    "#AugmentedTokens.append(BiAugmentedTokens)\n",
    "#AugmentedTokens.append(TriAugmentedTokens)\n",
    "#AugmentedTokens.append(QuadAugmentedTokens)\n",
    "\n",
    "for i in range(len(AugmentedTokens)):\n",
    "    print(AugmentedTokens[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: {'<s>': 8, 'this': 4, 'document': 3, 'is': 4, 'just': 1, 'a': 3, 'sample': 1, '.': 7, '</s>': 8, 'hello': 1, 'world': 1, '!': 1, 'my': 1, 'really': 1, 'awesome': 1, 'that': 1, 'i': 1, 'love': 1, 'writing': 1, 'into': 1, 'one': 1, 'big': 1, 'long': 1, 'new': 1, 'line': 1, 'it': 2, 'should': 2, 'be': 1, 'represented': 1, 'as': 1, 'separate': 1, 'array': 1, 'in': 1, 'paragraph': 1, 'mode': 1, 'the': 1, 'end': 2, 'now': 1}\n",
      "Bigrams: {(None, '<s>'): 1, ('<s>', '<s>'): 8, ('<s>', 'this'): 4, ('this', 'document'): 1, ('document', 'is'): 1, ('is', 'just'): 1, ('just', 'a'): 1, ('a', 'sample'): 1, ('sample', '.'): 1, ('.', '</s>'): 7, ('</s>', '<s>'): 7, ('<s>', 'hello'): 1, ('hello', 'world'): 1, ('world', '!'): 1, ('!', '</s>'): 1, ('this', 'is'): 3, ('is', 'my'): 1, ('my', 'really'): 1, ('really', 'awesome'): 1, ('awesome', 'document'): 1, ('document', 'that'): 1, ('that', 'i'): 1, ('i', 'love'): 1, ('love', 'writing'): 1, ('writing', 'into'): 1, ('into', '.'): 1, ('<s>', 'one'): 1, ('one', 'big'): 1, ('big', 'long'): 1, ('long', 'document'): 1, ('document', '.'): 1, ('is', 'a'): 1, ('a', 'new'): 1, ('new', 'line'): 1, ('line', '.'): 1, ('<s>', 'it'): 2, ('it', 'should'): 2, ('should', 'be'): 1, ('be', 'represented'): 1, ('represented', 'as'): 1, ('as', 'a'): 1, ('a', 'separate'): 1, ('separate', 'array'): 1, ('array', 'in'): 1, ('in', 'paragraph'): 1, ('paragraph', 'mode'): 1, ('mode', '.'): 1, ('is', 'the'): 1, ('the', 'end'): 1, ('end', '.'): 1, ('should', 'end'): 1, ('end', 'now'): 1, ('now', '.'): 1}\n",
      "Trigrams: {(None, None, '<s>'): 1, (None, '<s>', '<s>'): 1, ('<s>', '<s>', '<s>'): 8, ('<s>', '<s>', 'this'): 4, ('<s>', 'this', 'document'): 1, ('this', 'document', 'is'): 1, ('document', 'is', 'just'): 1, ('is', 'just', 'a'): 1, ('just', 'a', 'sample'): 1, ('a', 'sample', '.'): 1, ('sample', '.', '</s>'): 1, ('.', '</s>', '<s>'): 6, ('</s>', '<s>', '<s>'): 7, ('<s>', '<s>', 'hello'): 1, ('<s>', 'hello', 'world'): 1, ('hello', 'world', '!'): 1, ('world', '!', '</s>'): 1, ('!', '</s>', '<s>'): 1, ('<s>', 'this', 'is'): 3, ('this', 'is', 'my'): 1, ('is', 'my', 'really'): 1, ('my', 'really', 'awesome'): 1, ('really', 'awesome', 'document'): 1, ('awesome', 'document', 'that'): 1, ('document', 'that', 'i'): 1, ('that', 'i', 'love'): 1, ('i', 'love', 'writing'): 1, ('love', 'writing', 'into'): 1, ('writing', 'into', '.'): 1, ('into', '.', '</s>'): 1, ('<s>', '<s>', 'one'): 1, ('<s>', 'one', 'big'): 1, ('one', 'big', 'long'): 1, ('big', 'long', 'document'): 1, ('long', 'document', '.'): 1, ('document', '.', '</s>'): 1, ('this', 'is', 'a'): 1, ('is', 'a', 'new'): 1, ('a', 'new', 'line'): 1, ('new', 'line', '.'): 1, ('line', '.', '</s>'): 1, ('<s>', '<s>', 'it'): 2, ('<s>', 'it', 'should'): 2, ('it', 'should', 'be'): 1, ('should', 'be', 'represented'): 1, ('be', 'represented', 'as'): 1, ('represented', 'as', 'a'): 1, ('as', 'a', 'separate'): 1, ('a', 'separate', 'array'): 1, ('separate', 'array', 'in'): 1, ('array', 'in', 'paragraph'): 1, ('in', 'paragraph', 'mode'): 1, ('paragraph', 'mode', '.'): 1, ('mode', '.', '</s>'): 1, ('this', 'is', 'the'): 1, ('is', 'the', 'end'): 1, ('the', 'end', '.'): 1, ('end', '.', '</s>'): 1, ('it', 'should', 'end'): 1, ('should', 'end', 'now'): 1, ('end', 'now', '.'): 1, ('now', '.', '</s>'): 1}\n",
      "Quadgrams: {(None, None, None, '<s>'): 1, (None, None, '<s>', '<s>'): 1, (None, '<s>', '<s>', '<s>'): 1, ('<s>', '<s>', '<s>', '<s>'): 8, ('<s>', '<s>', '<s>', 'this'): 4, ('<s>', '<s>', 'this', 'document'): 1, ('<s>', 'this', 'document', 'is'): 1, ('this', 'document', 'is', 'just'): 1, ('document', 'is', 'just', 'a'): 1, ('is', 'just', 'a', 'sample'): 1, ('just', 'a', 'sample', '.'): 1, ('a', 'sample', '.', '</s>'): 1, ('sample', '.', '</s>', '<s>'): 1, ('.', '</s>', '<s>', '<s>'): 6, ('</s>', '<s>', '<s>', '<s>'): 7, ('<s>', '<s>', '<s>', 'hello'): 1, ('<s>', '<s>', 'hello', 'world'): 1, ('<s>', 'hello', 'world', '!'): 1, ('hello', 'world', '!', '</s>'): 1, ('world', '!', '</s>', '<s>'): 1, ('!', '</s>', '<s>', '<s>'): 1, ('<s>', '<s>', 'this', 'is'): 3, ('<s>', 'this', 'is', 'my'): 1, ('this', 'is', 'my', 'really'): 1, ('is', 'my', 'really', 'awesome'): 1, ('my', 'really', 'awesome', 'document'): 1, ('really', 'awesome', 'document', 'that'): 1, ('awesome', 'document', 'that', 'i'): 1, ('document', 'that', 'i', 'love'): 1, ('that', 'i', 'love', 'writing'): 1, ('i', 'love', 'writing', 'into'): 1, ('love', 'writing', 'into', '.'): 1, ('writing', 'into', '.', '</s>'): 1, ('into', '.', '</s>', '<s>'): 1, ('<s>', '<s>', '<s>', 'one'): 1, ('<s>', '<s>', 'one', 'big'): 1, ('<s>', 'one', 'big', 'long'): 1, ('one', 'big', 'long', 'document'): 1, ('big', 'long', 'document', '.'): 1, ('long', 'document', '.', '</s>'): 1, ('document', '.', '</s>', '<s>'): 1, ('<s>', 'this', 'is', 'a'): 1, ('this', 'is', 'a', 'new'): 1, ('is', 'a', 'new', 'line'): 1, ('a', 'new', 'line', '.'): 1, ('new', 'line', '.', '</s>'): 1, ('line', '.', '</s>', '<s>'): 1, ('<s>', '<s>', '<s>', 'it'): 2, ('<s>', '<s>', 'it', 'should'): 2, ('<s>', 'it', 'should', 'be'): 1, ('it', 'should', 'be', 'represented'): 1, ('should', 'be', 'represented', 'as'): 1, ('be', 'represented', 'as', 'a'): 1, ('represented', 'as', 'a', 'separate'): 1, ('as', 'a', 'separate', 'array'): 1, ('a', 'separate', 'array', 'in'): 1, ('separate', 'array', 'in', 'paragraph'): 1, ('array', 'in', 'paragraph', 'mode'): 1, ('in', 'paragraph', 'mode', '.'): 1, ('paragraph', 'mode', '.', '</s>'): 1, ('mode', '.', '</s>', '<s>'): 1, ('<s>', 'this', 'is', 'the'): 1, ('this', 'is', 'the', 'end'): 1, ('is', 'the', 'end', '.'): 1, ('the', 'end', '.', '</s>'): 1, ('end', '.', '</s>', '<s>'): 1, ('<s>', 'it', 'should', 'end'): 1, ('it', 'should', 'end', 'now'): 1, ('should', 'end', 'now', '.'): 1, ('end', 'now', '.', '</s>'): 1}\n"
     ]
    }
   ],
   "source": [
    "# Convert augmented tokens into n-grams\n",
    "\n",
    "# Dictionaries of n-grams\n",
    "unigrams = {}   # (): [\"word\", count]\n",
    "bigrams = {}    # (context1): [\"word\", count]\n",
    "trigrams = {}   # (c1, c2): [\"word\", count]\n",
    "quadgrams = {}  # (c1, c2, c3): [\"word\", count]\n",
    "grams = [unigrams, bigrams, trigrams, quadgrams]\n",
    "\n",
    "# Count unigrams\n",
    "for tokenList in AugmentedTokens[0]: #0 context words\n",
    "    for word in tokenList:\n",
    "        if word not in unigrams:\n",
    "            unigrams[word] = 1 # Initialize count as 1\n",
    "        else:\n",
    "            unigrams[word] += 1 # Increment unigram count\n",
    "\n",
    "print(\"Unigrams:\", unigrams)\n",
    "\n",
    "# Count bigrams\n",
    "context = None\n",
    "for tokenList in AugmentedTokens[1]: #1 context word\n",
    "    for word in tokenList:\n",
    "        if context != None:\n",
    "            bigram = (context, word) # push data into bigram dictionary\n",
    "        if (context, word) not in bigrams:\n",
    "            bigrams[(context, word)] = 1 # Initialize count as 1\n",
    "        else:\n",
    "            bigrams[(context, word)] += 1 # Increment bigram count\n",
    "        context = word\n",
    "\n",
    "print(\"Bigrams:\", bigrams)\n",
    "\n",
    "# Count trigrams\n",
    "context = None\n",
    "context2 = None\n",
    "for tokenList in AugmentedTokens[2]: #2 context words\n",
    "    for word in tokenList:\n",
    "        if context != None and context2 != None:\n",
    "            trigram = (context, context2, word) # push data into trigram dictionary\n",
    "        if (context, context2, word) not in trigrams:\n",
    "            trigrams[(context, context2, word)] = 1 # Initialize count as 1\n",
    "        else:\n",
    "            trigrams[(context, context2, word)] += 1 # Increment trigram count\n",
    "        context = context2\n",
    "        context2 = word\n",
    "\n",
    "print(\"Trigrams:\", trigrams)\n",
    "\n",
    "# Count quadgrams\n",
    "context = None\n",
    "context2 = None\n",
    "context3 = None\n",
    "for tokenList in AugmentedTokens[3]: #3 context words\n",
    "    for word in tokenList:\n",
    "        if context != None and context2 != None and context3 != None:\n",
    "            quadgram = (context, context2, context3, word) # push data into quadgram dictionary\n",
    "        if (context, context2,  context3, word) not in quadgrams:\n",
    "            quadgrams[(context, context2,  context3, word)] = 1 # Initialize count as 1\n",
    "        else:\n",
    "            quadgrams[(context, context2,  context3, word)] += 1 # Increment quadgram count\n",
    "        context = context2\n",
    "        context2 = context3\n",
    "        context3 = word\n",
    "\n",
    "print(\"Quadgrams:\", quadgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sam is not in the dictionary!\n",
      "do or ('i', 'do') is not in the dictionary!\n",
      "('sam', 'i') or ('sam', 'i', 'am') is not in the dictionary!\n",
      "('do', 'not', 'like') or ('i', 'do', 'not', 'like') is not in the dictionary!\n"
     ]
    }
   ],
   "source": [
    "# Definitions of gram probabilities\n",
    "\n",
    "def unigramProb(wordTest):\n",
    "    # Computes P(Wi)\n",
    "    # Probability of word test\n",
    "    if wordTest in unigrams.keys():\n",
    "        print(f\"{unigrams[wordTest]/len(unigrams):.2f}\") # .2f rounds to hundredths decimal\n",
    "    else:\n",
    "        print(wordTest, \"is not in the dictionary!\")\n",
    "\n",
    "unigramProb(\"sam\")\n",
    "###\n",
    "\n",
    "def bigramProb(wordTest, contextWord): # 1 context word\n",
    "    # Computes P(Wi|Wi-1)\n",
    "    # Probability of word test, given that its context came before it\n",
    "    bigram = (contextWord, wordTest)\n",
    "    if bigram in bigrams.keys() and wordTest in unigrams.keys():\n",
    "        #print(\"P(\", bigram, \"|\", contextWord,\") = \", bigrams[bigram], \"/\", unigrams[contextWord], \"=\")\n",
    "        print(f\"{bigrams[bigram]/unigrams[contextWord]:.2f}\") # .2f rounds to hundredths decimal\n",
    "    else:\n",
    "        print(wordTest, \"or\", bigram, \"is not in the dictionary!\")\n",
    "\n",
    "bigramProb(\"do\", \"i\")\n",
    "###\n",
    "\n",
    "def trigramProb(wordTest, contextWord, contextWord2): # 2 context words\n",
    "    # Computes P(Wi|Wi-2,Wi-1)\n",
    "    # Probability of word test, given that its context came before it\n",
    "    trigram = (contextWord, contextWord2, wordTest)\n",
    "    bigram = (contextWord, contextWord2)\n",
    "    if trigram in trigrams.keys() and bigram in bigrams.keys():\n",
    "        #print(\"P(\", trigram, \"|\", bigram, \") = \", trigrams[trigram], \"/\", bigrams[bigram], \"=\")\n",
    "        print(f\"{trigrams[trigram]/bigrams[bigram]:.2f}\") # .2f rounds to hundredths decimal\n",
    "    else:\n",
    "        print(bigram, \"or\", trigram, \"is not in the dictionary!\")\n",
    "\n",
    "trigramProb(\"am\", \"sam\", \"i\")\n",
    "###\n",
    "\n",
    "# Note: I don't think compacting this into (wordTest, trigram) would be a good idea\n",
    "def quadgramProb(wordTest, contextWord, contextWord2, contextWord3): # 3 context words\n",
    "    # Computes P(Wi|Wi-3,wi-2,Wi-1) \n",
    "    # Probability of word test, given that its context came before it\n",
    "    quadgram = (contextWord, contextWord2, contextWord3, wordTest)\n",
    "    trigram = (contextWord2, contextWord3, wordTest)\n",
    "    if quadgram in quadgrams.keys() and trigram in trigrams.keys():\n",
    "        #print(\"P(\", quadgram, \"|\", trigram, \") = \", quadgrams[quadgram], \"/\", trigrams[trigram], \"=\")\n",
    "        print(f\"{quadgrams[quadgram]/trigrams[trigram]:.2f}\") # .2f rounds to hundredths decimal\n",
    "    else:\n",
    "        print(trigram, \"or\", quadgram, \"is not in the dictionary!\")\n",
    "\n",
    "quadgramProb(\"like\", \"i\", \"do\", \"not\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.21052631578947367, 0.10526315789473684, 0.07894736842105263, 0.10526315789473684, 0.02631578947368421, 0.07894736842105263, 0.02631578947368421, 0.18421052631578946, 0.21052631578947367, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.05263157894736842, 0.05263157894736842, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.02631578947368421, 0.05263157894736842, 0.02631578947368421]\n",
      "[0.018867924528301886, 0.1509433962264151, 0.07547169811320754, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.1320754716981132, 0.1320754716981132, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.05660377358490566, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.03773584905660377, 0.03773584905660377, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886]\n",
      "[0.018867924528301886, 0.1509433962264151, 0.07547169811320754, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.1320754716981132, 0.1320754716981132, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.05660377358490566, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.03773584905660377, 0.03773584905660377, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886, 0.018867924528301886]\n",
      "[0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.11428571428571428, 0.05714285714285714, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.08571428571428572, 0.1, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.04285714285714286, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.02857142857142857, 0.02857142857142857, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285, 0.014285714285714285]\n"
     ]
    }
   ],
   "source": [
    "# Calculate probabilities of each gram\n",
    "\n",
    "probUnigram\t\t= [unigrams.get(unigram) / len(unigrams) for unigram in unigrams]\n",
    "probBigram\t\t= [bigrams.get(bigram) / len(bigrams) for bigram in bigrams]\n",
    "probTrigram\t\t= [trigrams.get(trigram) / len(trigrams) for trigram in trigrams]\n",
    "probQuadgram\t= [quadgrams.get(quadgram) / len(quadgrams) for quadgram in quadgrams]\n",
    "print(probUnigram)\n",
    "print(probBigram)\n",
    "print(probBigram)\n",
    "print(probQuadgram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert probabilities to log space\n",
    "# log(p1 * p2 * p3 * p4) = log(p1) + log(p2) + log(p3) + log(p4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3533842127.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[463], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    np.random.choice(unigrams, size=1, p=)\u001b[0m\n\u001b[1;37m                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# This is where I pull randomized words out of the dictionaries\n",
    "\n",
    "current = \"\"\n",
    "output = \"\"\n",
    "while current != END:\n",
    "    np.random.choice(unigrams, size=1, p=)\n",
    "    current = END\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 12 unique 1-grams\n",
      "Extracted 18 unique 2-grams\n",
      "Extracted 22 unique 3-grams\n",
      "Extracted 25 unique 4-grams\n",
      "Seed text: YYYY\n",
      "Generated 1-gram text of length X\n",
      "<1-gram text generated>\n",
      "Generated 2-gram text of length X\n",
      "<2-gram text generated>\n",
      "Generated 3-gram text of length X\n",
      "<3-gram text generated>\n",
      "Generated 4-gram text of length X\n",
      "<4-gram text generated>\n"
     ]
    }
   ],
   "source": [
    "# Output\n",
    "\n",
    "# This will be printed 4 times. Sentence/Paragraph splits of CNN/Shakespeare\n",
    "for i in range(1,5):\n",
    "    print(f\"Extracted {len(grams[i-1])} unique {i}-grams\")\n",
    "print(\"Seed text:\", \"YYYY\")\n",
    "for i in range(1, 5):\n",
    "    print(f\"Generated {i}-gram text of length X\")\n",
    "    print(f\"<{i}-gram text generated>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
