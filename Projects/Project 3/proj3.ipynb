{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\\\n",
    "CSCI 4820-001\\\n",
    "Project #3\\\n",
    "Due: 10/9/24"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI Usage Disclaimer:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lexicon-Based Sentiment Analysis using Custom Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment Requirements:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input\n",
    "---\n",
    "\n",
    "- Positive words\n",
    "- Negative words\n",
    "- IMDb reviews\n",
    "\n",
    "### Processing\n",
    "---\n",
    "\n",
    "- There are two classifiers\n",
    "\t- Custom Logistic Regression\n",
    "\t- sklearn LogisticRegression\n",
    "- Implement a Python class (CustomLogisticRegression)\n",
    "\t- \\__init\\__(self, learning_rate, num_iters) method\n",
    "\t\t- self.learning_rate\n",
    "\t\t- self.num_iters\n",
    "\t\t- self.weights = None\n",
    "\t\t- self.bias = None\n",
    "\t- sigmoid(z)\n",
    "\t\t- return result\n",
    "\t- fit(X, y)\n",
    "\t\t- Sets weights to correct shape and initializes them to 0\n",
    "\t\t- Applies batch gradient descent to the entire dataset in a loop for num_iters\n",
    "\t- predict(X)\n",
    "\t\t- z = w dot x + b\n",
    "\t\t- return sigmoid(z) \n",
    "\n",
    "### Output\n",
    "---\n",
    "\n",
    "- For each trial and for each classifier\n",
    "\t- Print the sklearn confusion_matrix and classification_Report\n",
    "- Output the average of the confusion matrices across trials for each classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Markdown above each cell used to explain each block of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load and preprocess IMDb reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "sentimentWords = {}\n",
    "posWords = {}\n",
    "negWords = {}\n",
    "with open(\"positive-words.txt\", encoding=\"utf-8\") as positivewords:\n",
    "\tlines = positivewords.readlines()\n",
    "\tfor line in lines:\n",
    "\t\tif line[0] != \";\" and line.strip() != '': \n",
    "\t\t\tposWords[line.rstrip('\\n')] = 1\n",
    "with open(\"negative-words.txt\", encoding=\"utf-8\") as negativewords:\n",
    "\tlines = negativewords.readlines()\n",
    "\tfor line in lines:\n",
    "\t\tif line[0] != \";\" and line.strip() != '':\n",
    "\t\t\tnegWords[line.rstrip('\\n')] = 1\n",
    "sentimentWords = {**posWords, **negWords} # Combine positive words and negative words into one dictionary\n",
    "#print(sentimentWords)\n",
    "\n",
    "# Add each line of the IMDb reviews to the reviews array\n",
    "reviews = []\n",
    "trueValues = []\n",
    "with open(\"imdb_reviews.txt\", encoding=\"utf-8\") as imdbreviews:\n",
    "\tlines = imdbreviews.readlines()\n",
    "\tfor line in lines:\n",
    "\t\tsplitLine = line.rstrip().rsplit(' ', 1)\n",
    "\t\treviews.append(splitLine[0]) # removes true sentiment label from data\n",
    "\n",
    "\t\tsentiment = splitLine[1].strip()[-8:] # the last 8 characters are either positive or negative\n",
    "\t\tif sentiment == \"positive\":\n",
    "\t\t\ttrueValues.append(1)\n",
    "\t\telif sentiment == \"negative\":\n",
    "\t\t\ttrueValues.append(0)\n",
    "\t\telse:\n",
    "\t\t\tprint(\"Error: sentiment analysis not found at end of line!\")\n",
    "print(reviews[0])\n",
    "print(trueValues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create Features(X) table and Labels(y) array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X and y need to be np.arrays\n",
    "X = np.zeros((len(reviews), len(sentimentWords)), dtype=bool) \t# Features\n",
    "y = np.zeros(len(reviews), dtype=int) \t\t# Labels\n",
    "posCount = 0\n",
    "negCount = 0\n",
    "\n",
    "for review in range(len(reviews)):\n",
    "\tfor posWord in posWords:\n",
    "\t\tif posWord in reviews[review]: #make two loops, one for pos/neg\n",
    "\t\t\tX[review, posWords[posWord]] = True\n",
    "\t\t\t#print(f\"Positive: {sentimentWords[word]}\")\n",
    "\t\t\tposCount += 1 \t\n",
    "\tfor negWord in negWords:\n",
    "\t\tif negWord in reviews[review]:\n",
    "\t\t\tX[review, negWords[negWord]] = False\n",
    "\t\t\t#print(f\"Negative: {sentimentWords[word]}\")\n",
    "\t\t\tnegCount += 1\n",
    "\n",
    "\t#print(posCount, negCount)\n",
    "\tif posCount >= negCount:\n",
    "\t\ty[review] = 1\n",
    "\telse:\n",
    "\t\ty[review] = 0\n",
    "\tposCount = 0\n",
    "\tnegCount = 0\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "# for review in range(10):\n",
    "# \tprint(y[review], end=\" \")\n",
    "# \tprint(X[review])\n",
    "\t#if y[review]:\n",
    "\t\t#print(f\"Review {review} is positive!\")\n",
    "\n",
    "# Compare sentiment count compared to true value\n",
    "count = 0\n",
    "for i in range(len(y)):\n",
    "\t#print(y[i], end=\" \")\n",
    "\t#print(trueValues[i])\n",
    "\tif y[i] == trueValues[i]: # Count how many pos/negCount labels match the trueValue in each review line\n",
    "\t\tcount+=1\n",
    "print(f\"{count} out of {len(y)} are matching. {count/len(y)*100}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Define Custom Logistic Regression class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLogisticRegression():\n",
    "\t# Constructor\n",
    "\tdef __init__(self, learning_rate, num_iters): \n",
    "\t\tself.learning_rate = learning_rate\n",
    "\t\tself.num_iters = num_iters\n",
    "\t\tself.weights = None\n",
    "\t\tself.bias = None\n",
    "\n",
    "\t# Train the model using gradient descent\n",
    "\t# X is training features, y is labels\n",
    "\tdef fit(self, X, y):\n",
    "\t\t# Sets the weights to the correct shape and initializes them to 0\n",
    "\t\tfeatures = X.shape[1]\n",
    "\t\tself.weights = np.zeros(features) # weight for each feature\n",
    "\t\tself.bias = 0\n",
    "\n",
    "\t\t# Apply batch gradient descent on entire dataset\n",
    "\t\t# Gradient descent\n",
    "\t\t# This for loop was written by Claude 3.5 Sonnet and modified by myself\n",
    "\t\tfor _ in range(self.num_iters):\n",
    "\t\t\tpredictions = self.sigmoid(self.linearTransform(X)) # Calculate array of sigmoidal probabilities\n",
    "\t\t\terror = predictions - y # Calculate the difference between predicted and actual labels\n",
    "\n",
    "\t\t\t# Compute gradient for weights\n",
    "\t\t\tdw = (1 / len(y)) * np.dot(X.T, error) # X.T is transposed \n",
    "\t\t\t# Compute gradient for bias\n",
    "\t\t\tdb = (1 / len(y)) * np.sum(error) # Average of all errors\n",
    "\n",
    "\t\t\t# Update weights and biases\n",
    "\t\t\tself.weights -= self.learning_rate * dw\n",
    "\t\t\tself.bias -= self.learning_rate * db\n",
    "\n",
    "\t# Inputs either scalar or array and outputs sigmoid function of the scalar or array\n",
    "\tdef sigmoid(self, z):\n",
    "\t\toutput = 1 / (1 + np.exp(-z)) # np.exp does e^(-z) for all samples in the reviews array\n",
    "\t\treturn output\n",
    "\n",
    "\t# Calculate probability of a sample being a class (positive or negative)\n",
    "\tdef predict(self, X):\n",
    "\t\tz = self.linearTransform(X)\n",
    "\t\tprob = self.sigmoid(z)\n",
    "\t\tprob = int(prob >= 0.5) # Convert to binary output\n",
    "\t\treturn prob\n",
    "\t\n",
    "\t# Function for X dot W + b\n",
    "\tdef linearTransform(self, X):\n",
    "\t\tz = np.dot(X, self.weights) + self.bias\n",
    "\t\treturn z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn documentation\n",
    "\t\n",
    "\t- https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict\n",
    "\n",
    "\t- https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.confusion_matrix.html\n",
    "\t\n",
    "\t- https://scikit-learn.org/1.5/modules/generated/sklearn.metrics.classification_report.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model as lm\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn import metrics\n",
    "\n",
    "# Initialize variables to store average confusion matrices\n",
    "avgConfusionMatrix_skllr = np.zeros((2, 2))\n",
    "avgConfusionMatrix_mylr = np.zeros((2, 2))\n",
    "\n",
    "trialCount = 5\n",
    "iterationCount = 1\n",
    "for trial in range(trialCount):\n",
    "\t# Shuffle input data\n",
    "\t# Split into 80% 20% split of training and test sets\n",
    "\t# Line from Claude 3.5 Sonnet\n",
    "\tX_train, X_test, y_train, y_test = ms.train_test_split(X, y, test_size=0.2, random_state=trial, shuffle=True)\n",
    "\t\n",
    "\n",
    "\tskllr = lm.LogisticRegression(solver='sag', C=0.001, max_iter=iterationCount)\n",
    "\tskllr.fit(X_train, y_train) # Only use the 80% of the data marked for training\n",
    "\tskllrPredictions = skllr.predict(X_test) # Use the remaining 20% of the data marked for testing\n",
    "\t\n",
    "\tmylr = CustomLogisticRegression(learning_rate=0.1, num_iters=iterationCount)\n",
    "\tmylr.fit(X_train, y_train) # Only use the 80% of the data marked for training\n",
    "\tmylrPredictions = np.array([mylr.predict(x) for x in X_test]) # Use the remaining 20% of the data marked for testing\n",
    "\t\n",
    "\t# for i in skllrPredictions:\n",
    "\t# \tprint(skllrPredictions[i], end=\" \")\n",
    "\t# \tprint(mylrPredictions[i])\n",
    "\t\n",
    "\t# Evaluate sklearn model\n",
    "\tprint(f\"Trial {trial + 1} - Sklearn LogisticRegression:\")\n",
    "\tprint(metrics.confusion_matrix(y_test, skllrPredictions))\n",
    "\tprint(metrics.classification_report(y_test, skllrPredictions, target_names=[\"Positive\", \"Negative\"]))\n",
    "\n",
    "\t# Evaluate custom model\n",
    "\tprint(f\"Trial {trial + 1} - Custom LogisticRegression:\")\n",
    "\tprint(metrics.confusion_matrix(y_test, mylrPredictions))\n",
    "\tprint(metrics.classification_report(y_test, mylrPredictions, target_names=[\"Positive\", \"Negative\"]))\n",
    "\n",
    "\t# Update average confusion matrices\n",
    "\tavgConfusionMatrix_skllr += metrics.confusion_matrix(y_test, skllrPredictions)\n",
    "\tavgConfusionMatrix_mylr += metrics.confusion_matrix(y_test, mylrPredictions)\n",
    "# Calculate and print average confusion matrices\n",
    "avgConfusionMatrix_skllr /= 5\n",
    "avgConfusionMatrix_mylr /= 5\n",
    "\n",
    "# After all trials are completed, print average of the trials\n",
    "print(\"Average Confusion Matrix - Sklearn LogisticRegression:\")\n",
    "print(avgConfusionMatrix_skllr)\n",
    "\n",
    "print(\"Average Confusion Matrix - Custom LogisticRegression:\")\n",
    "print(avgConfusionMatrix_mylr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
