{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LH2YgC7LfzJZ"
   },
   "source": [
    "# Project 6 Starter Code (downloads/uses OpenAI's GPT-2 Model with 345M parameters)\n",
    "### Dr. Sal Barbosa, Department of Computer Science, Middle Tennessee State University\n",
    "> Some elements of this notebook are borrowed from the example by Denis Rothman in Transformers for Natural Language Processing. Packt Publishing Ltd, 2021.\n",
    "> That example was built for Tensorflow version 1 and no longer works with Tensorflow 2.\n",
    "> Some aspects of that example and the underlying model code from OpenAI's repository have been modified by me to address below listed errors:\n",
    "> <li>Incompatibilities arising from code developed in Tensorflow v1 but running in Tensorflow v2</li>\n",
    "> <li>Removal of the tf.contrib module from Tensorflow 2</li>\n",
    "> <li>Lack of backward compatibility between the older HParams module in tf.contrib (Tensorflow v1) and the new hparams module (which is separately pip installed)</li>\n",
    "> \n",
    "> \n",
    "> Code Repository:\n",
    "> [OpenAI GPT-2 Repository](https://github.com/openai/gpt-2)\n",
    ">\n",
    "> Model Paper:\n",
    ">[Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever,2019,'Language Models are Unsupervised Multitask Learners'](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load web proxy for TAMU FASTER system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = 'http://10.72.8.25:8080'\n",
    "os.environ['https_proxy'] = 'http://10.72.8.25:8080'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone the OpenAI GPT-2 repository (comment out after first use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2122,
     "status": "ok",
     "timestamp": 1611121642694,
     "user": {
      "displayName": "Karan Sonawane",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64",
      "userId": "05479461208077736330"
     },
     "user_tz": -330
    },
    "id": "isqdu1fpfmqM",
    "outputId": "0893439c-1785-4977-ac91-9fa8088c3b03"
   },
   "outputs": [],
   "source": [
    "#!git clone https://github.com/openai/gpt-2.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace files to address tensorflow incompatibilities (comment out after first use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!unzip -o gpt2_replacement_files.zip \"gpt-2/src/*\" -d ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change into the repository directory and pip install requirements and the updated <i>hparams</i> module (comment out pip installs after first use)\n",
    ">##### If installing on a clean default container, messages suggesting adding certain elements to PATH and those referring to version incompatibilities may be disregarded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14069,
     "status": "ok",
     "timestamp": 1611121666299,
     "user": {
      "displayName": "Karan Sonawane",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64",
      "userId": "05479461208077736330"
     },
     "user_tz": -330
    },
    "id": "7RHOjN-TjUbj",
    "outputId": "3d3312bf-e2c9-489f-9a6f-061ea6a34340"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"gpt-2\")    \n",
    "#!pip3 install -r requirements.txt\n",
    "#!pip3 install hparams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import tensorflow and check its version (this currently issues some errors but everything works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6103,
     "status": "ok",
     "timestamp": 1611121682119,
     "user": {
      "displayName": "Karan Sonawane",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64",
      "userId": "05479461208077736330"
     },
     "user_tz": -330
    },
    "id": "_kpNCnh9fyYD",
    "outputId": "828003c4-1dff-4c43-d438-4c91bc573ab1"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Pre-Trained Model (it is approximately 1.4 GB in size and may take a few minutes - comment out after first use)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 download_model.py '345M' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30531,
     "status": "ok",
     "timestamp": 1611121728589,
     "user": {
      "displayName": "Karan Sonawane",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64",
      "userId": "05479461208077736330"
     },
     "user_tz": -330
    },
    "id": "jvVj0cLVkaPL",
    "outputId": "c3cabb3a-0dbf-40aa-d231-5de3b242baab"
   },
   "source": [
    "### Set output to UTF encoded text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1106,
     "status": "ok",
     "timestamp": 1611121821353,
     "user": {
      "displayName": "Karan Sonawane",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64",
      "userId": "05479461208077736330"
     },
     "user_tz": -330
    },
    "id": "boCr2SydkydA"
   },
   "outputs": [],
   "source": [
    "!export PYTHONIOENCODING=UTF-8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change to src directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1043,
     "status": "ok",
     "timestamp": 1611121828604,
     "user": {
      "displayName": "Karan Sonawane",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64",
      "userId": "05479461208077736330"
     },
     "user_tz": -330
    },
    "id": "T7C7JhElk-Lh"
   },
   "outputs": [],
   "source": [
    "os.chdir(\"src\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1122,
     "status": "ok",
     "timestamp": 1611121842649,
     "user": {
      "displayName": "Karan Sonawane",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64",
      "userId": "05479461208077736330"
     },
     "user_tz": -330
    },
    "id": "ckSsdAnblFIg"
   },
   "outputs": [],
   "source": [
    "# General imports\n",
    "import json\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules from the cloned repository. NOTE: The first time this notebook is run, it may not recognize some pip installed modules. <p><p>FIX: Ensure unnecessary downloads and installations are commented out and RESTART notebook kernel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3099,
     "status": "ok",
     "timestamp": 1611121856018,
     "user": {
      "displayName": "Karan Sonawane",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64",
      "userId": "05479461208077736330"
     },
     "user_tz": -330
    },
    "id": "2mtuJxl8tb_B"
   },
   "outputs": [],
   "source": [
    "# Local imports (from the cloned repository)\n",
    "import model, sample, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1058,
     "status": "ok",
     "timestamp": 1611121861066,
     "user": {
      "displayName": "Karan Sonawane",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64",
      "userId": "05479461208077736330"
     },
     "user_tz": -330
    },
    "id": "SAuHo4TilJhQ"
   },
   "outputs": [],
   "source": [
    "# Model interaction function (modified from function in script \"interactive_conditional_samples.py\" and included in the notebook)\n",
    "def interact_model(\n",
    "    model_name,\n",
    "    seed,\n",
    "    nsamples,\n",
    "    batch_size,\n",
    "    length,\n",
    "    temperature,\n",
    "    top_k,\n",
    "    models_dir\n",
    "):\n",
    "    models_dir = os.path.expanduser(os.path.expandvars(models_dir))\n",
    "    if batch_size is None:\n",
    "        batch_size = 1\n",
    "    assert nsamples % batch_size == 0\n",
    "\n",
    "    enc = encoder.get_encoder(model_name, models_dir)\n",
    "\n",
    "    with open(os.path.join(models_dir, model_name, 'hparams.json')) as f:\n",
    "        hparams = json.load(f)\n",
    "        #print(hparams)\n",
    "\n",
    "    if length is None:\n",
    "        length = hparams['n_ctx'] // 2\n",
    "    elif length > hparams['n_ctx']:\n",
    "        raise ValueError(\"Can't get samples longer than window size: %s\" % hparams['n_ctx'])\n",
    "\n",
    "    with tf.compat.v1.Session(graph=tf.Graph()) as sess:\n",
    "        context = tf.compat.v1.placeholder(tf.int32, [batch_size, None])\n",
    "        np.random.seed(seed)\n",
    "        tf.compat.v1.set_random_seed(seed)\n",
    "        output = sample.sample_sequence(\n",
    "            hparams=hparams, length=length,\n",
    "            context=context,\n",
    "            batch_size=batch_size,\n",
    "            temperature=temperature, top_k=top_k\n",
    "        )\n",
    "\n",
    "        saver = tf.compat.v1.train.Saver()\n",
    "        ckpt = tf.train.latest_checkpoint(os.path.join(models_dir, model_name))\n",
    "        saver.restore(sess, ckpt)\n",
    "\n",
    "        while True:\n",
    "            raw_text = input(\"Model prompt >>> \")\n",
    "            while not raw_text:\n",
    "                print('Prompt should not be empty!')\n",
    "                raw_text = input(\"Model prompt >>> \")\n",
    "            context_tokens = enc.encode(raw_text)\n",
    "            generated = 0\n",
    "            for _ in range(nsamples // batch_size):\n",
    "                out = sess.run(output, feed_dict={\n",
    "                    context: [context_tokens for _ in range(batch_size)]\n",
    "                })[:, len(context_tokens):]\n",
    "                for i in range(batch_size):\n",
    "                    generated += 1\n",
    "                    text = enc.decode(out[i])\n",
    "                    print(\"=\" * 40 + \" SAMPLE \" + str(generated) + \" \" + \"=\" * 40)\n",
    "                    print(text)\n",
    "            print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To summarize prompts paste the prompt and add TL;DR: after the prompt\n",
    "# To ask a question paste the prompt, add a space, and paste the question/answer text and enter.\n",
    "\n",
    "# Prompt is excerpted from: Why Astronauts Are 'Stuck' on the International Space Station (https://www.usnews.com/news/national-news/articles/2024-08-07/why-astronauts-are-stuck-on-the-international-space-station)\n",
    "prompt = '''\n",
    "Boeing’s Starliner spacecraft carried two astronauts to the International Space Station roughly two months ago for its first crewed test flight. \\\n",
    "Now, their return to Earth is about seven weeks overdue with no return flight yet scheduled. The path home for Butch Wilmore and Suni Williams \\\n",
    "remains unclear, with NASA floating a few options on Wednesday at a press conference.  The Boeing Starliner spacecraft successfully docked to the \\\n",
    "International Space Station on June 6, a day after launching, but several thrusters shut down during its approach, pushing the docking more than \\\n",
    "an hour behind schedule. However, NASA announced Tuesday that it has delayed SpaceX’s Crew-9 launch to the space station to allow for \"more time \\ \n",
    "for mission managers to finalize return planning for the agency’s Boeing Crew Flight Test currently docked to the orbiting laboratory.\" If the \\ \n",
    "astronauts don’t come home on Starliner, they would return to Earth on SpaceX’s Crew Dragon vehicle. \n",
    "'''\n",
    "qa= '''\n",
    "Q: What is the spacecraft called?\n",
    "A: Starliner.\n",
    "Q: How long was the docking delayed?\n",
    "A: More than one hour.\n",
    "Q: Has the astronauts' return been delayed?\n",
    "A: Yes.\n",
    "Q: What is the last name of the astronaut named Suni?\n",
    "A: Williams.\n",
    "Q: What is the name of the other astronaut flying with Suni?\n",
    "A:\n",
    "'''\n",
    "# Prompt2 is excerpted from: 12 things that wowed us at the Paris Olympics (https://www.npr.org/2024/08/12/g-s1-16581/paris-olympics-best-moments)\n",
    "prompt2 = '''\n",
    "Before this summer, Steph Curry had achieved almost everything in basketball: four NBA titles, twice named NBA MVP, two-time gold \\\n",
    "medalist in the FIBA World Cup. But there was one big item missing from his resume: He'd never been to the Olympics. When he finally arrived \\\n",
    "in Paris, the 36-year-old was clearly determined to make the most of it. He went to root for other athletes. He traded pins and autographs. \\\n",
    "And by God he was going to win that Olympic gold. The U.S. men's basketball team was pushed to the limit in its semifinal game against Serbia. \\\n",
    "As his teammates struggled to find the basket, Curry made up the difference with nine three-pointers and 36 points overall. Then, two days  \\\n",
    "later, after France clawed back to make the gold medal match a three-point game with less than 3 minutes to go, Curry's vision turned gold. \\\n",
    "He hit four triples to put the game away as the French play-by-play announcer called him \"the devil named Curry.\"\n",
    "'''\n",
    "\n",
    "qa2 = '''\n",
    "Q: By the time the Olympic games began, what had Steph Curry not done?\n",
    "A: Win a gold medal.\n",
    "Q: Haw many times was Curry named Most Valuable Player?\n",
    "A: Twice.\n",
    "Q: What team made it difficult for the U.S. team to win?\n",
    "A: Serbia.\n",
    "Q: How many 3-point baskets did Steph Curry get?\n",
    "A: Nine.\n",
    "Q: Did he enjoy the Olympics?\n",
    "A: Yes\n",
    "Q: How did the game caller refer to Curry?\n",
    "A:\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> [!WARNING AND DISCLAIMER]  \n",
    ">This model is trained on over 40 Gigabytes of data. It is impossible to know all of that information or what the model output might be, given certain prompts.\n",
    "> The model's ouput may be <b>offensive</b> in a number of ways, including (but not limited to) expressions of <b>racial, gender, or religious biases and stereotypes</b>, use of <b>profanity</b>, utterances of <b>violent rhetoric</b>, or language or depictions containing <b>strong sexual content</b>.\n",
    "> The instructor will not induce generation of such material deliberately, and is not responsible for any such output.\n",
    "### Prompt the Model to Generate Text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 976
    },
    "executionInfo": {
     "elapsed": 4045790,
     "status": "error",
     "timestamp": 1611127917030,
     "user": {
      "displayName": "Karan Sonawane",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjWjX1_4b0iu2fEkjbIRKIHq-Molc5N_CnbcU75=s64",
      "userId": "05479461208077736330"
     },
     "user_tz": -330
    },
    "id": "P8Prbrs-UHu3",
    "outputId": "9f768ee1-75a5-499a-f7f9-be0889a29f22"
   },
   "outputs": [],
   "source": [
    "# Interactively generate text with the GPT-2 model\n",
    "interact_model(model_name='345M',seed=None,nsamples=5,batch_size=1,length=100,temperature=0.9,top_k=40,models_dir='../models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OpenAI_GPT_2_KS.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
