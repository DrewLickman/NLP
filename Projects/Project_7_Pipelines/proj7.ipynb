{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drew Lickman\n",
    "\n",
    "CSCI 4820-001\n",
    "\n",
    "Project #7\n",
    "\n",
    "Due 12/3/24\n",
    "\n",
    "AI Disclaimer: A.I. Disclaimer: Work for this assignment was completed with the aid of artificial intelligence tools and comprehensive documentation of the names of, input provided to, and output obtained from, these tools is included as part of my assignment submission.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom NLP Project using 3 Hugging Face Pipelines\n",
    "### Dr. Sal Barbosa, Department of Computer Science, Middle Tennessee State University\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Description\n",
    "This project is used to analyze the transcripts of the Federal Open Market Committees (FOMC)\n",
    "\n",
    "Takes about 30 minutes to run the entire program\n",
    "\n",
    "### The Problem:\n",
    "I chose this project because I believe it is important for people to get a quick and easy-to-understand analysis of the FOMC meetings. The FOMC \"reviews economic and financial conditions, determines the appropriate stance of monetary policy, and assesses the risks to its long-run goals of price stability and sustainable economic growth\" (https://www.federalreserve.gov/monetarypolicy/fomc.htm)\n",
    "\n",
    "### The Dataset:\n",
    "The dataset I used is the FOMC transcripts from each of their meetings. I created (with Claude 3.5 Sonnet (New)) a web scraper to read the FOMC website and download the PDFs\n",
    "\n",
    "### The Solution:\n",
    "[1.](#web-scraping) Download PDF transcripts from the official FOMC website using `fomc-crawler.py`\n",
    "\n",
    "[2.](#Conversion) Convert the PDFs to text files with `pdf-to-txt.py`\n",
    "\n",
    "[3.](#BERT-based-Sentiment-Analysis) Utilize a slightly modified version of tabularisai's robust-sentiment-analysis (distil)BERT-based Sentiment Classification Model `https://huggingface.co/tabularisai/robust-sentiment-analysis` for sentiment analysis\n",
    "\n",
    "[4.](#Summarization) Summarize each document via pipeline of Falconsai's text_summarization Fine-Tuned T5 Small for Text Summarization Model `https://huggingface.co/Falconsai/text_summarization`\n",
    "\n",
    "[5.](#Question-Answering) Answer the question \"What is the current status of the economy?\" from each meeting by using consciousAI's question-answering-roberta-base-s-v2 for Question Answering `https://huggingface.co/consciousAI/question-answering-roberta-base-s-v2`\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following pip installs may be necessary to run the web scraper and pdf-to-text converter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Web Scraper and PDF-to-TXT:\n",
    "!pip install requests tqdm beautifulsoup4 pdfplumber datasets\n",
    "\n",
    "# If you encounter an error, you may not have Windows Long Path support enabled. \n",
    "# You can find information on how to enable this at https://pip.pypa.io/warnings/enable-long-paths\n",
    "!pip install transformers\n",
    "!pip install nbformat=4.2.0\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "import plotly.graph_objects as go\n",
    "from   nltk.tokenize import sent_tokenize\n",
    "from   transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "\n",
    "#nltk.download('punkt') # comment after downloading\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Web Scraping](#Project-Description)\n",
    "\n",
    "To retrieve fresh data, you must run `./data/fomc-crawler.py` and `./data/pdf-to-txt.py` to download all the FOMC transcript PDFs first, then convert the PDFs to TXT\n",
    "\n",
    "Scrape FOMC Transcripts from https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\n",
    "\n",
    "Please wait about 1 to 3 minutes\n",
    "\n",
    "Code written by Claude 3.5 Sonnet (New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./data/fomc-crawler.py\n",
    "# Outputs to ./data/fomc_transcripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [Conversion](#Project-Description)\n",
    "\n",
    "Convert PDFs to TXT\n",
    "\n",
    "Please wait 1 to 3 minutes\n",
    "\n",
    "Code written by Claude 3.5 Sonnet (New)\n",
    "\n",
    "--- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./data/pdf-to-txt.py\n",
    "# Outputs to ./data/extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data directory\n",
    "TEXT_DIR = \"./data/extracted_text\" # Local FOMC transcript data as .txt\n",
    "\n",
    "# Summary directory\n",
    "SUMMARY_DIR = \"./data/summaries\"\n",
    "\n",
    "#  Save text files and their data to a dictionary\n",
    "txt_fileNames = [txt for txt in os.listdir(TEXT_DIR) if txt.endswith('.txt')]\n",
    "\n",
    "txt_data = [open(os.path.join(TEXT_DIR, file), 'r', encoding='utf-8').read() for file in txt_fileNames]\n",
    "\n",
    "textDict = {fileName: data for fileName, data in zip(txt_fileNames, txt_data)}\n",
    "\n",
    "print(f\"{len(txt_fileNames)} documents ready for analysis!\")\n",
    "\n",
    "# If I had more time to fix up the code to get it using datasets I would use this\n",
    "# From https://www.youtube.com/watch?v=enObIMzyaE4\n",
    "# transcripts = []\n",
    "# for t in textDict:\n",
    "#     transcripts.append({\n",
    "#         'title': t,\n",
    "#         'body': textDict[t]\n",
    "#     })\n",
    "# import json\n",
    "# def save_as_jsonl(data, filename):\n",
    "#     with open(filename, \"w\") as f:\n",
    "#         for transcript in data:\n",
    "#             f.write(json.dumps(transcript) + \"\\n\")\n",
    "# save_as_jsonl(transcripts, \"train.jsonl\")\n",
    "# data_files = {\"train\": \"train.jsonl\"}\n",
    "# dataset = load_dataset(\"json\", data_files=data_files)\n",
    "# print(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Below is a helper function that splits input text into chunks due to limited context sizes of the semantic analyzer and summarizer.\n",
    "\n",
    "Written by Claude 3.5 Sonnet (New)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, max_chunk_size):\n",
    "    \"\"\"\n",
    "    Split text into chunks based on sentences to respect max token limit.\n",
    "    Tries to keep sentences together while staying under the token limit.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Rough approximation of tokens (words + punctuation)\n",
    "        sentence_length = len(sentence.split())\n",
    "        \n",
    "        if current_length + sentence_length > max_chunk_size:\n",
    "            if current_chunk:  # Save current chunk if it exists\n",
    "                chunks.append(' '.join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = sentence_length\n",
    "            else:  # Handle case where single sentence exceeds max_chunk_size\n",
    "                chunks.append(sentence)\n",
    "                current_chunk = []\n",
    "                current_length = 0\n",
    "        else:\n",
    "            current_chunk.append(sentence)\n",
    "            current_length += sentence_length\n",
    "    \n",
    "    # Add the last chunk if it exists\n",
    "    if current_chunk:\n",
    "        chunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [BERT-based Sentiment Analysis](#Project-Description)\n",
    "\n",
    "tabularisai's robust-sentiment-analysis used via pipeline:\n",
    "\n",
    "Modified to be chunked for longer input texts\n",
    "\n",
    "also outputs probability distribution, rather than just the highest result\n",
    "\n",
    "Please wait 2 to 4 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tabularisai/robust-sentiment-analysis\"\n",
    "sentimentAnalysis = pipeline(model=model_name, device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Pipeline from Hugging Face (copied from example on page, had to modify to get probability distribution)\n",
    "def predict_sentiment(text):\n",
    "\tinputs = tokenizer(text.lower(), return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "\twith torch.no_grad():\n",
    "\t\toutputs = model(**inputs)\n",
    "\t\n",
    "\tprobabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "\tpredicted_class = torch.argmax(probabilities, dim=-1).item()\n",
    "\t\n",
    "\tprobs_list = probabilities[0].tolist()\n",
    "\tsentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "\t\n",
    "\t# Create a dictionary of sentiment labels and their probabilities\n",
    "\tsentiment_probs = {\n",
    "\t\t\t\t\t\tsentiment_map[i]: prob\n",
    "\t\t\t\t\t\tfor i, prob in enumerate(probs_list)\n",
    "\t\t\t\t\t\t}\n",
    "\n",
    "\treturn {\n",
    "\t\t\t'predicted_class': sentiment_map[predicted_class],\n",
    "\t\t\t'probabilities': sentiment_probs\n",
    "\t\t\t}\n",
    "\n",
    "# Function written by Claude 3.5 Sonnet (New) to allow the pipeline to handle longer input text\n",
    "def analyze_long_text(text, max_chunk_size):\n",
    "\t\"\"\"\n",
    "\tAnalyze sentiment of long text by breaking it into chunks and averaging results.\n",
    "\t\"\"\"\n",
    "\t# Clean text\n",
    "\ttext = text.replace('\\n', ' ').strip()\n",
    "\t\n",
    "\t# Split into chunks using existing chunk_text function\n",
    "\tchunks = chunk_text(text, max_chunk_size)\n",
    "\t\n",
    "\t# Analyze each chunk\n",
    "\tchunk_sentiments = {\"Very Negative\": 0, \"Negative\": 0, \"Neutral\": 0, \"Positive\": 0, \"Very Positive\": 0}\n",
    "\tvalid_chunks = 0\n",
    "\t\n",
    "\tfor chunk in chunks:\n",
    "\t\ttry:\n",
    "\t\t\tresult = predict_sentiment(chunk) # Uses modified pipeline\n",
    "\t\t\tfor sentiment, prob in result['probabilities'].items():\n",
    "\t\t\t\tchunk_sentiments[sentiment] += prob\n",
    "\t\t\tvalid_chunks += 1\n",
    "\t\texcept Exception as e:\n",
    "\t\t\tprint(f\"Error processing chunk: {e}\")\n",
    "\t\t\tcontinue\n",
    "\t\n",
    "\t# Average the sentiments\n",
    "\tif valid_chunks > 0:\n",
    "\t\tfor sentiment in chunk_sentiments:\n",
    "\t\t\tchunk_sentiments[sentiment] /= valid_chunks\n",
    "\t\n",
    "\t# Determine overall sentiment\n",
    "\tmax_sentiment = max(chunk_sentiments.items(), key=lambda x: x[1])\n",
    "\t\n",
    "\treturn {\n",
    "\t\t\t'predicted_class': max_sentiment[0],\n",
    "\t\t\t'probabilities': chunk_sentiments\n",
    "\t\t\t}\n",
    "\n",
    "# Updated sentiment analysis loop\n",
    "sentimentCount = {\"Very Negative\": 0, \"Negative\": 0, \"Neutral\": 0, \"Positive\": 0, \"Very Positive\": 0}\n",
    "sentimentProbs = {\"Very Negative\": [], \"Negative\": [], \"Neutral\": [], \"Positive\": [], \"Very Positive\": []}\n",
    "for txt in textDict:\n",
    "    try:\n",
    "        result = analyze_long_text(textDict[txt], max_chunk_size=256)\n",
    "        print(f\"File: {txt}\")\n",
    "        print(f\"Predicted Sentiment: {result['predicted_class']}\")\n",
    "        print(\"Probability Distribution:\")\n",
    "        for sentiment, prob in result['probabilities'].items():\n",
    "            print(f\"  {sentiment}: {prob * 100:.2f}%\")\n",
    "            sentimentCount[sentiment] += prob \t\t# Save the probability to get the averages\n",
    "            sentimentProbs[sentiment].append(prob)\t# Save each probability for each sentiment\n",
    "        print()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {txt}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print average sentiment confidence\n",
    "avgSentimentPcts = []\n",
    "for sentiment in sentimentCount:\n",
    "\tavgSentimentPcts.append(float(f\"{sentimentCount[sentiment]/len(textDict) * 100:.2f}\"))\n",
    "\tprint(f\"Average {sentiment}: \\t{sentimentCount[sentiment]/len(textDict) * 100:.2f}%\")\n",
    "#print(avgSentimentPcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "sentiments = [\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"]\n",
    "percentages = avgSentimentPcts\n",
    "colors = [\"#ff4d4d\", \"#ff8c8c\", \"#8c8c8c\", \"#7fbf7f\", \"#2eb82e\"]\n",
    "\n",
    "# Create the bar chart\n",
    "barChart = go.Figure(data=[\n",
    "    go.Bar(\n",
    "        x=sentiments,\n",
    "        y=percentages,\n",
    "        marker_color=colors,\n",
    "        text=[f'{p}%' for p in percentages],\n",
    "        textposition='auto',\n",
    "    )\n",
    "])\n",
    "\n",
    "barChart.update_layout(\n",
    "    title='Average FOMC Sentiment Distribution',\n",
    "    xaxis_title='Sentiment',\n",
    "    yaxis_title='Percentage (%)',\n",
    "    yaxis_range=[0, 100],\n",
    "    template='plotly_white',\n",
    "    bargap=0.2\n",
    ")\n",
    "\n",
    "barChart.show()\n",
    "\n",
    "#####\n",
    "\n",
    "# Create the line chart with 5 different lines for each sentiment\n",
    "lineChart = go.Figure()\n",
    "\n",
    "for i, sentiment in enumerate(sentiments):\n",
    "    lineChart.add_scatter(\n",
    "        x=list(range(len(sentimentProbs[sentiment]))),\n",
    "        y=[p * 100 for p in sentimentProbs[sentiment]],\n",
    "        mode='lines',\n",
    "        name=sentiment,\n",
    "        line=dict(color=colors[i])\n",
    "    )\n",
    "\n",
    "lineChart.update_layout(\n",
    "    title='Sentiment Over Time',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Percentage (%)',\n",
    "    template='plotly_white'\n",
    ")\n",
    "\n",
    "lineChart.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [Summarization](#Project-Description)\n",
    "\n",
    "Falconsai's text_summarization used via pipeline:\n",
    "\n",
    "Modified to be chunked for longer input texts\n",
    "\n",
    "Please wait 14 - 18 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarizer = pipeline(model=\"Falconsai/text_summarization\", device=device)\n",
    "\n",
    "# Function written by Claude 3.5 Sonnet (New) to allow the pipeline to handle longer input text\n",
    "def summarize_long_text(text, summarizer, max_length_div, min_length_div, max_chunk_size):\n",
    "    \"\"\"\n",
    "    Summarize long text by breaking it into chunks and combining summaries.\n",
    "    \"\"\"\n",
    "    # Clean text\n",
    "    text = text.replace('\\n', ' ').strip()\n",
    "    \n",
    "    # Split into chunks\n",
    "    chunks = chunk_text(text, max_chunk_size)\n",
    "    chunkLen = len(chunks)\n",
    "    max_length = chunkLen // max_length_div\n",
    "    min_length = chunkLen // min_length_div\n",
    "\n",
    "    # Summarize each chunk\n",
    "    chunk_summaries = []\n",
    "    for chunk in chunks:\n",
    "        try:\n",
    "            result = summarizer(chunk, max_length=max_length, min_length=min_length) # Pipeline from Hugging Face\n",
    "            chunk_summaries.append(result[0]['summary_text'])\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing chunk: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine chunk summaries by appending them\n",
    "    if len(chunks) == 1:\n",
    "        return chunk_summaries[0]\n",
    "    else:\n",
    "        # For multiple chunks, append the summaries together\n",
    "        combined_summary = ' '.join(chunk_summaries)\n",
    "        return combined_summary\n",
    "\n",
    "counter = 0\n",
    "total = len(textDict)\n",
    "for txt in textDict:\n",
    "    try:\n",
    "        length = len(textDict[txt])\n",
    "        summary = summarize_long_text(\n",
    "            text=textDict[txt],\n",
    "            summarizer=summarizer,\n",
    "            max_length_div=2, \t# divisor of chunk\n",
    "            min_length_div=4, \t# divisor of chunk\n",
    "            max_chunk_size=256\t# Adjust based on model's token limit\n",
    "        )\n",
    "        if not os.path.exists(SUMMARY_DIR):\n",
    "            os.makedirs(SUMMARY_DIR)\n",
    "        with open(os.path.join(SUMMARY_DIR, txt), \"w+\") as summary_file:\n",
    "            summary_file.write(f\"File: {txt}\\nSummary: {summary}\\n\")\n",
    "            counter += 1\n",
    "            print(f\"{counter}/{total} files summarized.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {txt}: {e}\")\n",
    "print(f\"Finished outputting all summaries to ./data/summaries!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "List compression rate of summaries\n",
    "\n",
    "Written by Claude 3.5 Sonnet (New)\n",
    "\n",
    "Modified by myself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of original and summary files\n",
    "original_files = [f for f in os.listdir(TEXT_DIR) if f.endswith('.txt')]\n",
    "summary_files = [f for f in os.listdir(SUMMARY_DIR) if f.endswith('.txt')]\n",
    "\n",
    "# Initialize a list to store compression results\n",
    "compression_results = []\n",
    "\n",
    "# Function to read file content with error handling\n",
    "def read_file_content(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:  # Fallback encoding\n",
    "            return file.read()\n",
    "\n",
    "# Compare lengths and calculate compression percentage\n",
    "for original_file in original_files:\n",
    "\toriginal_path = os.path.join(TEXT_DIR, original_file)\n",
    "\tsummary_path = os.path.join(SUMMARY_DIR, original_file)\n",
    "\n",
    "\t# Check if summary file exists\n",
    "\tif original_file in summary_files:\n",
    "\t\toriginal_content = read_file_content(original_path)\n",
    "\t\tsummary_content = read_file_content(summary_path)\n",
    "\n",
    "\t\toriginal_length = len(original_content)\n",
    "\t\tsummary_length = len(summary_content)\n",
    "\n",
    "\t\t# Calculate compression percentage\n",
    "\t\tcompression_percent = ((original_length - summary_length) / original_length) * 100\n",
    "\t\tcompression_results.append({\n",
    "\t\t\t\"file\": original_file,\n",
    "\t\t\t\"original_length\": original_length,\n",
    "\t\t\t\"summary_length\": summary_length,\n",
    "\t\t\t\"compression_percent\": compression_percent\n",
    "\t\t})\n",
    "\n",
    "# Display results\n",
    "for result in compression_results:\n",
    "    print(f\"{result['file']}: {result['original_length']} -> {result['summary_length']} (characters) | Compression: {result['compression_percent']:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# [Question Answering](#Project-Description)\n",
    "\n",
    "consciousAI's question answering used via pipeline:\n",
    "\n",
    "Ask a question to see how the FOMC's answer changes over time\n",
    "\n",
    "Please wait 8 - 12 minutes\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questAns = pipeline(model=\"consciousAI/question-answering-roberta-base-s-v2\", device=device)\n",
    "\n",
    "# Example Questions:\t\t\t\t\t\t\t\t\t\t\t#Avg Question Quality/Confidence\n",
    "#question=\"What is the current status of the economy? \"\t\t\t#57.97%\n",
    "#question=\"What is the future of the economy going to be? \"\t\t#44.17%\n",
    "#question=\"What is the current rate of inflation? \" \t\t\t#89.51% \t#useful question and high quality rating\n",
    "#question=\"What is the status of the stock market? \" \t\t\t#25.63%\n",
    "#question=\"What have been the main economic concerns lately? \" \t#65.54%\n",
    "#question=\"What are the key decisions being made today? \" \t\t#51.64%\n",
    "#question=\"What is the current federal funds rate? \"\t\t\t#73.75%\n",
    "#question=\"How long until the quantitative easing ends? \"\t\t#48.38%\n",
    "#question=\"How much debt is the government in? \"\t\t\t\t#12.43% \t#useful question but low quality rating\n",
    "#question=\"How many Americans are unemployed? \"\t\t\t\t\t#66.61%\n",
    "#question=\"What is the best news from this meeting? \"\t\t\t#55.04%\n",
    "#question=\"What time of day is it? \"\t\t\t\t\t\t\t#78.1% \t\t#non-useful question but high quality rating\n",
    "#question=\"What color is my underwear? \"\t\t\t\t\t\t#16.05% \t#non-useful question and low quality rating\n",
    "question=\"What is the current rate of inflation? Only show me the exact number \" #91.02% \t#useful question and high quality rating, fine tuned prompt results in better quality resposnes\n",
    "#question = input(\"Enter your question: \")\n",
    "print(question)\n",
    "print()\n",
    "scoreArray = []\n",
    "for file in textDict:\n",
    "    answer \t= questAns(question=question, context=textDict[file])\n",
    "    date \t= file[12:20]\n",
    "    year \t= date[0:4]\n",
    "    month \t= date[4:6]\n",
    "    day \t= date[6:8]\n",
    "    print(f\"{month}/{day}/{year}: {round(answer['score'] * 100, 2)}%:\\t\", end=\"\")\n",
    "    scoreArray.append(answer['score'])\n",
    "    answer = re.sub(r'\\n', ' ', answer['answer'])\n",
    "    print(f\"{answer}\")\n",
    "\n",
    "npScoreArray= np.array(scoreArray)\n",
    "mean \t\t= np.mean(npScoreArray)\n",
    "variance \t= np.var(npScoreArray)\n",
    "std_dev \t= np.std(npScoreArray)\n",
    "std_err \t= std_dev/np.sqrt(len(npScoreArray))\n",
    "\n",
    "print()\n",
    "print(f\"Mean (confidence): {mean:.2%}\")\n",
    "print(f\"Standard Deviation: {std_dev:.2%}\")\n",
    "print(f\"Variance: {variance:.2%}\")\n",
    "print(f\"Standard Error: {std_err:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
